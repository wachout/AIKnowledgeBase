"""
æ–‡ä»¶åˆ†æè¿è¡Œæ¨¡å—

æ­¤æ¨¡å—æä¾›æ–‡ä»¶åˆ†æåŠŸèƒ½çš„å¼‚æ­¥æµå¼æ¥å£ï¼Œæ”¯æŒï¼š
- æ–‡ä»¶è·¯å¾„è¾“å…¥
- ç›´æ¥æ–‡æœ¬å†…å®¹è¾“å…¥
- å¼‚æ­¥æµå¼è¿”å›OpenAIæ ¼å¼çš„è‡ªç„¶è¯­è¨€åˆ†æç»“æœ
- åŒæ­¥åŒ…è£…å‡½æ•°

ä½¿ç”¨ç¤ºä¾‹ï¼š

# æµå¼åˆ†ææ–‡ä»¶ - è¿”å›è‡ªç„¶è¯­è¨€æ–‡æœ¬
for chunk in run_file_analysis_sync_stream("path/to/file.pdf"):
    content = chunk["choices"][0]["delta"]["content"]
    print(content)  # ç›´æ¥è¾“å‡ºè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼Œå¦‚"ğŸ“„ æ–‡ä»¶ä¿¡æ¯ï¼šæ–‡ä»¶åï¼štest.pdf"
    print(f"Chunk ID: {chunk['id']}, Model: {chunk['model']}")

# å¼‚æ­¥åˆ†æ
result = await run_file_analysis_async({"file_path": "test.txt", "content": "å†…å®¹"})

# åŒæ­¥åˆ†æ
result = file_analysis_run("path/to/file.md")
"""

import re
import asyncio
import json
import time
import uuid
import threading
import queue
import logging
from typing import Union, Dict, Any, AsyncGenerator

from .FileAnalyseAgent import run_file_analysis, FileAnalysisResult

logger = logging.getLogger(__name__)


def _split_long_text(content: str, max_length: int = 50000) -> list[str]:
    """
    æ™ºèƒ½åˆ‡åˆ†é•¿æ–‡æœ¬ï¼Œå°½é‡ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚
    
    Args:
        content: è¦åˆ‡åˆ†çš„æ–‡æœ¬å†…å®¹
        max_length: æ¯ä¸ªå—çš„æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
    
    Returns:
        åˆ‡åˆ†åçš„æ–‡æœ¬å—åˆ—è¡¨
    """
    if len(content) <= max_length:
        return [content]
    
    chunks = []
    # é¦–å…ˆå°è¯•æŒ‰æ®µè½åˆ‡åˆ†
    paragraphs = re.split(r'\n\s*\n', content)
    current_chunk = ""
    
    for para in paragraphs:
        # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡é™åˆ¶ï¼Œåˆ™åˆå¹¶
        if len(current_chunk) + len(para) + 2 <= max_length:
            current_chunk += para + "\n\n"
        else:
            # ä¿å­˜å½“å‰å—
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†
            if len(para) > max_length:
                # æŒ‰å¥å­åˆ‡åˆ†
                sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+', para)
                temp_chunk = ""
                for sent in sentences:
                    if len(temp_chunk) + len(sent) + 1 <= max_length:
                        temp_chunk += sent + " "
                    else:
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                        # å¦‚æœå•ä¸ªå¥å­ä¹Ÿè¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶åˆ‡åˆ†
                        if len(sent) > max_length:
                            # æŒ‰å­—ç¬¦åˆ‡åˆ†ï¼Œä½†å°½é‡åœ¨ç©ºæ ¼å¤„æ–­å¼€
                            words = sent.split()
                            temp_word_chunk = ""
                            for word in words:
                                if len(temp_word_chunk) + len(word) + 1 <= max_length:
                                    temp_word_chunk += word + " "
                                else:
                                    if temp_word_chunk:
                                        chunks.append(temp_word_chunk.strip())
                                    temp_word_chunk = word + " "
                            if temp_word_chunk:
                                temp_chunk = temp_word_chunk
                        else:
                            temp_chunk = sent + " "
                if temp_chunk:
                    current_chunk = temp_chunk
            else:
                current_chunk = para + "\n\n"
    
    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks if chunks else [content[:max_length]]


async def run_file_analysis_streaming(input_data: Union[str, Dict[str, Any]]) -> AsyncGenerator[Dict[str, Any], None]:
    """
    å¼‚æ­¥æµå¼æ‰§è¡Œæ–‡ä»¶åˆ†æï¼Œè¿”å›OpenAIæ ¼å¼çš„è‡ªç„¶è¯­è¨€æµå¼å“åº”ã€‚
    
    æ”¯æŒé•¿æ–‡æœ¬æ™ºèƒ½åˆ‡åˆ†ï¼šå¦‚æœæ–‡æœ¬è¿‡é•¿ï¼Œä¼šè‡ªåŠ¨åˆ‡åˆ†ä¸ºå¤šä¸ªå—åˆ†åˆ«åˆ†æï¼Œç„¶ååˆå¹¶ç»“æœã€‚

    Args:
        input_data: æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²æˆ–åŒ…å«å†…å®¹çš„å­—å…¸ï¼Œå¯ä»¥åŒ…å« query å‚æ•°ç”¨äºé’ˆå¯¹æ€§åˆ†æ

    Yields:
        OpenAIæ ¼å¼çš„æµå¼å“åº”å—ï¼Œå…¶ä¸­contentå­—æ®µåŒ…å«æ ¼å¼åŒ–çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼š
        {
            'id': 'file-analysis-xxxxx',
            'object': 'file.analysis.chunk',
            'created': 1234567890,
            'model': 'file-analysis-model',
            'choices': [{'index': 0, 'delta': {'content': 'ğŸ“„ æ–‡ä»¶ä¿¡æ¯ï¼šæ–‡ä»¶åï¼štest.pdf...'}, 'finish_reason': None}]
        }
    """
    # ç”Ÿæˆç»Ÿä¸€çš„IDå’ŒåŸºç¡€ä¿¡æ¯
    _id = f"file-analysis-{uuid.uuid4().hex}"
    created = int(time.time())
    model = "file-analysis-model"

    try:
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        logger.info(f"ğŸ“Š å¼€å§‹æ–‡ä»¶åˆ†æï¼Œinput_dataç±»å‹: {type(input_data)}")
        
        # æå–æ–‡æœ¬å†…å®¹å’ŒæŸ¥è¯¢
        content = ""
        query = ""
        file_path = "unknown_file"
        
        if isinstance(input_data, dict):
            content = input_data.get('content', '')
            query = input_data.get('query', '')
            file_path = input_data.get('file_path', 'unknown_file')
            logger.info(f"ğŸ“Š è¾“å…¥æ•°æ®åŒ…å«: file_path={file_path}, "
                       f"contenté•¿åº¦={len(str(content))}, "
                       f"query={query}")
        elif isinstance(input_data, str):
            file_path = input_data
        
        # å…ˆå‘é€ä¸€ä¸ªå¼€å§‹chunkï¼Œè®©è°ƒç”¨è€…çŸ¥é“å·²ç»å¼€å§‹å¤„ç†
        start_chunk = {
            "id": _id,
            "object": "file.analysis.chunk",
            "created": created,
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "content": "ğŸ“„ å¼€å§‹åˆ†ææ–‡ä»¶...\n",
                    "type": "text"
                },
                "finish_reason": None
            }]
        }
        yield start_chunk
        
        # å¦‚æœè¾“å…¥æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œéœ€è¦å…ˆè¯»å–å†…å®¹
        if isinstance(input_data, str):
            loop = asyncio.get_event_loop()
            # è¯»å–æ–‡ä»¶å†…å®¹
            from .FileAnalyseAgent import read_file_content
            content = await loop.run_in_executor(None, read_file_content, input_data)
            file_path = input_data
        
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœè¿‡é•¿åˆ™åˆ‡åˆ†
        MAX_CHUNK_LENGTH = 50000  # æ¯ä¸ªå—çš„æœ€å¤§é•¿åº¦
        text_chunks = []
        
        if isinstance(content, str) and len(content) > MAX_CHUNK_LENGTH:
            logger.warning(f"âš ï¸ æ–‡æœ¬å†…å®¹è¿‡é•¿ï¼ˆ{len(content)} å­—ç¬¦ï¼‰ï¼Œå°†æ™ºèƒ½åˆ‡åˆ†ä¸ºå¤šä¸ªå—è¿›è¡Œåˆ†æ")
            text_chunks = _split_long_text(content, MAX_CHUNK_LENGTH)
            logger.info(f"ğŸ“Š æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(text_chunks)} ä¸ªéƒ¨åˆ†")
            
            # å‘é€åˆ‡åˆ†æç¤º
            chunk_notice = f"\nâš ï¸ æ³¨æ„ï¼šæ–‡æœ¬å†…å®¹è¾ƒé•¿ï¼ˆ{len(content)} å­—ç¬¦ï¼‰ï¼Œå·²æ™ºèƒ½åˆ‡åˆ†ä¸º {len(text_chunks)} ä¸ªéƒ¨åˆ†è¿›è¡Œåˆ†æã€‚\n\n"
            notice_chunk = {
                "id": _id,
                "object": "file.analysis.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {
                        "content": chunk_notice,
                        "type": "text"
                    },
                    "finish_reason": None
                }]
            }
            yield notice_chunk
        else:
            # æ–‡æœ¬é•¿åº¦åœ¨é™åˆ¶å†…ï¼Œç›´æ¥ä½¿ç”¨
            text_chunks = [content] if content else []
        
        # å¯¹æ¯ä¸ªæ–‡æœ¬å—è¿›è¡Œåˆ†æï¼Œå¹¶ç«‹å³æµå¼è¿”å›ç»“æœ
        loop = asyncio.get_event_loop()
        total_chunks = len(text_chunks)
        
        for i, chunk_content in enumerate(text_chunks):
            if not chunk_content:
                continue
                
            chunk_num = i + 1
            
            # å¦‚æœæœ‰å¤šä¸ªå—ï¼Œå‘é€è¿›åº¦æç¤º
            if total_chunks > 1:
                progress_notice = f"\n## ğŸ“„ ç¬¬ {chunk_num}/{total_chunks} éƒ¨åˆ†åˆ†æï¼ˆ{len(chunk_content)} å­—ç¬¦ï¼‰\n\n"
                progress_chunk = {
                    "id": _id,
                    "object": "file.analysis.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "content": progress_notice,
                            "type": "text"
                        },
                        "finish_reason": None
                    }]
                }
                yield progress_chunk
            
            # æ„å»ºå½“å‰å—çš„è¾“å…¥æ•°æ®
            chunk_input_data = {
                "file_path": file_path,
                "content": chunk_content,
                "query": query
            }
            
            # æ‰§è¡Œæ–‡ä»¶åˆ†æï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡ï¼‰
            result = await loop.run_in_executor(None, run_file_analysis, chunk_input_data)
            
            # æ£€æŸ¥åˆ†æç»“æœ
            if not result.get("success", False):
                error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
                logger.error(f"âŒ ç¬¬ {chunk_num} éƒ¨åˆ†åˆ†æå¤±è´¥: {error_msg}")
                # ç»§ç»­å¤„ç†å…¶ä»–å—ï¼Œä½†è®°å½•é”™è¯¯
                error_text = f"âš ï¸ ç¬¬ {chunk_num} éƒ¨åˆ†åˆ†æå¤±è´¥: {error_msg}\n\n"
                error_chunk = {
                    "id": _id,
                    "object": "file.analysis.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "content": error_text,
                            "type": "text"
                        },
                        "finish_reason": None
                    }]
                }
                yield error_chunk
                continue
            
            # è·å–å½“å‰å—çš„åˆ†æç»“æœ
            chunk_analysis = result.get("result", "")
            
            if not chunk_analysis:
                logger.warning(f"âš ï¸ ç¬¬ {chunk_num} éƒ¨åˆ†åˆ†æå®Œæˆä½†æœªè¿”å›ç»“æœ")
                continue
            
            # å¦‚æœæœ‰å¤šä¸ªå—ï¼Œæ·»åŠ å—æ ‡é¢˜
            if total_chunks > 1:
                section_header = f"### ğŸ“‹ ç¬¬ {chunk_num} éƒ¨åˆ†åˆ†æç»“æœ\n\n"
                header_chunk = {
                    "id": _id,
                    "object": "file.analysis.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "content": section_header,
                            "type": "text"
                        },
                        "finish_reason": None
                    }]
                }
                yield header_chunk
            
            # ç«‹å³æµå¼è¿”å›å½“å‰å—çš„åˆ†æç»“æœ
            # æŒ‰æ®µè½åˆ†å‰²ï¼ˆä¿ç•™æ®µè½ç»“æ„ï¼‰
            paragraphs = re.split(r'\n\s*\n', chunk_analysis.strip())
            
            if not paragraphs or not any(p.strip() for p in paragraphs):
                # å¦‚æœæ²¡æœ‰æ®µè½ï¼Œå°è¯•æŒ‰è¡Œåˆ†å‰²
                paragraphs = [line for line in chunk_analysis.split('\n') if line.strip()]

            for paragraph in paragraphs:
                if paragraph.strip():  # è·³è¿‡ç©ºæ®µè½
                    # å¦‚æœæ®µè½å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
                    if len(paragraph) > 300:
                        # æŒ‰å¥å­åˆ†å‰²
                        sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+', paragraph)
                        for sentence in sentences:
                            if sentence.strip():
                                chunk = {
                                    "id": _id,
                                    "object": "file.analysis.chunk",
                                    "created": created,
                                    "model": model,
                                    "choices": [
                                        {
                                            "index": 0,
                                            "delta": {
                                                "content": sentence.strip() + " ",
                                                "type": "text"
                                            },
                                            "finish_reason": None,
                                        }
                                    ]
                                }
                                yield chunk
                                await asyncio.sleep(0.02)  # å°å»¶è¿Ÿæ¨¡æ‹Ÿæµå¼æ•ˆæœ
                    else:
                        # ç›´æ¥è¾“å‡ºæ•´ä¸ªæ®µè½
                        chunk = {
                            "id": _id,
                            "object": "file.analysis.chunk",
                            "created": created,
                            "model": model,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {
                                        "content": paragraph.strip() + "\n\n",
                                        "type": "text"
                                    },
                                    "finish_reason": None
                                }
                            ]
                        }
                        yield chunk
                        await asyncio.sleep(0.05)  # æ®µè½é—´ç¨é•¿å»¶è¿Ÿ
            
            # å¦‚æœæœ‰å¤šä¸ªå—ï¼Œåœ¨å—ä¹‹é—´æ·»åŠ åˆ†éš”ç¬¦
            if total_chunks > 1 and chunk_num < total_chunks:
                separator = "\n---\n\n"
                separator_chunk = {
                    "id": _id,
                    "object": "file.analysis.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "content": separator,
                            "type": "text"
                        },
                        "finish_reason": None
                    }]
                }
                yield separator_chunk
        
        # å¦‚æœæœ‰å¤šå—åˆ†æï¼Œæ·»åŠ æ€»ç»“
        if total_chunks > 1:
            summary_text = "\n## ğŸ“ åˆ†ææ€»ç»“\n\nä»¥ä¸Šæ˜¯å¯¹æ–‡ä»¶å„éƒ¨åˆ†çš„è¯¦ç»†åˆ†æï¼Œå·²å…¨éƒ¨å®Œæˆã€‚\n\n"
            summary_chunk = {
                "id": _id,
                "object": "file.analysis.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {
                        "content": summary_text,
                        "type": "text"
                    },
                    "finish_reason": None
                }]
            }
            yield summary_chunk

        # å‘é€å®Œæˆæ ‡è®°
        complete_chunk = {
            "id": _id,
            "object": "file.analysis.chunk",
            "created": created,
            "model": model,
            "choices": [
                {
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }
            ]
        }
        yield complete_chunk

    except Exception as e:
        error_text = f"âŒ æ–‡ä»¶åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_text, exc_info=True)

        error_chunk = {
            "id": _id,
            "object": "file.analysis.chunk",
            "created": created,
            "model": model,
            "choices": [
                {
                    "index": 0,
                    "delta": {
                        "content": error_text,
                        "type": "text"
                    },
                    "finish_reason": "stop"
                }
            ]
        }
        yield error_chunk


def run_file_analysis_sync_stream(input_data: Union[str, Dict[str, Any]]):
    """
    åŒæ­¥æµå¼æ–‡ä»¶åˆ†æå‡½æ•°ï¼Œè¿”å›ç¬¦åˆOpenAIæ ¼å¼çš„æµå¼å“åº”ã€‚

    Args:
        input_data: æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²æˆ–åŒ…å«å†…å®¹çš„å­—å…¸ï¼Œå¯ä»¥åŒ…å« query å‚æ•°ç”¨äºé’ˆå¯¹æ€§åˆ†æ

    Yields:
        OpenAIæ ¼å¼çš„æµå¼å“åº”å—
    """
    logger.info(f"ğŸ“Š å¼€å§‹åŒæ­¥æµå¼æ–‡ä»¶åˆ†æï¼Œinput_dataç±»å‹: {type(input_data)}")
    if isinstance(input_data, dict):
        logger.info(f"ğŸ“Š è¾“å…¥æ•°æ®è¯¦æƒ…: file_path={input_data.get('file_path', 'N/A')}, "
                   f"contenté•¿åº¦={len(str(input_data.get('content', '')))}, "
                   f"query={input_data.get('query', 'N/A')}")
    
    # ä½¿ç”¨é˜Ÿåˆ—åœ¨çº¿ç¨‹é—´ä¼ é€’æ•°æ®
    q = queue.Queue()
    error_occurred = False
    first_chunk_received = False
    
    # å®šä¹‰åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œçš„å¼‚æ­¥å‡½æ•°
    def run_async_in_thread():
        nonlocal error_occurred, first_chunk_received
        async def async_part():
            try:
                logger.info("ğŸ“Š å¼‚æ­¥çº¿ç¨‹å¼€å§‹æ‰§è¡Œæ–‡ä»¶åˆ†æ")
                chunk_count = 0
                async for chunk in run_file_analysis_streaming(input_data):
                    chunk_count += 1
                    if chunk_count == 1:
                        first_chunk_received = True
                        logger.info(f"ğŸ“Š æ”¶åˆ°ç¬¬ä¸€ä¸ªchunkï¼ŒID: {chunk.get('id', 'N/A')}")
                    q.put(chunk)
                logger.info(f"ğŸ“Š å¼‚æ­¥çº¿ç¨‹å®Œæˆï¼Œå…±å‘é€ {chunk_count} ä¸ªchunksï¼Œå‘é€ç»“æŸä¿¡å·")
                q.put(None)  # å‘é€ç»“æŸä¿¡å·
            except Exception as e:
                logger.error(f"âŒ å¼‚æ­¥çº¿ç¨‹æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                error_occurred = True
                q.put(e)
        
        # åœ¨æ–°äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(async_part())
        except Exception as e:
            logger.error(f"âŒ äº‹ä»¶å¾ªç¯æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            error_occurred = True
            q.put(e)
        finally:
            try:
                loop.close()
            except:
                pass
    
    # å¯åŠ¨çº¿ç¨‹
    t = threading.Thread(target=run_async_in_thread)
    t.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
    t.start()
    logger.info("ğŸ“Š å·²å¯åŠ¨å¼‚æ­¥çº¿ç¨‹ï¼Œç­‰å¾…ç¬¬ä¸€ä¸ªchunk...")
    
    # ä»é˜Ÿåˆ—ä¸­è·å–ç»“æœå¹¶yield
    timeout_count = 0
    max_timeout = 300  # æœ€å¤§ç­‰å¾…æ—¶é—´300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰
    chunk_count = 0
    
    while True:
        try:
            # ä½¿ç”¨è¾ƒå°çš„è¶…æ—¶æ—¶é—´ä»¥é¿å…é˜»å¡
            item = q.get(timeout=1)
            timeout_count = 0  # é‡ç½®è¶…æ—¶è®¡æ•°
            
            if item is None:  # ç»“æŸä¿¡å·
                logger.info(f"ğŸ“Š æ”¶åˆ°ç»“æŸä¿¡å·ï¼Œå…±å¤„ç†äº† {chunk_count} ä¸ªchunks")
                break
            if isinstance(item, Exception):
                # è¿”å›é”™è¯¯chunk
                logger.error(f"âŒ æ”¶åˆ°å¼‚å¸¸: {item}")
                error_chunk = {
                    "id": f"file-analysis-error-{uuid.uuid4().hex[:16]}",
                    "object": "file.analysis.chunk",
                    "created": int(time.time()),
                    "model": "file-analysis-model",
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "content": f"âŒ æ–‡ä»¶åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(item)}",
                            "type": "text"
                        },
                        "finish_reason": "stop"
                    }]
                }
                yield error_chunk
                break
            
            chunk_count += 1
            if chunk_count == 1:
                logger.info(f"ğŸ“Š æ”¶åˆ°ç¬¬ä¸€ä¸ªchunkå¹¶yieldï¼ŒID: {item.get('id', 'N/A')}")
            yield item
        except queue.Empty:
            timeout_count += 1
            # å¦‚æœç­‰å¾…ç¬¬ä¸€ä¸ªchunkè¶…è¿‡5ç§’ï¼Œè®°å½•è­¦å‘Š
            if not first_chunk_received and timeout_count > 5:
                logger.warning(f"âš ï¸ ç­‰å¾…ç¬¬ä¸€ä¸ªchunkå·²è¶…è¿‡ {timeout_count} ç§’ï¼Œçº¿ç¨‹çŠ¶æ€: {'alive' if t.is_alive() else 'dead'}")
            
            # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦è¿˜æ´»ç€
            if not t.is_alive():
                logger.warning("âš ï¸ å¼‚æ­¥çº¿ç¨‹å·²ç»“æŸï¼Œä½†æœªæ”¶åˆ°ç»“æŸä¿¡å·")
                if error_occurred:
                    # å¦‚æœå‘ç”Ÿé”™è¯¯ä½†é˜Ÿåˆ—ä¸ºç©ºï¼Œç”Ÿæˆé”™è¯¯å“åº”
                    error_chunk = {
                        "id": f"file-analysis-error-{uuid.uuid4().hex[:16]}",
                        "object": "file.analysis.chunk",
                        "created": int(time.time()),
                        "model": "file-analysis-model",
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "content": "âŒ æ–‡ä»¶åˆ†æçº¿ç¨‹å¼‚å¸¸ç»“æŸ",
                                "type": "text"
                            },
                            "finish_reason": "stop"
                        }]
                    }
                    yield error_chunk
                elif not first_chunk_received:
                    # å¦‚æœä»æœªæ”¶åˆ°ä»»ä½•chunkï¼Œå¯èƒ½æ˜¯å¯åŠ¨å¤±è´¥
                    error_chunk = {
                        "id": f"file-analysis-error-{uuid.uuid4().hex[:16]}",
                        "object": "file.analysis.chunk",
                        "created": int(time.time()),
                        "model": "file-analysis-model",
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "content": "âŒ æ–‡ä»¶åˆ†æçº¿ç¨‹å¯åŠ¨å¤±è´¥ï¼Œæœªæ”¶åˆ°ä»»ä½•æ•°æ®",
                                "type": "text"
                            },
                            "finish_reason": "stop"
                        }]
                    }
                    yield error_chunk
                break
            # å¦‚æœè¶…æ—¶æ—¶é—´è¿‡é•¿ï¼Œä¹Ÿé€€å‡º
            if timeout_count > max_timeout:
                logger.error(f"âŒ ç­‰å¾…è¶…æ—¶ï¼ˆ{max_timeout}ç§’ï¼‰ï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunks")
                error_chunk = {
                    "id": f"file-analysis-error-{uuid.uuid4().hex[:16]}",
                    "object": "file.analysis.chunk",
                    "created": int(time.time()),
                    "model": "file-analysis-model",
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "content": f"âŒ æ–‡ä»¶åˆ†æè¶…æ—¶ï¼ˆè¶…è¿‡{max_timeout}ç§’ï¼‰",
                            "type": "text"
                        },
                        "finish_reason": "stop"
                    }]
                }
                yield error_chunk
                break
            continue
    
    # ç­‰å¾…çº¿ç¨‹å®Œæˆ
    t.join(timeout=5)  # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
    logger.info("ğŸ“Š åŒæ­¥æµå¼æ–‡ä»¶åˆ†æå®Œæˆ")


def file_analysis_run(input_data: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
    """
    åŒæ­¥æ‰§è¡Œæ–‡ä»¶åˆ†æçš„ä¾¿æ·å‡½æ•°ã€‚

    Args:
        input_data: æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²æˆ–åŒ…å«å†…å®¹çš„å­—å…¸

    Returns:
        åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    # è¿è¡Œå¼‚æ­¥æ–‡ä»¶åˆ†æ
    result = loop.run_until_complete(run_file_analysis_async(input_data))

    return result


async def run_file_analysis_async(input_data: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
    """
    å¼‚æ­¥æ‰§è¡Œæ–‡ä»¶åˆ†æã€‚

    Args:
        input_data: æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²æˆ–åŒ…å«å†…å®¹çš„å­—å…¸

    Returns:
        åŒ…å«å®Œæ•´åˆ†æç»“æœçš„å­—å…¸
    """
    # æ”¶é›†æ‰€æœ‰æµå¼è¾“å‡º
    chunks = []
    async for chunk in run_file_analysis_streaming(input_data):
        chunks.append(chunk)

    # è§£ææœ€åä¸€ä¸ªæœ‰æ•ˆç»“æœ
    if not chunks:
        return {
            "success": False,
            "error": "No analysis results generated",
            "chunks": []
        }

    try:
        # å°è¯•è§£ææœ€åä¸€ä¸ªchunkæ¥è·å–æœ€ç»ˆçŠ¶æ€
        last_chunk = json.loads(chunks[-1])

        if last_chunk.get("type") == "complete":
            return {
                "success": True,
                "file_path": last_chunk.get("file_path", "unknown"),
                "chunks": chunks
            }
        elif last_chunk.get("type") == "error":
            return {
                "success": False,
                "error": last_chunk.get("message", "Unknown error"),
                "chunks": chunks
            }
        else:
            return {
                "success": True,
                "chunks": chunks
            }

    except json.JSONDecodeError:
        return {
            "success": False,
            "error": "Failed to parse analysis results",
            "chunks": chunks
        }