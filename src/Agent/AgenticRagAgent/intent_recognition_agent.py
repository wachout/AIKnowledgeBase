"""
ç”¨æˆ·æ„å›¾è¯†åˆ«å’Œæœç´¢æ™ºèƒ½ä½“

åŠŸèƒ½ç‰¹æ€§ï¼š
1. ç”¨æˆ·æ„å›¾è¯†åˆ«å’Œqueryåˆ†æ
2. åŸºäºæ„å›¾è¿›è¡ŒåŒå¼•æ“æœç´¢ï¼ˆMilvus + Elasticsearchï¼‰
3. è¯­ä¹‰å®ä½“æ‰©å±•å’Œé¢å¤–æœç´¢
4. ç»“æœåˆå¹¶å’Œè¿”å›

å·¥ä½œæµç¨‹ï¼š
1. æ„å›¾è¯†åˆ«å’Œqueryåˆ†æ â†’ 2. åˆå§‹åŒå¼•æ“æœç´¢ â†’ 3. å®ä½“æ‰©å±• â†’ 4. æ‰©å±•æœç´¢ â†’ 5. ç»“æœåˆå¹¶
"""

from concurrent.futures import ThreadPoolExecutor, as_completed
from re import S
from typing import List, Dict, Any, TypedDict, Optional

# LangChain æ ¸å¿ƒç»„ä»¶
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers.json import JsonOutputParser
from langchain_core.tools import tool
# from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_community.chat_models.tongyi import ChatTongyi
from Config.llm_config import get_chat_tongyi

# é¡¹ç›®å†…éƒ¨æ¨¡å—
from Config.embedding_config import get_embeddings
from Control.control_search import CControl as ControlSearch
from Db.sqlite_db import cSingleSqlite

# ============================================================================
# å¤§æ¨¡å‹é…ç½®
# ============================================================================

llm = get_chat_tongyi(temperature=0.7, streaming=False, enable_thinking=False)

# ============================================================================
# å·¥å…·å®šä¹‰
# ============================================================================

def file_statistics_impl(knowledge_id: str) -> Dict[str, Any]:
    """çŸ¥è¯†åº“çš„æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯

    Args:
        knowledge_id: çŸ¥è¯†åº“ID

    Returns:
        æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        # è·å–çŸ¥è¯†åº“æ–‡ä»¶æ•°é‡
        file_count = cSingleSqlite.search_file_num_by_knowledge_id(knowledge_id)
        knowledge_dict = cSingleSqlite.query_knowledge_by_knowledge_id(knowledge_id)
        knowledge_name = knowledge_dict.get("name", "æœªçŸ¥çŸ¥è¯†åº“")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if file_count == 0:
            return {
                "knowledge_name": knowledge_name,
                "file_count": 0,
                "description": f"çŸ¥è¯†åº“ {knowledge_name} ç›®å‰è¿˜æ²¡æœ‰ä¸Šä¼ ä»»ä½•æ–‡ä»¶ï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶åå†æŸ¥è¯¢ã€‚",
                "tool_result": True,
                "empty_data": True
            }

        return {
            "knowledge_name": knowledge_name,
            "file_count": file_count,
            "description": f"çŸ¥è¯†åº“ {knowledge_name} åŒ…å« {file_count} ä¸ªæ–‡ä»¶",
            "tool_result": True
        }
    except Exception as e:
        return {"error": f"è·å–æ–‡ä»¶ç»Ÿè®¡å¤±è´¥: {str(e)}", "tool_result": False}

def file_list_impl(knowledge_id: str) -> Dict[str, Any]:
    """è·å–çŸ¥è¯†åº“çš„æ–‡ä»¶åˆ—è¡¨

    Args:
        knowledge_id: çŸ¥è¯†åº“ID

    Returns:
        æ–‡ä»¶åˆ—è¡¨ä¿¡æ¯
    """
    try:
        # è·å–çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨
        files = cSingleSqlite.search_file_name_by_knowledge_id(knowledge_id)
        knowledge_dict = cSingleSqlite.query_knowledge_by_knowledge_id(knowledge_id)
        knowledge_name = knowledge_dict.get("name", "æœªçŸ¥çŸ¥è¯†åº“")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not files or len(files) == 0:
            return {
                "knowledge_name": knowledge_name,
                "files": [],
                "file_count": 0,
                "description": f"çŸ¥è¯†åº“ {knowledge_name} ç›®å‰è¿˜æ²¡æœ‰ä¸Šä¼ ä»»ä½•æ–‡ä»¶ï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶åå†æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨ã€‚",
                "tool_result": True,
                "empty_data": True
            }

        return {
            "knowledge_name": knowledge_name,
            "files": files,
            "file_count": len(files),
            "description": f"çŸ¥è¯†åº“ {knowledge_name} çš„æ–‡ä»¶åˆ—è¡¨ï¼Œæœ€å¤šæ˜¾ç¤ºå‰ {len(files)} ä¸ªæ–‡ä»¶",
            "tool_result": True
        }
    except Exception as e:
        return {"error": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}", "tool_result": False}

def file_summary_impl(file_name: str) -> Dict[str, Any]:
    """è·å–æ–‡ä»¶çš„è¯¦ç»†ä¸»æ—¨ä¿¡æ¯

    Args:
        file_name: æ–‡ä»¶å

    Returns:
        æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
    """
    try:
        # è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        file_info = cSingleSqlite.search_file_detail_info_by_file_name(file_name)

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not file_info:
            return {
                "file_name": file_name,
                "file_info": None,
                "description": f"æœªæ‰¾åˆ°æ–‡ä»¶ {file_name} çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·ç¡®è®¤æ–‡ä»¶åæ˜¯å¦æ­£ç¡®ã€‚",
                "tool_result": True,
                "empty_data": True
            }

        return {
            "file_name": file_name,
            "file_info": file_info,
            "description": f"æ–‡ä»¶ {file_name} çš„è¯¦ç»†ä¿¡æ¯",
            "tool_result": True
        }
    except Exception as e:
        return {
            "file_name": file_name,
            "file_info": None,
            "description": f"æŸ¥è¯¢æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}",
            "tool_result": True,
            "empty_data": True
        }

# åˆ›å»ºå·¥å…·å¯¹è±¡
file_statistics_tool = tool("file_statistics")(file_statistics_impl)
file_list_tool = tool("file_list")(file_list_impl)
file_summary_tool = tool("file_summary")(file_summary_impl)

# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

class IntentSearchState(TypedDict):
    """æ„å›¾è¯†åˆ«å’Œæœç´¢çš„çŠ¶æ€"""
    question: str                           # ç”¨æˆ·åŸå§‹é—®é¢˜
    knowledge_id: str                       # çŸ¥è¯†åº“ID
    user_id: str                            # ç”¨æˆ·ID
    intent_analysis: Dict[str, Any]         # æ„å›¾åˆ†æç»“æœ
    search_results: List[Dict[str, Any]]    # æœç´¢ç»“æœ
    expanded_queries: List[str]             # æ‰©å±•æŸ¥è¯¢
    flag: bool                              # æƒé™æ ‡å¿—

# ============================================================================
# ç”¨æˆ·æ„å›¾è¯†åˆ«å’Œæœç´¢æ™ºèƒ½ä½“
# ============================================================================

class IntentRecognitionAgent:
    """ç”¨æˆ·æ„å›¾è¯†åˆ«å’Œæ™ºèƒ½æœç´¢æ™ºèƒ½ä½“

    åŠŸèƒ½æµç¨‹ï¼š
    1. ç”¨æˆ·æ„å›¾è¯†åˆ«å’Œqueryåˆ†æ
    2. åŸºäºæ„å›¾è¿›è¡ŒåŒå¼•æ“æœç´¢ï¼ˆMilvus + Elasticsearchï¼‰
    3. è¯­ä¹‰å®ä½“æ‰©å±•å’Œé¢å¤–æœç´¢
    4. ç»“æœåˆå¹¶å’Œè¿”å›
    """

    def __init__(self):
        self.llm = llm  # ä½¿ç”¨é…ç½®çš„å¤§æ¨¡å‹
        self.search_obj = ControlSearch()  # ç»Ÿä¸€æœç´¢æ§åˆ¶å™¨

    def simple_tool_judgment(self, query: str, intent_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºå…³é”®è¯çš„ç®€å•å·¥å…·åˆ¤æ–­

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            intent_analysis: æ„å›¾åˆ†æç»“æœ

        Returns:
            å·¥å…·åˆ¤æ–­ç»“æœ
        """
        query_lower = query.lower()
        keywords = intent_analysis.get("keywords", [])

        # æ–‡ä»¶ç»Ÿè®¡ç›¸å…³çš„å…³é”®è¯
        file_stats_keywords = ["å¤šå°‘", "å‡ ä¸ª", "æ•°é‡", "ç»Ÿè®¡", "æ€»æ•°", "æ€»å…±", "count", "number"]
        # æ–‡ä»¶åˆ—è¡¨ç›¸å…³çš„å…³é”®è¯
        file_list_keywords = ["åˆ—å‡º", "åˆ—è¡¨", "æ¸…å•", "æœ‰å“ªäº›", "æ˜¾ç¤º", "æŸ¥çœ‹", "list", "show"]
        # æ–‡ä»¶æ‘˜è¦ç›¸å…³çš„å…³é”®è¯ (è¿™äº›å…³é”®è¯å‡ºç°æ—¶é€šå¸¸ä¸åº”è¯¥ä½¿ç”¨å·¥å…·ï¼Œå› ä¸ºéœ€è¦å…·ä½“å†…å®¹)
        file_content_keywords = ["å†…å®¹", "æ‘˜è¦", "æ€»ç»“", "åˆ†æ", "è§£é‡Š", "content", "summary", "analyze"]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶å†…å®¹ç›¸å…³çš„å…³é”®è¯ï¼Œå¦‚æœæœ‰åˆ™ä¸ä½¿ç”¨å·¥å…·
        for keyword in file_content_keywords:
            if keyword in query_lower:
                return {
                    "can_answer_directly": False,
                    "tool_to_use": "none",
                    "tool_params": {},
                    "reasoning": f"æŸ¥è¯¢åŒ…å«å†…å®¹åˆ†æå…³é”®è¯'{keyword}'ï¼Œéœ€è¦æœç´¢è€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨å·¥å…·"
                }

        # æ£€æŸ¥æ–‡ä»¶ç»Ÿè®¡å…³é”®è¯
        for keyword in file_stats_keywords:
            if keyword in query_lower or any(k.lower() == keyword for k in keywords):
                return {
                    "can_answer_directly": True,
                    "tool_to_use": "file_statistics",
                    "tool_params": {},
                    "reasoning": f"æ£€æµ‹åˆ°ç»Ÿè®¡ç›¸å…³å…³é”®è¯'{keyword}'ï¼Œå»ºè®®ä½¿ç”¨file_statisticså·¥å…·"
                }

        # æ£€æŸ¥æ–‡ä»¶åˆ—è¡¨å…³é”®è¯
        for keyword in file_list_keywords:
            if keyword in query_lower or any(k.lower() == keyword for k in keywords):
                return {
                    "can_answer_directly": True,
                    "tool_to_use": "file_list",
                    "tool_params": {},
                    "reasoning": f"æ£€æµ‹åˆ°åˆ—è¡¨ç›¸å…³å…³é”®è¯'{keyword}'ï¼Œå»ºè®®ä½¿ç”¨file_listå·¥å…·"
                }

        # æ£€æŸ¥æ˜¯å¦è¯¢é—®ç‰¹å®šæ–‡ä»¶
        # å¦‚æœæŸ¥è¯¢ä¸­åŒ…å«æ–‡ä»¶åç›¸å…³çš„æ¨¡å¼ï¼Œå¯èƒ½éœ€è¦file_summary
        # ä½†è¿™ä¸ªæ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶äº¤ç»™LLMåˆ¤æ–­

        return {
            "can_answer_directly": False,
            "tool_to_use": "none",
            "tool_params": {},
            "reasoning": "æœªæ£€æµ‹åˆ°æ˜ç¡®çš„å·¥å…·ä½¿ç”¨å…³é”®è¯"
        }

    def can_answer_with_tools(self, query: str, intent_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ¤æ–­æ˜¯å¦å¯ä»¥é€šè¿‡å·¥å…·ç›´æ¥å›ç­”é—®é¢˜

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            intent_analysis: æ„å›¾åˆ†æç»“æœ

        Returns:
            åŒ…å«æ˜¯å¦èƒ½å›ç­”å’Œå·¥å…·ç»“æœçš„å­—å…¸
        """
        try:
            # åˆ†ææŸ¥è¯¢æ˜¯å¦å¯ä»¥ç›´æ¥é€šè¿‡å·¥å…·å›ç­”ï¼ˆåŸºäºè¯­ä¹‰ç†è§£ï¼‰
            analysis_prompt = ChatPromptTemplate.from_template(
                """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œéœ€è¦é€šè¿‡**è¯­ä¹‰ç†è§£**æ¥åˆ¤æ–­ç”¨æˆ·æŸ¥è¯¢æ˜¯å¦å¯ä»¥é€šè¿‡ç‰¹å®šå·¥å…·ç›´æ¥å›ç­”ã€‚

**é‡è¦åŸåˆ™ï¼š**
- å¿…é¡»é€šè¿‡ç†è§£ç”¨æˆ·æŸ¥è¯¢çš„**çœŸå®æ„å›¾å’Œè¯­ä¹‰**æ¥åˆ¤æ–­ï¼Œè€Œä¸æ˜¯ç®€å•çš„å…³é”®è¯åŒ¹é…
- ä»”ç»†åˆ†æç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒè¯‰æ±‚ï¼Œåˆ¤æ–­æ˜¯å¦çœŸçš„åªéœ€è¦å·¥å…·æä¾›çš„ä¿¡æ¯å°±èƒ½å›ç­”
- å¦‚æœç”¨æˆ·é—®é¢˜æ¶‰åŠæ–‡ä»¶å†…å®¹åˆ†æã€çŸ¥è¯†æ£€ç´¢ã€ä¿¡æ¯æå–ç­‰ï¼Œåˆ™ä¸åº”è¯¥ä½¿ç”¨å·¥å…·

**å¯ç”¨å·¥å…·è¯¦ç»†è¯´æ˜ï¼š**

1. **file_statistics** - è·å–çŸ¥è¯†åº“çš„æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
   - **è¯­ä¹‰ç‰¹å¾**ï¼šç”¨æˆ·è¯¢é—®çš„æ˜¯çŸ¥è¯†åº“çš„**æ–‡ä»¶æ•°é‡ç»Ÿè®¡**ï¼Œä¸æ¶‰åŠæ–‡ä»¶å†…å®¹
   - **é€‚ç”¨åœºæ™¯**ï¼šè¯¢é—®"æœ‰å¤šå°‘æ–‡ä»¶"ã€"æ–‡ä»¶æ€»æ•°"ã€"æ–‡ä»¶æ•°é‡ç»Ÿè®¡"ç­‰çº¯æ•°é‡é—®é¢˜
   - **ä¸é€‚ç”¨åœºæ™¯**ï¼šè™½ç„¶åŒ…å«"å¤šå°‘"ã€"ç»Ÿè®¡"ç­‰è¯ï¼Œä½†å®é™…æ˜¯åœ¨è¯¢é—®æ–‡ä»¶å†…å®¹ã€è´¨é‡ã€ç±»å‹ç­‰éœ€è¦æœç´¢çš„é—®é¢˜
   - ç¤ºä¾‹æŸ¥è¯¢ï¼ˆé€‚ç”¨ï¼‰ï¼š"çŸ¥è¯†åº“æœ‰å¤šå°‘æ–‡ä»¶ï¼Ÿ", "æ–‡ä»¶æ€»æ•°æ˜¯å¤šå°‘ï¼Ÿ", "ç»Ÿè®¡ä¸€ä¸‹æ–‡ä»¶æ•°é‡"
   - ç¤ºä¾‹æŸ¥è¯¢ï¼ˆä¸é€‚ç”¨ï¼‰ï¼š"ç»Ÿè®¡ä¸€ä¸‹æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯", "æœ‰å¤šå°‘æ–‡ä»¶æåˆ°äº†XXX", "æ–‡ä»¶å†…å®¹ç»Ÿè®¡"
   - å‚æ•°ï¼š{{"knowledge_id": "çŸ¥è¯†åº“ID"}}

2. **file_list** - è·å–çŸ¥è¯†åº“çš„å®Œæ•´æ–‡ä»¶åˆ—è¡¨
   - **è¯­ä¹‰ç‰¹å¾**ï¼šç”¨æˆ·æƒ³æŸ¥çœ‹çŸ¥è¯†åº“åŒ…å«çš„**æ–‡ä»¶åç§°åˆ—è¡¨**ï¼Œä¸æ¶‰åŠæ–‡ä»¶å†…å®¹
   - **é€‚ç”¨åœºæ™¯**ï¼šè¯¢é—®"æœ‰å“ªäº›æ–‡ä»¶"ã€"æ–‡ä»¶æ¸…å•"ã€"åˆ—å‡ºæ–‡ä»¶"ç­‰çº¯åˆ—è¡¨é—®é¢˜
   - **ä¸é€‚ç”¨åœºæ™¯**ï¼šè™½ç„¶åŒ…å«"åˆ—å‡º"ã€"æœ‰å“ªäº›"ç­‰è¯ï¼Œä½†å®é™…æ˜¯åœ¨è¯¢é—®æ–‡ä»¶å†…å®¹ã€æ–‡ä»¶ä¸­çš„ä¿¡æ¯ç­‰éœ€è¦æœç´¢çš„é—®é¢˜
   - ç¤ºä¾‹æŸ¥è¯¢ï¼ˆé€‚ç”¨ï¼‰ï¼š"åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶", "çŸ¥è¯†åº“æœ‰å“ªäº›æ–‡ä»¶ï¼Ÿ", "æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨", "æ–‡ä»¶æ¸…å•"
   - ç¤ºä¾‹æŸ¥è¯¢ï¼ˆä¸é€‚ç”¨ï¼‰ï¼š"åˆ—å‡ºåŒ…å«XXXçš„æ–‡ä»¶", "æœ‰å“ªäº›æ–‡ä»¶æåˆ°äº†YYY", "åˆ—å‡ºæ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯"
   - å‚æ•°ï¼š{{"knowledge_id": "çŸ¥è¯†åº“ID"}}

3. **file_summary** - è·å–ç‰¹å®šæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯å’Œæ‘˜è¦
   - **è¯­ä¹‰ç‰¹å¾**ï¼šç”¨æˆ·è¯¢é—®çš„æ˜¯**å·²çŸ¥æ–‡ä»¶å**çš„å…ƒæ•°æ®ä¿¡æ¯ï¼ˆå¦‚åˆ›å»ºæ—¶é—´ã€å¤§å°ç­‰ï¼‰ï¼Œè€Œä¸æ˜¯æ–‡ä»¶çš„å…·ä½“å†…å®¹
   - **é€‚ç”¨åœºæ™¯**ï¼šç”¨æˆ·æ˜ç¡®æåˆ°æ–‡ä»¶åï¼Œä¸”è¯¢é—®çš„æ˜¯æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯ã€å…ƒæ•°æ®
   - **ä¸é€‚ç”¨åœºæ™¯**ï¼šè¯¢é—®æ–‡ä»¶å†…å®¹ã€æ–‡ä»¶ä¸­çš„å…·ä½“ä¿¡æ¯ã€éœ€è¦æœç´¢æ–‡ä»¶å†…å®¹çš„é—®é¢˜
   - ç¤ºä¾‹æŸ¥è¯¢ï¼ˆé€‚ç”¨ï¼‰ï¼š"æ–‡ä»¶XXXçš„åŸºæœ¬ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ", "XXXæ–‡ä»¶çš„å…ƒæ•°æ®", "XXXæ–‡ä»¶ä»€ä¹ˆæ—¶å€™åˆ›å»ºçš„ï¼Ÿ"
   - ç¤ºä¾‹æŸ¥è¯¢ï¼ˆä¸é€‚ç”¨ï¼‰ï¼š"æ–‡ä»¶XXXçš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ", "XXXæ–‡ä»¶è®²äº†ä»€ä¹ˆï¼Ÿ", "æ€»ç»“XXXæ–‡ä»¶çš„å†…å®¹"
   - å‚æ•°ï¼š{{"file_name": "å…·ä½“çš„æ–‡ä»¶å"}}

**ç”¨æˆ·æŸ¥è¯¢ï¼š** {query}

**æ„å›¾åˆ†æï¼š**
- ä¸»è¦æ„å›¾ï¼š{main_intent}
- æŸ¥è¯¢ç±»å‹ï¼š{query_type}
- å…³é”®è¯ï¼š{keywords}

**è¯­ä¹‰ç†è§£åˆ¤æ–­è§„åˆ™ï¼š**
1. **ä»”ç»†åˆ†æç”¨æˆ·çœŸå®æ„å›¾**ï¼š
   - ç”¨æˆ·æ˜¯åœ¨è¯¢é—®æ–‡ä»¶/çŸ¥è¯†åº“çš„å…ƒæ•°æ®ï¼ˆæ•°é‡ã€åˆ—è¡¨ã€åŸºæœ¬ä¿¡æ¯ï¼‰ï¼Ÿ
   - è¿˜æ˜¯éœ€è¦æœç´¢ã€åˆ†æã€æå–æ–‡ä»¶å†…å®¹ä¸­çš„ä¿¡æ¯ï¼Ÿ

2. **åŒºåˆ†å·¥å…·é€‚ç”¨åœºæ™¯**ï¼š
   - âœ… é€‚ç”¨ï¼šçº¯å…ƒæ•°æ®æŸ¥è¯¢ï¼ˆæ–‡ä»¶æ•°é‡ã€æ–‡ä»¶åˆ—è¡¨ã€æ–‡ä»¶åŸºæœ¬ä¿¡æ¯ï¼‰
   - âŒ ä¸é€‚ç”¨ï¼šå†…å®¹æŸ¥è¯¢ï¼ˆæ–‡ä»¶å†…å®¹ã€æ–‡ä»¶ä¸­çš„ä¿¡æ¯ã€éœ€è¦æœç´¢çš„é—®é¢˜ï¼‰

3. **å¸¸è§è¯¯åˆ¤æƒ…å†µ**ï¼š
   - åŒ…å«"ç»Ÿè®¡"ä½†å®é™…æ˜¯"ç»Ÿè®¡æ–‡ä»¶å†…å®¹" â†’ ä¸ä½¿ç”¨å·¥å…·
   - åŒ…å«"åˆ—å‡º"ä½†å®é™…æ˜¯"åˆ—å‡ºæ–‡ä»¶ä¸­çš„ä¿¡æ¯" â†’ ä¸ä½¿ç”¨å·¥å…·
   - åŒ…å«"å¤šå°‘"ä½†å®é™…æ˜¯"æ–‡ä»¶ä¸­æœ‰å¤šå°‘ç›¸å…³ä¿¡æ¯" â†’ ä¸ä½¿ç”¨å·¥å…·

**è¯·åŸºäºè¯­ä¹‰ç†è§£è¿›è¡Œåˆ¤æ–­ï¼š**
1. æ·±å…¥ç†è§£ç”¨æˆ·æŸ¥è¯¢çš„çœŸå®æ„å›¾
2. åˆ¤æ–­æ˜¯å¦å¯ä»¥é€šè¿‡ä¸Šè¿°å·¥å…·ç›´æ¥å›ç­”é—®é¢˜ (can_answer_directly)
3. å¦‚æœå¯ä»¥ï¼Œå…·ä½“ä½¿ç”¨å“ªä¸ªå·¥å…· (tool_to_use)
4. å·¥å…·çš„å‚æ•°æ˜¯ä»€ä¹ˆ (tool_params)
5. æä¾›è¯¦ç»†çš„è¯­ä¹‰ç†è§£åˆ¤æ–­ç†ç”± (reasoning)

**è¿”å›JSONæ ¼å¼ï¼š**
{{
    "can_answer_directly": true/false,
    "tool_to_use": "file_statistics/file_list/file_summary/none",
    "tool_params": {{"knowledge_id": "xxx"}} æˆ– {{"file_name": "xxx"}},
    "reasoning": "åŸºäºè¯­ä¹‰ç†è§£çš„è¯¦ç»†åˆ¤æ–­ç†ç”±ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆé€‚ç”¨æˆ–ä¸é€‚ç”¨å·¥å…·"
}}"""
            )

            try:
                chain = analysis_prompt | self.llm | JsonOutputParser()
                analysis_result = chain.invoke({
                    "query": query,
                    "main_intent": intent_analysis.get("main_intent", ""),
                    "query_type": intent_analysis.get("query_type", ""),
                    "keywords": intent_analysis.get("keywords", [])
                })
                return analysis_result
            except Exception as json_error:
                print(f"âŒ JSONè§£æå¤±è´¥: {json_error}")
                # è¿”å›ä¿å®ˆçš„é»˜è®¤å€¼
                return {
                    "can_answer_directly": False,
                    "tool_to_use": "none",
                    "tool_params": {},
                    "reasoning": f"JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ: {str(json_error)}"
                }

        except Exception as e:
            print(f"âŒ å·¥å…·åˆ¤æ–­å¤±è´¥: {e}")
            return {
                "can_answer_directly": False,
                "tool_to_use": "none",
                "tool_params": {},
                "reasoning": f"åˆ†æå¤±è´¥: {str(e)}"
            }

    def execute_tool(self, tool_name: str, tool_params: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒæŒ‡å®šçš„å·¥å…·

        Args:
            tool_name: å·¥å…·åç§°
            tool_params: å·¥å…·å‚æ•°

        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            if tool_name == "file_statistics":
                knowledge_id = tool_params.get("knowledge_id", "")
                return file_statistics_impl(knowledge_id)
            elif tool_name == "file_list":
                knowledge_id = tool_params.get("knowledge_id", "")
                return file_list_impl(knowledge_id)
            elif tool_name == "file_summary":
                file_name = tool_params.get("file_name", "")
                return file_summary_impl(file_name)
            else:
                return {"error": f"æœªçŸ¥å·¥å…·: {tool_name}", "tool_result": False}

        except Exception as e:
            print(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
            return {"error": f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}", "tool_result": False}

    def _convert_enhanced_intent_to_search_intent(self, enhanced_intent_result: Dict[str, Any], query: str) -> Dict[str, Any]:
        """å°†å¢å¼ºçš„æ„å›¾è¯†åˆ«ç»“æœè½¬æ¢ä¸ºæœç´¢æ‰€éœ€çš„æ„å›¾åˆ†ææ ¼å¼
        
        Args:
            enhanced_intent_result: å¢å¼ºçš„æ„å›¾è¯†åˆ«ç»“æœï¼ˆæ¥è‡ª EnhancedIntentAgentï¼‰
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            æœç´¢æ‰€éœ€çš„æ„å›¾åˆ†ææ ¼å¼
        """
        # æå–è¯­ä¹‰æçº¯åçš„æŸ¥è¯¢
        semantic_purified_query = enhanced_intent_result.get("semantic_purified_query", query)
        core_intent = enhanced_intent_result.get("core_intent", "")
        entities = enhanced_intent_result.get("entities", [])
        relationships = enhanced_intent_result.get("relationships", [])
        
        # æ„å»ºæœç´¢æ‰€éœ€çš„æ„å›¾åˆ†ææ ¼å¼
        intent_analysis = {
            "main_intent": core_intent or semantic_purified_query or query,
            "query_type": "factual",  # é»˜è®¤ç±»å‹ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
            "keywords": entities + [semantic_purified_query] if semantic_purified_query != query else entities,
            "entities": entities,
            "relationships": relationships,
            "search_strategy": "åŸºäºè¯­ä¹‰æçº¯çš„æ™ºèƒ½æœç´¢",
            "complexity": "medium",  # é»˜è®¤å¤æ‚åº¦
            # ä¿ç•™å¢å¼ºæ„å›¾è¯†åˆ«çš„å…¶ä»–ä¿¡æ¯
            "semantic_purified_query": semantic_purified_query,
            "core_intent": core_intent,
            "attributes": enhanced_intent_result.get("attributes", []),
            "mathematical_logic": enhanced_intent_result.get("mathematical_logic", []),
            "logical_relations": enhanced_intent_result.get("logical_relations", []),
            "set_theory_relations": enhanced_intent_result.get("set_theory_relations", []),
            "relational_algebra": enhanced_intent_result.get("relational_algebra", []),
            "graph_theory_relations": enhanced_intent_result.get("graph_theory_relations", []),
        }
        
        return intent_analysis
    
    def analyze_intent(self, query: str) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·æ„å›¾å’Œqueryæ‹†è§£

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            æ„å›¾åˆ†æç»“æœ
        """
        prompt = ChatPromptTemplate.from_template(
            """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æä¸“å®¶ã€‚è¯·å¯¹ç”¨æˆ·çš„æŸ¥è¯¢è¿›è¡Œæ·±å…¥åˆ†æã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æå¹¶ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼š
1. ä¸»è¦æ„å›¾ (main_intent)ï¼šç”¨æˆ·æƒ³è¦åšä»€ä¹ˆ
2. æŸ¥è¯¢ç±»å‹ (query_type)ï¼šfactual(äº‹å®), explanatory(è§£é‡Š), comparative(æ¯”è¾ƒ), procedural(è¿‡ç¨‹)
3. å…³é”®è¯åˆ—è¡¨ (keywords)ï¼šé‡è¦çš„å…³é”®è¯
4. å¯èƒ½çš„å®ä½“ (entities)ï¼šäººåã€åœ°åã€ç»„ç»‡åã€æŠ€æœ¯åè¯ç­‰ï¼ˆå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºæ•°ç»„ï¼‰
5. æœç´¢ç­–ç•¥å»ºè®® (search_strategy)ï¼šéœ€è¦ä»€ä¹ˆç±»å‹çš„æœç´¢
6. å¤æ‚åº¦è¯„ä¼° (complexity)ï¼šsimple, medium, complex

é‡è¦ï¼šè¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ å¤šä½™çš„å­—æ®µæˆ–å€¼ã€‚

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
{{
    "main_intent": "è·å–çŸ¥è¯†åº“æ–‡ä»¶æ•°é‡",
    "query_type": "factual",
    "keywords": ["çŸ¥è¯†åº“", "æ–‡ä»¶", "æ•°é‡"],
    "entities": [],
    "search_strategy": "æŸ¥è¯¢å…ƒæ•°æ®",
    "complexity": "simple"
}}"""
        )

        try:
            chain = prompt | self.llm | JsonOutputParser()
            result = chain.invoke({"query": query})
            return result
        except Exception as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return {
                "main_intent": f"åˆ†ææŸ¥è¯¢ï¼š{query}",
                "query_type": "factual",
                "keywords": [query.split()[0] if query.split() else "æŸ¥è¯¢"],
                "entities": [],
                "search_strategy": "é€šç”¨æœç´¢",
                "complexity": "medium"
            }

    def expand_entities(self, intent_analysis: Dict[str, Any], num_expansions: int = 3) -> List[str]:
        """åŸºäºæ„å›¾åˆ†æè¿›è¡Œè¯­ä¹‰å®ä½“æ‰©å±•

        Args:
            intent_analysis: æ„å›¾åˆ†æç»“æœ
            num_expansions: æ‰©å±•æŸ¥è¯¢æ•°é‡

        Returns:
            æ‰©å±•æŸ¥è¯¢åˆ—è¡¨
        """
        entities = intent_analysis.get("entities", [])
        keywords = intent_analysis.get("keywords", [])
        query_type = intent_analysis.get("query_type", "factual")

        prompt = ChatPromptTemplate.from_template(
            """åŸºäºç”¨æˆ·çš„æ„å›¾åˆ†æï¼Œè¿›è¡Œè¯­ä¹‰å®ä½“æ‰©å±•ï¼Œç”Ÿæˆå¤šä¸ªç›¸å…³çš„æœç´¢æŸ¥è¯¢ã€‚

åŸå§‹æ„å›¾åˆ†æï¼š
- ä¸»è¦æ„å›¾ï¼š{main_intent}
- æŸ¥è¯¢ç±»å‹ï¼š{query_type}
- å…³é”®è¯ï¼š{keywords}
- å®ä½“ï¼š{entities}
- æœç´¢ç­–ç•¥ï¼š{search_strategy}
- å¤æ‚åº¦ï¼š{complexity}

è¯·ç”Ÿæˆ{num_expansions}ä¸ªä¸åŒçš„æ‰©å±•æŸ¥è¯¢ï¼Œè¿™äº›æŸ¥è¯¢åº”è¯¥ï¼š
1. åŒ…å«è¯­ä¹‰ç›¸å…³çš„å®ä½“å’Œå…³é”®è¯
2. è¦†ç›–ä¸åŒçš„æœç´¢è§’åº¦
3. é€‚åˆåŒå¼•æ“æœç´¢ï¼ˆMilvus + Elasticsearchï¼‰
4. æ¯ä¸ªæŸ¥è¯¢éƒ½ä»¥å­—ç¬¦ä¸²å½¢å¼è¿”å›

è¿”å›JSONæ ¼å¼ï¼š
{{
    "expanded_queries": [
        "æ‰©å±•æŸ¥è¯¢1",
        "æ‰©å±•æŸ¥è¯¢2",
        "æ‰©å±•æŸ¥è¯¢3"
    ]
}}"""
        )

        chain = prompt | self.llm | JsonOutputParser()
        result = chain.invoke({
            "main_intent": intent_analysis.get("main_intent", ""),
            "query_type": query_type,
            "keywords": keywords,
            "entities": entities,
            "search_strategy": intent_analysis.get("search_strategy", ""),
            "complexity": intent_analysis.get("complexity", "medium"),
            "num_expansions": num_expansions
        })

        return result.get("expanded_queries", [])

    def search_milvus(self, state: IntentSearchState, query_text: str) -> List[Dict[str, Any]]:
        """åœ¨Milvusä¸­æœç´¢ç›¸å…³å†…å®¹ï¼ˆä»£ç†æ–¹æ³•ï¼Œå®é™…å®ç°åœ¨ control_search.pyï¼‰

        Args:
            state: æ™ºèƒ½ä½“çŠ¶æ€
            query_text: æœç´¢æŸ¥è¯¢æ–‡æœ¬

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        param = {
            "query": query_text,
            "knowledge_id": state["knowledge_id"],
            "user_id": state.get("user_id", ""),
            "top_k": 10
        }
        return self.search_obj.search_milvus_formatted(param)

    def search_elasticsearch(self, state: IntentSearchState, query_text: str) -> List[Dict[str, Any]]:
        """åœ¨Elasticsearchä¸­æœç´¢ç›¸å…³å†…å®¹ï¼ˆä»£ç†æ–¹æ³•ï¼Œå®é™…å®ç°åœ¨ control_search.pyï¼‰

        Args:
            state: æ™ºèƒ½ä½“çŠ¶æ€
            query_text: æœç´¢æŸ¥è¯¢æ–‡æœ¬

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        param = {
            "query": query_text,
            "knowledge_id": state["knowledge_id"],
            "user_id": state.get("user_id", ""),
            "flag": state["flag"],
            "size": 10
        }
        return self.search_obj.query_elasticsearch(param)

    def search_graph_data(self, state: IntentSearchState, query_text: str) -> List[Dict[str, Any]]:
        """åœ¨å›¾æ•°æ®ä¸­æœç´¢ç›¸å…³å†…å®¹ï¼ˆä»£ç†æ–¹æ³•ï¼Œå®é™…å®ç°åœ¨ control_search.pyï¼‰

        Args:
            state: æ™ºèƒ½ä½“çŠ¶æ€
            query_text: æœç´¢æŸ¥è¯¢æ–‡æœ¬

        Returns:
            å›¾æ•°æ®æœç´¢ç»“æœåˆ—è¡¨
        """
        param = {
            "query": query_text,
            "knowledge_id": state["knowledge_id"],
            "user_id": state.get("user_id", "")
        }
        return self.search_obj.search_graph_data(param)

    def search_triple_engines_with_graph(self, state: IntentSearchState, query_text: str) -> List[Dict[str, Any]]:
        """åŒæ—¶ä½¿ç”¨ Milvusã€Elasticsearch å’Œå›¾æ•°æ®è¿›è¡Œä¸‰å¼•æ“æœç´¢

        Args:
            state: æ™ºèƒ½ä½“çŠ¶æ€
            query_text: æœç´¢æŸ¥è¯¢æ–‡æœ¬

        Returns:
            åˆå¹¶çš„æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            print("ğŸ” æ‰§è¡Œä¸‰å¼•æ“å¹¶è¡Œæœç´¢ï¼ˆåŒ…å«å›¾æ•°æ®ï¼‰...")

            # ä½¿ç”¨ ThreadPoolExecutor å¹¶è¡Œæ‰§è¡Œä¸‰ä¸ªæœç´¢å¼•æ“
            with ThreadPoolExecutor(max_workers=3) as executor:
                milvus_future = executor.submit(self.search_milvus, state, query_text)
                elastic_future = executor.submit(self.search_elasticsearch, state, query_text)
                graph_future = executor.submit(self.search_graph_data, state, query_text)

                # ç­‰å¾…ç»“æœ
                milvus_results = milvus_future.result()
                elastic_results = elastic_future.result()
                graph_results = graph_future.result()

            # åˆå¹¶ç»“æœ
            combined_results = milvus_results + elastic_results + graph_results

            # ç»Ÿä¸€å¾—åˆ†å¤„ç†
            for result in combined_results:
                if result.get("search_engine") == "elasticsearch":
                    result["combined_score"] = result.get("_score", 0) * 10  # æ–‡æœ¬æœç´¢æƒé‡æœ€é«˜
                elif result.get("search_engine") == "graph_data":
                    result["combined_score"] = result.get("score", 0) * 12  # å›¾æ•°æ®æƒé‡æœ€é«˜ï¼Œå› ä¸ºåŒ…å«å…³ç³»ä¿¡æ¯
                else:
                    result["combined_score"] = result.get("score", 0)  # Milvusä¿æŒåŸæƒé‡

            # æŒ‰ç»„åˆå¾—åˆ†æ’åº
            combined_results.sort(key=lambda x: x.get("combined_score", 0), reverse=True)

            # é™åˆ¶ç»“æœæ•°é‡
            max_results = 10  # å¢åŠ ç»“æœæ•°é‡ä»¥é€‚åº”ä¸‰å¼•æ“
            combined_results = combined_results[:max_results]

            print(f"âœ… ä¸‰å¼•æ“æœç´¢å®Œæˆï¼Œè·å¾— {len(combined_results)} ä¸ªç»“æœ")
            print(f"   - Milvus: {len(milvus_results)} ä¸ªç»“æœ")
            print(f"   - Elasticsearch: {len(elastic_results)} ä¸ªç»“æœ")
            print(f"   - å›¾æ•°æ®: {len(graph_results)} ä¸ªç»“æœ")

            return combined_results

        except Exception as e:
            print(f"âŒ ä¸‰å¼•æ“æœç´¢å¤±è´¥: {e}ï¼Œå›é€€åˆ°åŒå¼•æ“æœç´¢")
            # å›é€€åˆ°åŒå¼•æ“æœç´¢
            try:
                with ThreadPoolExecutor(max_workers=2) as executor:
                    milvus_future = executor.submit(self.search_milvus, state, query_text)
                    elastic_future = executor.submit(self.search_elasticsearch, state, query_text)

                    milvus_results = milvus_future.result()
                    elastic_results = elastic_future.result()

                combined_results = milvus_results + elastic_results
                for result in combined_results:
                    if result.get("search_engine") == "elasticsearch":
                        result["combined_score"] = result.get("_score", 0) * 10
                    else:
                        result["combined_score"] = result.get("score", 0)

                combined_results.sort(key=lambda x: x.get("combined_score", 0), reverse=True)
                return combined_results[:30]

            except Exception as fallback_e:
                print(f"âŒ å›é€€æœç´¢ä¹Ÿå¤±è´¥: {fallback_e}ï¼Œä½¿ç”¨Milvuså•å¼•æ“")
                return self.search_milvus(state, query_text)

    def merge_search_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶å’Œå»é‡æœç´¢ç»“æœ

        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨

        Returns:
            å»é‡å¹¶æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        if not results:
            return []

        seen = set()
        unique_results = []

        for result in results:
            # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼ŒåŸºäºå†…å®¹å’Œæ ‡é¢˜
            content_hash = hash((result.get("content", "") + result.get("title", "")).strip())
            if content_hash not in seen:
                seen.add(content_hash)
                unique_results.append(result)

        # æŒ‰å¾—åˆ†æ’åº
        unique_results.sort(key=lambda x: x.get("score", x.get("_score", 0)), reverse=True)
        return unique_results

    def search_only(self, query: str, knowledge_id: str, user_id: str = "", flag: bool = True, 
                   intent_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """çº¯æœç´¢åŠŸèƒ½ï¼šåªè¿›è¡Œæœç´¢ï¼Œä¸è¿›è¡Œæ„å›¾è¯†åˆ«å’Œå·¥å…·åˆ¤æ–­
        
        æ³¨æ„ï¼šæ„å›¾è¯†åˆ«å’Œå·¥å…·åˆ¤æ–­å·²åœ¨å¤–éƒ¨å®Œæˆï¼ˆenhanced_intent_agent å’Œ tool_agentï¼‰
        å›¾æ•°æ®é€šè¿‡ä¸‰å¼•æ“æœç´¢å†…éƒ¨è·å–ï¼Œæ— éœ€å¤–éƒ¨ä¼ å…¥

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            knowledge_id: çŸ¥è¯†åº“ID
            user_id: ç”¨æˆ·ID
            flag: æƒé™æ ‡å¿—
            intent_result: æ„å›¾è¯†åˆ«ç»“æœï¼Œç”¨äºæŒ‡å¯¼æœç´¢

        Returns:
            æœç´¢ç»“æœå’Œæ„å›¾åˆ†æ
        """
        try:
            # åˆå§‹åŒ–çŠ¶æ€
            state: IntentSearchState = {
                "question": query,
                "knowledge_id": knowledge_id,
                "user_id": user_id,
                "intent_analysis": {},
                "search_results": [],
                "expanded_queries": [],
                "flag": flag
            }

            # æ­¥éª¤1: æ„å›¾æ•´ç†ï¼ˆå°†ä¼ å…¥çš„æ„å›¾ç»“æœè½¬æ¢ä¸ºæœç´¢æ‰€éœ€çš„æ ¼å¼ï¼‰
            if intent_result:
                print("ğŸ“‹ æ­¥éª¤1: æ•´ç†æ„å›¾è¯†åˆ«ç»“æœç”¨äºæœç´¢")
                # å°†å¢å¼ºçš„æ„å›¾è¯†åˆ«ç»“æœè½¬æ¢ä¸ºæœç´¢æ‰€éœ€çš„æ ¼å¼
                intent_analysis = self._convert_enhanced_intent_to_search_intent(intent_result, query)
                state["intent_analysis"] = intent_analysis
                print(f"âœ… æ„å›¾æ•´ç†å®Œæˆ: {intent_analysis.get('main_intent', 'æœªçŸ¥æ„å›¾')}")
                print(f"âœ… è¯­ä¹‰æçº¯æŸ¥è¯¢: {intent_result.get('semantic_purified_query', query)}")
            else:
                # å¦‚æœæ²¡æœ‰æä¾›æ„å›¾ç»“æœï¼Œåˆ›å»ºåŸºæœ¬çš„æ„å›¾åˆ†æ
                print("ğŸ“‹ æ­¥éª¤1: åˆ›å»ºåŸºæœ¬æ„å›¾åˆ†æ")
                intent_analysis = {
                    "main_intent": query,
                    "query_type": "factual",
                    "keywords": query.split(),
                    "entities": [],
                    "relationships": [],
                    "search_strategy": "é€šç”¨æœç´¢",
                    "complexity": "medium",
                    "semantic_purified_query": query
                }
                state["intent_analysis"] = intent_analysis

            # æ­¥éª¤2: åŸºäºæ„å›¾è¿›è¡Œåˆå§‹ä¸‰å¼•æ“æœç´¢ï¼ˆåŒ…å«å›¾æ•°æ®ï¼‰
            print("ğŸ” æ­¥éª¤2: æ‰§è¡Œåˆå§‹ä¸‰å¼•æ“æœç´¢ï¼ˆMilvus + Elasticsearch + Graphï¼‰")
            # å¦‚æœæä¾›äº†æ„å›¾è¯†åˆ«ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨è¯­ä¹‰æçº¯åçš„æŸ¥è¯¢
            search_query = query
            if intent_result:
                semantic_purified_query = intent_result.get("semantic_purified_query", query)
                if semantic_purified_query and semantic_purified_query != query:
                    search_query = semantic_purified_query
                    print(f"ğŸ” ä½¿ç”¨è¯­ä¹‰æçº¯åçš„æŸ¥è¯¢è¿›è¡Œæœç´¢: {search_query}")
            
            initial_results = self.search_triple_engines_with_graph(state, search_query)
            print(f"ğŸ“Š åˆå§‹æœç´¢è·å¾— {len(initial_results)} ä¸ªç»“æœ")

            # æ­¥éª¤3: åŸºäºæ„å›¾åˆ†æè¿›è¡Œæ‰©å±•æœç´¢
            print("ğŸš€ æ­¥éª¤3: æ‰§è¡Œæ‰©å±•æœç´¢")
            expanded_results = []
            expanded_queries = []
            
            # æ ¹æ®æ„å›¾åˆ†æçš„å¤æ‚åº¦å†³å®šæ˜¯å¦è¿›è¡Œæ‰©å±•æœç´¢
            complexity = intent_analysis.get("complexity", "medium")
            entities = intent_analysis.get("entities", [])
            
            # å¦‚æœæœ‰å®ä½“æˆ–å¤æ‚åº¦ä¸æ˜¯simpleï¼Œè¿›è¡Œæ‰©å±•æœç´¢
            if entities or complexity != "simple":
                try:
                    # ç”Ÿæˆæ‰©å±•æŸ¥è¯¢ï¼ˆç”Ÿæˆ1-2ä¸ªæ‰©å±•æŸ¥è¯¢ï¼‰
                    num_expansions = 1 if complexity == "medium" else 2
                    expanded_queries = self.expand_entities(intent_analysis, num_expansions=num_expansions)
                    state["expanded_queries"] = expanded_queries
                    print(f"âš¡ ç”Ÿæˆ {len(expanded_queries)} ä¸ªæ‰©å±•æŸ¥è¯¢")
                    
                    # å¯¹æ¯ä¸ªæ‰©å±•æŸ¥è¯¢è¿›è¡Œä¸‰å¼•æ“æœç´¢
                    for i, expanded_query in enumerate(expanded_queries):
                        print(f"  - æ‰©å±•æœç´¢ {i+1}: {expanded_query[:50]}...")
                        results = self.search_triple_engines_with_graph(state, expanded_query)
                        expanded_results.extend(results)
                    
                    print(f"ğŸ“ˆ æ‰©å±•æœç´¢è·å¾— {len(expanded_results)} ä¸ªç»“æœ")
                except Exception as e:
                    print(f"âš ï¸ æ‰©å±•æœç´¢å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨åˆå§‹æœç´¢ç»“æœ")
                    expanded_queries = []
                    expanded_results = []
            else:
                print("â„¹ï¸ æŸ¥è¯¢å¤æ‚åº¦ä¸ºsimpleä¸”æ— å®ä½“ï¼Œè·³è¿‡æ‰©å±•æœç´¢")

            # æ­¥éª¤4: åˆå¹¶æ‰€æœ‰æœç´¢ç»“æœ
            print("ğŸ”€ æ­¥éª¤4: åˆå¹¶æœç´¢ç»“æœ")
            all_results = initial_results + expanded_results

            # å»é‡å’Œæ’åº
            all_results = self.merge_search_results(all_results)
            print(f"ğŸ‰ æœ€ç»ˆè·å¾— {len(all_results)} ä¸ªå»é‡åçš„ç»“æœ")

            return {
                "success": True,
                "intent_analysis": intent_analysis,
                "search_results": all_results,
                "initial_results_count": len(initial_results),
                "expanded_results_count": len(expanded_results),
                "total_results_count": len(all_results),
                "query": query,
                "knowledge_id": knowledge_id,
                "expanded_queries": expanded_queries
            }

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "knowledge_id": knowledge_id,
                "search_results": []
            }
    
    def search_with_intent(self, query: str, knowledge_id: str, user_id: str = "", flag: bool = True, 
                          graph_data: List[List[Dict[str, Any]]] = None, 
                          intent_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """åŸºäºç”¨æˆ·æ„å›¾çš„æ™ºèƒ½æœç´¢ï¼ˆä¿ç•™åŸæ–¹æ³•ä»¥å…¼å®¹æ—§ä»£ç ï¼‰

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            knowledge_id: çŸ¥è¯†åº“ID
            user_id: ç”¨æˆ·ID
            flag: æƒé™æ ‡å¿—
            graph_data: å›¾æ•°æ®ï¼ŒåŒ…å«èŠ‚ç‚¹å…³ç³»ä¿¡æ¯
            intent_result: å¯é€‰çš„æ„å›¾è¯†åˆ«ç»“æœï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨å®ƒè€Œä¸æ˜¯é‡æ–°åˆ†æ

        Returns:
            æœç´¢ç»“æœå’Œæ„å›¾åˆ†æ
        """
        # ç›´æ¥è°ƒç”¨ search_onlyï¼Œå› ä¸ºæ„å›¾è¯†åˆ«å’Œå·¥å…·åˆ¤æ–­å·²åœ¨å¤–éƒ¨å®Œæˆ
        return self.search_only(query, knowledge_id, user_id, flag, graph_data, intent_result)

# ============================================================================
# IntentRecognitionAgent å®ä¾‹åŒ–å‡½æ•°
# ============================================================================

def create_intent_recognition_agent() -> IntentRecognitionAgent:
    """åˆ›å»ºç”¨æˆ·æ„å›¾è¯†åˆ«å’Œæœç´¢æ™ºèƒ½ä½“å®ä¾‹

    Returns:
        IntentRecognitionAgentå®ä¾‹
    """
    return IntentRecognitionAgent()

def run_intent_based_search(query: str, knowledge_id: str, user_id: str = "", flag: bool = True, 
                            intent_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """è¿è¡ŒåŸºäºç”¨æˆ·æ„å›¾çš„æ™ºèƒ½æœç´¢
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°åªè¿›è¡Œæœç´¢ï¼Œä¸è¿›è¡Œæ„å›¾è¯†åˆ«å’Œå·¥å…·åˆ¤æ–­ã€‚
    æ„å›¾è¯†åˆ«å’Œå·¥å…·åˆ¤æ–­å·²åœ¨å¤–éƒ¨å®Œæˆï¼ˆenhanced_intent_agent å’Œ tool_agentï¼‰ã€‚
    å›¾æ•°æ®é€šè¿‡ä¸‰å¼•æ“æœç´¢å†…éƒ¨è·å–ï¼Œæ— éœ€å¤–éƒ¨ä¼ å…¥ã€‚

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        knowledge_id: çŸ¥è¯†åº“ID
        user_id: ç”¨æˆ·ID
        flag: æƒé™æ ‡å¿—
        intent_result: å¯é€‰çš„æ„å›¾è¯†åˆ«ç»“æœï¼Œç”¨äºæŒ‡å¯¼æœç´¢

    Returns:
        æœç´¢ç»“æœå’Œæ„å›¾åˆ†æ
    """
    agent = create_intent_recognition_agent()
    return agent.search_only(query, knowledge_id, user_id, flag, intent_result)
