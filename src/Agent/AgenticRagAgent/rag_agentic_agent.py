import os
import re
from typing import Dict, Any, List, Generator, Optional
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models.tongyi import ChatTongyi
from Config.llm_config import get_chat_tongyi

# ============================================================================
# å¤§æ¨¡å‹é…ç½®
# ============================================================================

llm_stream = get_chat_tongyi(temperature=0.7, streaming=True, enable_thinking=False)

def rag_stream(query, intent_analysis, search_results, search_content, system_prompt=None):
    # æ­¥éª¤3: åˆ†ææœç´¢ç»“æœç»Ÿè®¡ä¿¡æ¯
    engine_stats = {}
    milvus_docs = []
    elastic_docs = []
    graph_docs = []

    for result in search_results:
        engine = result.get("search_engine", "unknown")
        engine_stats[engine] = engine_stats.get(engine, 0) + 1

        # åˆ†ç±»å­˜å‚¨ä¸åŒå¼•æ“çš„ç»“æœ
        if engine == "milvus":
            milvus_docs.append(result)
        elif engine == "elasticsearch":
            elastic_docs.append(result)
        elif engine == "graph_data":
            graph_docs.append(result)

    # æ„å»ºå¢å¼ºçš„æœç´¢ç»“æœç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸æ˜¾ç¤ºå…·ä½“æœç´¢å¼•æ“ï¼‰
    search_stats = f"""
æœç´¢ç»“æœç»Ÿè®¡ï¼š
- æ€»ç»“æœæ•°ï¼š{len(search_results)} ä¸ª
- ä»å¤šä¸ªæœç´¢å¼•æ“æœåˆ°çš„çŸ¥è¯†
"""

    # æ­¥éª¤4: æ„å»ºRAGæç¤º
    # å¦‚æœæä¾›äº†åŠ¨æ€ System Promptï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤æ¨¡æ¿
    if system_prompt:
        # ä½¿ç”¨åŠ¨æ€ System Prompt
        prompt_template = system_prompt + """

## æœç´¢ç»“æœç»Ÿè®¡
{search_stats}

## æœç´¢ç»“æœå†…å®¹ï¼ˆä»å¤šä¸ªæœç´¢å¼•æ“æœåˆ°çš„çŸ¥è¯†ï¼‰
{search_results}

## æ„å›¾åˆ†æ
- ä¸»è¦æ„å›¾ï¼š{main_intent}
- æŸ¥è¯¢ç±»å‹ï¼š{query_type}
- å…³é”®è¯ï¼š{keywords}
- å®ä½“ï¼š{entities}
- æœç´¢ç­–ç•¥ï¼š{search_strategy}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯æä¾›å‡†ç¡®ã€å…¨é¢ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚å›ç­”è¦ï¼š

å†…å®¹å¤„ç†åŸåˆ™ï¼š
1. **ç›´æ¥é’ˆå¯¹ç”¨æˆ·é—®é¢˜**ï¼Œå……åˆ†åˆ©ç”¨ä»å¤šä¸ªæœç´¢å¼•æ“æœåˆ°çš„çŸ¥è¯†
2. **ä¼˜å…ˆè€ƒè™‘å®ä½“å…³ç³»ä¿¡æ¯**ï¼Œè¿™äº›å¾€å¾€åŒ…å«å…³é”®çš„ä¸Šä¸‹æ–‡å’Œå…³è”
3. **åª’ä½“å†…å®¹å±•ç¤º**ï¼š
   - å¦‚æœæœç´¢ç»“æœä¸­åŒ…å«å›¾ç‰‡URLï¼Œç›´æ¥æ˜¾ç¤ºå›¾ç‰‡URLï¼ˆæ¯è¡Œä¸€ä¸ªURLï¼‰
   - å¦‚æœæœç´¢ç»“æœä¸­åŒ…å«è¡¨æ ¼æ•°æ®ï¼Œè¯·è¯´æ˜"ç›¸å…³å†…å®¹åŒ…å«è¡¨æ ¼æ•°æ®"
   - å¯¹äºå›¾ç‰‡ï¼Œç›´æ¥è¾“å‡ºURLï¼Œä¸éœ€è¦æ·»åŠ é¢å¤–çš„æç¤ºä¿¡æ¯
   - å¯¹äºè¡¨æ ¼ï¼Œæ€»ç»“è¡¨æ ¼ä¸­çš„å…³é”®ä¿¡æ¯

å›ç­”ç»“æ„ï¼š
4. **å®ä½“å…³ç³»ä¼˜å…ˆ**ï¼šé¦–å…ˆåŸºäºå®ä½“å…³ç³»å»ºç«‹ç­”æ¡ˆæ¡†æ¶
5. **å¤šæºä¿¡æ¯æ•´åˆ**ï¼šç»“åˆä»å¤šä¸ªæœç´¢å¼•æ“æœåˆ°çš„çŸ¥è¯†çš„äº’è¡¥ä¼˜åŠ¿
6. **åª’ä½“å†…å®¹é›†æˆ**ï¼šåœ¨ç›¸å…³ä½ç½®è‡ªç„¶èå…¥å›¾ç‰‡URLå’Œè¡¨æ ¼ä¿¡æ¯çš„æè¿°
7. **é€»è¾‘æ¸…æ™°åˆç†**ï¼šå¦‚æœä¿¡æ¯ä¸è¶³æ˜ç¡®è¯´æ˜ï¼Œå¼•å¯¼ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯

å›ç­”ï¼š"""
    else:
        # ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·åŸºäºå¤šå¼•æ“æœç´¢ç»“æœï¼ˆåŒ…æ‹¬å›¾æ•°æ®ï¼‰å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

æ„å›¾åˆ†æï¼š
- ä¸»è¦æ„å›¾ï¼š{main_intent}
- æŸ¥è¯¢ç±»å‹ï¼š{query_type}
- å…³é”®è¯ï¼š{keywords}
- å®ä½“ï¼š{entities}
- æœç´¢ç­–ç•¥ï¼š{search_strategy}

{search_stats}

æœç´¢ç»“æœï¼ˆä»å¤šä¸ªæœç´¢å¼•æ“æœåˆ°çš„çŸ¥è¯†ï¼‰ï¼š
{search_results}

é‡è¦è¯´æ˜ï¼š
- å›¾æ•°æ®åŒ…å«å®ä½“é—´çš„å…³ç³»ä¿¡æ¯ï¼Œè¿™å¯¹äºç†è§£å¤æ‚æ¦‚å¿µéå¸¸æœ‰ä»·å€¼
- å¦‚æœæœç´¢ç»“æœæåˆ°åŒ…å«å›¾ç‰‡æˆ–è¡¨æ ¼ï¼Œè¯·åœ¨å›ç­”ä¸­é€‚å½“æåŠ
- å®ä½“å…³ç³»å¯ä»¥æä¾›æ›´æ·±çš„ä¸Šä¸‹æ–‡ç†è§£

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯æä¾›å‡†ç¡®ã€å…¨é¢ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚å›ç­”è¦ï¼š

å†…å®¹å¤„ç†åŸåˆ™ï¼š
1. **ç›´æ¥é’ˆå¯¹ç”¨æˆ·é—®é¢˜**ï¼Œå……åˆ†åˆ©ç”¨ä»å¤šä¸ªæœç´¢å¼•æ“æœåˆ°çš„çŸ¥è¯†
2. **ä¼˜å…ˆè€ƒè™‘å®ä½“å…³ç³»ä¿¡æ¯**ï¼Œè¿™äº›å¾€å¾€åŒ…å«å…³é”®çš„ä¸Šä¸‹æ–‡å’Œå…³è”
3. **åª’ä½“å†…å®¹å±•ç¤º**ï¼š
   - å¦‚æœæœç´¢ç»“æœä¸­åŒ…å«å›¾ç‰‡URLï¼Œç›´æ¥æ˜¾ç¤ºå›¾ç‰‡URLï¼ˆæ¯è¡Œä¸€ä¸ªURLï¼‰
   - å¦‚æœæœç´¢ç»“æœä¸­åŒ…å«è¡¨æ ¼æ•°æ®ï¼Œè¯·è¯´æ˜"ç›¸å…³å†…å®¹åŒ…å«è¡¨æ ¼æ•°æ®"
   - å¯¹äºå›¾ç‰‡ï¼Œç›´æ¥è¾“å‡ºURLï¼Œä¸éœ€è¦æ·»åŠ é¢å¤–çš„æç¤ºä¿¡æ¯
   - å¯¹äºè¡¨æ ¼ï¼Œæ€»ç»“è¡¨æ ¼ä¸­çš„å…³é”®ä¿¡æ¯

å›ç­”ç»“æ„ï¼š
4. **å®ä½“å…³ç³»ä¼˜å…ˆ**ï¼šé¦–å…ˆåŸºäºå®ä½“å…³ç³»å»ºç«‹ç­”æ¡ˆæ¡†æ¶
5. **å¤šæºä¿¡æ¯æ•´åˆ**ï¼šç»“åˆä»å¤šä¸ªæœç´¢å¼•æ“æœåˆ°çš„çŸ¥è¯†çš„äº’è¡¥ä¼˜åŠ¿
6. **åª’ä½“å†…å®¹é›†æˆ**ï¼šåœ¨ç›¸å…³ä½ç½®è‡ªç„¶èå…¥å›¾ç‰‡URLå’Œè¡¨æ ¼ä¿¡æ¯çš„æè¿°
7. **é€»è¾‘æ¸…æ™°åˆç†**ï¼šå¦‚æœä¿¡æ¯ä¸è¶³æ˜ç¡®è¯´æ˜ï¼Œå¼•å¯¼ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯

å›ç­”ï¼š"""
    
    prompt = ChatPromptTemplate.from_template(prompt_template)

    # æ­¥éª¤4: ä½¿ç”¨æµå¼æ¨¡å‹ç”Ÿæˆå›ç­”
    print("ğŸ¯ å¼€å§‹ç”Ÿæˆæµå¼å›ç­”...")

    chain = prompt | llm_stream
    stream_response = chain.stream({
        "query": query,
        "main_intent": intent_analysis.get("main_intent", "æœªçŸ¥"),
        "query_type": intent_analysis.get("query_type", "æœªçŸ¥"),
        "keywords": ", ".join(intent_analysis.get("keywords", [])),
        "entities": ", ".join(intent_analysis.get("entities", [])),
        "search_strategy": intent_analysis.get("search_strategy", "æœªçŸ¥"),
        "search_stats": search_stats,
        "search_results": search_content if search_content else "æš‚æ— ç›¸å…³æœç´¢ç»“æœ"
    })

    # æ­¥éª¤5: æµå¼è¾“å‡ºç»“æœ
    chunk_index = 0
    for chunk in stream_response:
        chunk_index += 1

        # å¤„ç†chunkå†…å®¹
        if hasattr(chunk, 'content'):
            content = chunk.content
        else:
            content = str(chunk)

        if content.strip():  # åªè¾“å‡ºéç©ºå†…å®¹
            # æå–å›¾æ•°æ®ä¸­çš„å›¾ç‰‡å’Œè¡¨æ ¼ä¿¡æ¯
            media_info = extract_images_and_tables_from_graph(graph_docs) if graph_docs else {"images": [], "tables": []}

            yield {
                "id": f"rag_chunk_{hash(query)}_{chunk_index}",
                "object": "chat.completion.chunk",
                "created": int(os.times()[4]),
                "model": "rag-agentic-model",
                "choices": [{
                    "index": 0,
                    "delta": {
                        "content": content,
                        "type": "text",
                        "intent_analysis": intent_analysis,
                        "search_results_count": len(search_results),
                        "search_engine_stats": engine_stats,
                        "milvus_results_count": len(milvus_docs),
                        "elasticsearch_results_count": len(elastic_docs),
                        "graph_results_count": len(graph_docs),
                        "media_info": media_info
                    },
                    "finish_reason": None
                }]
            }

    # å‘é€ç»“æŸæ ‡è®°
    yield {
        "id": f"rag_end_{hash(query)}",
        "object": "chat.completion.chunk",
        "created": int(os.times()[4]),
        "model": "rag-agentic-model",
        "choices": [{
            "index": 0,
            "delta": {},
            "finish_reason": "stop"
        }]
    }


def format_graph_data_for_display(graph_result: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–å›¾æ•°æ®ç»“æœç”¨äºæ˜¾ç¤º

    Args:
        graph_result: å›¾æ•°æ®æœç´¢ç»“æœ

    Returns:
        æ ¼å¼åŒ–çš„æ˜¾ç¤ºæ–‡æœ¬
    """
    try:
        graph_relation = graph_result.get("graph_relation", {})
        metadata = graph_result.get("metadata", {})

        formatted_text = f"ğŸ•¸ï¸ å›¾å…³ç³»ä¿¡æ¯ï¼š\n"

        # å…³ç³»ä¿¡æ¯
        relation = graph_relation.get("relation", {})
        if relation:
            formatted_text += f"ğŸ“‹ å…³ç³»æè¿°ï¼š{relation.get('description', 'æ— æè¿°')}\n"
            if relation.get('keywords'):
                formatted_text += f"ğŸ·ï¸ å…³é”®è¯ï¼š{relation.get('keywords')}\n"
            if relation.get('weight'):
                formatted_text += f"âš–ï¸ æƒé‡ï¼š{relation.get('weight')}\n"

        # èµ·å§‹èŠ‚ç‚¹
        start_node = graph_relation.get("start_node", {})
        if start_node:
            formatted_text += f"\nğŸ”µ èµ·å§‹èŠ‚ç‚¹ï¼š{start_node.get('entity_id', 'æœªçŸ¥')} ({start_node.get('entity_type', 'æœªçŸ¥ç±»å‹')})\n"
            desc = start_node.get('description', '')
            if desc:
                formatted_text += f"ğŸ“ æè¿°ï¼š{desc[:300]}{'...' if len(desc) > 300 else ''}\n"

            # å¤„ç†chunksä¸­çš„å›¾ç‰‡å’Œè¡¨æ ¼
            chunks = start_node.get('chunks', [])
            titles = start_node.get('titles', [])

            for j, chunk in enumerate(chunks[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªchunk
                if isinstance(chunk, str):
                    # æå–å›¾ç‰‡URLå¹¶ç›´æ¥è¾“å‡º
                    img_urls = []
                    # æå–<img>æ ‡ç­¾ä¸­çš„URL
                    img_matches = re.findall(r'<img[^>]+src=["\']([^"\']+)["\']', chunk, re.IGNORECASE)
                    img_urls.extend(img_matches)
                    # æå–HTTPå›¾ç‰‡é“¾æ¥
                    http_matches = re.findall(r'https?://[^\s]+\.(?:jpg|jpeg|png|gif|webp|bmp|svg)(?:\?[^\s]*)?', chunk, re.IGNORECASE)
                    img_urls.extend(http_matches)
                    
                    if img_urls:
                        # ç›´æ¥è¾“å‡ºå›¾ç‰‡URLï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸æ·»åŠ æç¤ºä¿¡æ¯
                        for img_url in img_urls:
                            formatted_text += f"{img_url}\n"
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼
                    elif '<table' in chunk or '<tr' in chunk:
                        formatted_text += f"ğŸ“Š åŒ…å«è¡¨æ ¼æ•°æ®\n"
                    else:
                        # æ˜¾ç¤ºæ–‡æœ¬å†…å®¹æ‘˜è¦
                        clean_chunk = re.sub(r'<[^>]+>', '', chunk)  # ç§»é™¤HTMLæ ‡ç­¾
                        if len(clean_chunk.strip()) > 50:
                            formatted_text += f"ğŸ“„ å†…å®¹ï¼š{clean_chunk.strip()[:200]}...\n"

                    # æ˜¾ç¤ºå¯¹åº”æ ‡é¢˜
                    if j < len(titles) and titles[j]:
                        formatted_text += f"ğŸ“– æ ‡é¢˜ï¼š{titles[j]}\n"

        # ç»“æŸèŠ‚ç‚¹
        end_node = graph_relation.get("end_node", {})
        if end_node:
            formatted_text += f"\nğŸ”´ ç»“æŸèŠ‚ç‚¹ï¼š{end_node.get('entity_id', 'æœªçŸ¥')} ({end_node.get('entity_type', 'æœªçŸ¥ç±»å‹')})\n"
            desc = end_node.get('description', '')
            if desc:
                formatted_text += f"ğŸ“ æè¿°ï¼š{desc[:300]}{'...' if len(desc) > 300 else ''}\n"

            # å¤„ç†chunksä¸­çš„å›¾ç‰‡å’Œè¡¨æ ¼ï¼ˆç±»ä¼¼èµ·å§‹èŠ‚ç‚¹å¤„ç†ï¼‰
            chunks = end_node.get('chunks', [])
            titles = end_node.get('titles', [])

            for j, chunk in enumerate(chunks[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªchunk
                if isinstance(chunk, str):
                    # æå–å›¾ç‰‡URLå¹¶ç›´æ¥è¾“å‡º
                    img_urls = []
                    # æå–<img>æ ‡ç­¾ä¸­çš„URL
                    img_matches = re.findall(r'<img[^>]+src=["\']([^"\']+)["\']', chunk, re.IGNORECASE)
                    img_urls.extend(img_matches)
                    # æå–HTTPå›¾ç‰‡é“¾æ¥
                    http_matches = re.findall(r'https?://[^\s]+\.(?:jpg|jpeg|png|gif|webp|bmp|svg)(?:\?[^\s]*)?', chunk, re.IGNORECASE)
                    img_urls.extend(http_matches)
                    
                    if img_urls:
                        # ç›´æ¥è¾“å‡ºå›¾ç‰‡URLï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸æ·»åŠ æç¤ºä¿¡æ¯
                        for img_url in img_urls:
                            formatted_text += f"{img_url}\n"
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼
                    elif '<table' in chunk or '<tr' in chunk:
                        formatted_text += f"ğŸ“Š åŒ…å«è¡¨æ ¼æ•°æ®\n"
                    else:
                        # æ˜¾ç¤ºæ–‡æœ¬å†…å®¹æ‘˜è¦
                        clean_chunk = re.sub(r'<[^>]+>', '', chunk)  # ç§»é™¤HTMLæ ‡ç­¾
                        if len(clean_chunk.strip()) > 50:
                            formatted_text += f"ğŸ“„ å†…å®¹ï¼š{clean_chunk.strip()[:200]}...\n"

                    # æ˜¾ç¤ºå¯¹åº”æ ‡é¢˜
                    if j < len(titles) and titles[j]:
                        formatted_text += f"ğŸ“– æ ‡é¢˜ï¼š{titles[j]}\n"

        return formatted_text

    except Exception as e:
        return f"âŒ å›¾æ•°æ®æ ¼å¼åŒ–å¤±è´¥: {str(e)}"


def generate_graph_data_summary(graph_results: List[Dict[str, Any]], query: str) -> str:
    """ç”Ÿæˆå›¾æ•°æ®æœç´¢ç»“æœçš„æ±‡æ€»ä¿¡æ¯

    Args:
        graph_results: å›¾æ•°æ®æœç´¢ç»“æœåˆ—è¡¨
        query: ç”¨æˆ·æŸ¥è¯¢

    Returns:
        æ±‡æ€»ä¿¡æ¯å­—ç¬¦ä¸²
    """
    if not graph_results:
        return "æœªæ‰¾åˆ°ç›¸å…³çš„å›¾æ•°æ®ä¿¡æ¯ã€‚"

    summary = f"ğŸ“Š åŸºäºæŸ¥è¯¢ '{query}' æ‰¾åˆ°çš„å›¾æ•°æ®ä¿¡æ¯ï¼š\n\n"

    # ç»Ÿè®¡ä¿¡æ¯
    total_relations = len(graph_results)
    entities = set()
    relations_found = []

    for result in graph_results:
        metadata = result.get("metadata", {})
        graph_relation = result.get("graph_relation", {})

        start_entity = metadata.get("start_entity", "")
        end_entity = metadata.get("end_entity", "")
        relation_desc = graph_relation.get("relation", {}).get("description", "")

        entities.add(start_entity)
        entities.add(end_entity)
        relations_found.append(f"{start_entity} â†’ {end_entity}")

    summary += f"ğŸ”— å‘ç° {total_relations} ä¸ªç›¸å…³å…³ç³»\n"
    summary += f"ğŸ·ï¸ æ¶‰åŠ {len(entities)} ä¸ªå®ä½“: {', '.join(list(entities)[:10])}{'...' if len(entities) > 10 else ''}\n\n"

    # åª’ä½“å†…å®¹ç»Ÿè®¡
    media_info = extract_images_and_tables_from_graph(graph_results)
    if media_info["image_count"] > 0:
        summary += f"ğŸ–¼ï¸ åŒ…å« {media_info['image_count']} å¼ ç›¸å…³å›¾ç‰‡\n"
    if media_info["table_count"] > 0:
        summary += f"ğŸ“Š åŒ…å« {media_info['table_count']} ä¸ªç›¸å…³è¡¨æ ¼\n"

    if media_info["image_count"] > 0 or media_info["table_count"] > 0:
        summary += "\n"

    # ä¸»è¦å…³ç³»å±•ç¤º
    summary += "ğŸ“‹ ä¸»è¦å…³ç³»æ¦‚è§ˆï¼š\n"
    for i, relation in enumerate(relations_found[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
        summary += f"{i+1}. {relation}\n"
    if len(relations_found) > 5:
        summary += f"... è¿˜æœ‰ {len(relations_found) - 5} ä¸ªå…³ç³»\n"

    return summary


def extract_images_and_tables_from_graph(graph_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ä»å›¾æ•°æ®ç»“æœä¸­æå–å›¾ç‰‡å’Œè¡¨æ ¼ä¿¡æ¯

    Args:
        graph_results: å›¾æ•°æ®æœç´¢ç»“æœåˆ—è¡¨

    Returns:
        åŒ…å«å›¾ç‰‡URLsã€è¡¨æ ¼ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®çš„å­—å…¸
        å›¾ç‰‡URLç›´æ¥è¿”å›ï¼Œä¸æ·»åŠ æ¥æºä¿¡æ¯
    """
    images = []
    tables = []

    for result in graph_results:
        if result.get("search_engine") == "graph_data":
            media_content = result.get("media_content", {})
            graph_relation = result.get("graph_relation", {})

            # ä»é¢„å¤„ç†çš„media_contentä¸­è·å–ä¿¡æ¯
            result_images = media_content.get("images", [])
            result_tables = media_content.get("tables", [])

            # ç›´æ¥æ·»åŠ å›¾ç‰‡URLï¼Œä¸æ·»åŠ æ¥æºä¿¡æ¯
            images.extend(result_images)

            # æ·»åŠ è¡¨æ ¼ä¿¡æ¯
            tables.extend(result_tables)
            
            # ä»graph_relationçš„chunksä¸­æå–å›¾ç‰‡URL
            for node_key in ["start_node", "end_node"]:
                node = graph_relation.get(node_key, {})
                chunks = node.get("chunks", [])
                for chunk in chunks:
                    if isinstance(chunk, str):
                        # æå–<img>æ ‡ç­¾ä¸­çš„URL
                        img_matches = re.findall(r'<img[^>]+src=["\']([^"\']+)["\']', chunk, re.IGNORECASE)
                        images.extend(img_matches)
                        
                        # æå–HTTPå›¾ç‰‡é“¾æ¥
                        http_matches = re.findall(r'https?://[^\s]+\.(?:jpg|jpeg|png|gif|webp|bmp|svg)(?:\?[^\s]*)?', chunk, re.IGNORECASE)
                        images.extend(http_matches)

    # å¯¹è¡¨æ ¼è¿›è¡Œå»é‡ï¼ˆåŸºäºå†…å®¹ï¼‰
    unique_tables = []
    seen_table_contents = set()
    for table in tables:
        content = table.get("content", "")
        if content not in seen_table_contents:
            seen_table_contents.add(content)
            unique_tables.append(table)

    # å›¾ç‰‡URLå»é‡å¹¶ç›´æ¥è¿”å›
    unique_images = list(set(images))

    return {
        "images": unique_images,  # ç›´æ¥è¿”å›URLåˆ—è¡¨ï¼Œä¸æ·»åŠ æ¥æºä¿¡æ¯
        "tables": unique_tables,  # åŸºäºå†…å®¹å»é‡
        "image_count": len(unique_images),
        "table_count": len(unique_tables)
    }


