# -*- coding:utf-8 -*-
"""
æ–‡æœ¬å†—ä½™ä¿¡æ¯èåˆæ™ºèƒ½ä½“ï¼ˆä¸ä½¿ç”¨å¤§æ¨¡å‹ï¼‰
è§£å†³ä¿¡æ¯å†—ä½™ï¼Œå°†æ£€ç´¢å•å…ƒä»"æ®µ"ç¼©å‡åˆ°"å¥å­"ï¼ŒæŒ‰é€»è¾‘å…³ç³»æ„å»ºä¸‰å±‚å›¾
"""

import re
from typing import Dict, Any, List, Tuple
from collections import defaultdict
import jieba
from Config.embedding_config import get_embeddings
from Control.control_milvus import CControl as MilvusController
from Config.milvus_config import is_milvus_enabled


class RedundancyFusionAgent:
    """æ–‡æœ¬å†—ä½™ä¿¡æ¯èåˆæ™ºèƒ½ä½“ï¼šå¥å­çº§æ£€ç´¢ + ä¸‰å±‚å›¾ç»“æ„"""
    
    def __init__(self):
        self.embedding = get_embeddings()
        self.enabled = is_milvus_enabled()
        if self.enabled:
            self.milvus_control = MilvusController()
        else:
            self.milvus_control = None
        
        # 12ç§ä¿®è¾å…³ç³»ï¼ˆç²¾ç®€ç‰ˆRSTï¼‰
        self.rhetorical_relations = {
            "å› æœ": ["å› ä¸º", "ç”±äº", "æ‰€ä»¥", "å› æ­¤", "å¯¼è‡´", "é€ æˆ", "å¼•èµ·"],
            "æ¡ä»¶": ["å¦‚æœ", "å‡å¦‚", "å€˜è‹¥", "åªè¦", "é™¤é", "å½“"],
            "è½¬æŠ˜": ["ä½†æ˜¯", "ç„¶è€Œ", "ä¸è¿‡", "å¯æ˜¯", "å´", "å°½ç®¡"],
            "å¹¶åˆ—": ["å¹¶ä¸”", "åŒæ—¶", "å¦å¤–", "æ­¤å¤–", "è€Œä¸”", "ä»¥åŠ"],
            "é€’è¿›": ["ä¸ä»…", "è€Œä¸”", "ç”šè‡³", "æ›´", "è¿˜", "è¿›ä¸€æ­¥"],
            "ä¸¾ä¾‹": ["ä¾‹å¦‚", "æ¯”å¦‚", "è­¬å¦‚", "å¦‚", "åƒ"],
            "å¯¹æ¯”": ["ç›¸æ¯”", "ç›¸å¯¹äº", "ä¸...ç›¸æ¯”", "è€Œ", "ç›¸å"],
            "æ€»ç»“": ["æ€»ä¹‹", "ç»¼ä¸Šæ‰€è¿°", "æ€»çš„æ¥è¯´", "æ¦‚æ‹¬"],
            "è§£é‡Š": ["å³", "ä¹Ÿå°±æ˜¯è¯´", "æ¢å¥è¯è¯´", "æ¢è¨€ä¹‹"],
            "æ—¶é—´": ["é¦–å…ˆ", "ç„¶å", "æ¥ç€", "æœ€å", "ä¹‹å", "ä¹‹å‰"],
            "ç›®çš„": ["ä¸ºäº†", "ä»¥ä¾¿", "æ—¨åœ¨", "ç›®çš„æ˜¯"],
            "è®©æ­¥": ["è™½ç„¶", "å°½ç®¡", "å³ä½¿", "çºµç„¶"]
        }
    
    def split_into_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬æ‹†åˆ†æˆå¥å­
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        # ä¸­æ–‡å¥å­åˆ†å‰²ï¼šã€‚ï¼ï¼Ÿï¼›\n
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]+', text)
        # è¿‡æ»¤ç©ºå¥å­å’Œè¿‡çŸ­å¥å­
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]
        return sentences
    
    def identify_rhetorical_relation(self, sentence: str) -> Tuple[str, float]:
        """
        è¯†åˆ«å¥å­çš„ä¿®è¾å…³ç³»
        
        Args:
            sentence: å¥å­æ–‡æœ¬
            
        Returns:
            (å…³ç³»ç±»å‹, ç½®ä¿¡åº¦)
        """
        sentence_lower = sentence.lower()
        max_score = 0
        best_relation = "èƒŒæ™¯"  # é»˜è®¤å…³ç³»
        
        for relation, keywords in self.rhetorical_relations.items():
            score = sum(1 for kw in keywords if kw in sentence_lower)
            if score > max_score:
                max_score = score
                best_relation = relation
        
        confidence = min(max_score / 3.0, 1.0)  # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        return best_relation, confidence
    
    def extract_entities(self, sentence: str) -> List[str]:
        """
        æå–å¥å­ä¸­çš„å®ä½“ï¼ˆç®€å•ç‰ˆæœ¬ï¼Œä½¿ç”¨jiebaåˆ†è¯ï¼‰
        
        Args:
            sentence: å¥å­æ–‡æœ¬
            
        Returns:
            å®ä½“åˆ—è¡¨
        """
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(sentence)
        # è¿‡æ»¤åœç”¨è¯å’Œæ ‡ç‚¹
        stopwords = {"çš„", "äº†", "åœ¨", "æ˜¯", "å’Œ", "ä¸", "æˆ–", "ä½†", "è€Œ", "ç­‰", "ã€", "ï¼Œ", "ã€‚"}
        entities = [w for w in words if w.strip() and w not in stopwords and len(w) > 1]
        return entities[:5]  # æœ€å¤šè¿”å›5ä¸ªå®ä½“
    
    def build_three_layer_graph(self, sentences: List[str], 
                                search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºä¸‰å±‚å›¾ç»“æ„
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            search_results: æœç´¢ç»“æœ
            
        Returns:
            ä¸‰å±‚å›¾ç»“æ„ï¼š
            - sub_sentences: subå¥å­å±‚ï¼ˆèƒŒæ™¯ã€å› æœã€ä¸¾ä¾‹ç­‰ï¼‰
            - core_sentences: coreå¥å±‚ï¼ˆæ ¸å¿ƒäº‹å®ï¼‰
            - topic_bridges: Topicå±‚ï¼ˆæ–‡æ¡£"æ¡¥æ¢"ï¼‰
        """
        sub_sentences = []  # subå¥å­å±‚
        core_sentences = []  # coreå¥å±‚
        topic_bridges = []  # Topicå±‚
        
        # ä¸ºæ¯ä¸ªå¥å­åˆ†ç±»
        for i, sentence in enumerate(sentences):
            relation, confidence = self.identify_rhetorical_relation(sentence)
            entities = self.extract_entities(sentence)
            
            sentence_info = {
                "sentence": sentence,
                "index": i,
                "relation": relation,
                "confidence": confidence,
                "entities": entities
            }
            
            # æ ¹æ®ä¿®è¾å…³ç³»åˆ†ç±»
            if relation in ["èƒŒæ™¯", "ä¸¾ä¾‹", "è§£é‡Š", "æ—¶é—´"]:
                sub_sentences.append(sentence_info)
            elif relation in ["å› æœ", "æ¡ä»¶", "ç›®çš„"]:
                # è¿™äº›å…³ç³»å¯èƒ½åŒ…å«æ ¸å¿ƒé€»è¾‘ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯èƒŒæ™¯
                if confidence > 0.5:
                    core_sentences.append(sentence_info)
                else:
                    sub_sentences.append(sentence_info)
            else:
                # å…¶ä»–å…³ç³»ï¼ˆè½¬æŠ˜ã€å¹¶åˆ—ã€é€’è¿›ç­‰ï¼‰é€šå¸¸æ˜¯æ ¸å¿ƒäº‹å®
                core_sentences.append(sentence_info)
        
        # æ„å»ºTopicå±‚ï¼ˆæ–‡æ¡£"æ¡¥æ¢"ï¼‰
        # é€šè¿‡å®ä½“å¯¹é½ï¼Œæ‰¾åˆ°è·¨æ–‡æ¡£çš„è¿æ¥
        entity_to_sentences = defaultdict(list)
        for sentence_info in core_sentences + sub_sentences:
            for entity in sentence_info["entities"]:
                entity_to_sentences[entity].append(sentence_info)
        
        # æ‰¾å‡ºè¿æ¥å¤šä¸ªå¥å­çš„å®ä½“ï¼ˆTopicæ¡¥æ¢ï¼‰
        for entity, linked_sentences in entity_to_sentences.items():
            if len(linked_sentences) >= 2:  # è‡³å°‘è¿æ¥2ä¸ªå¥å­
                topic_bridges.append({
                    "entity": entity,
                    "linked_sentences": [s["sentence"] for s in linked_sentences[:3]],  # æœ€å¤š3ä¸ªå¥å­
                    "sentence_count": len(linked_sentences)
                })
        
        return {
            "sub_sentences": sub_sentences,
            "core_sentences": core_sentences,
            "topic_bridges": topic_bridges,
            "total_sentences": len(sentences)
        }
    
    def fuse_redundant_information(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        èåˆå†—ä½™ä¿¡æ¯ï¼šæ‹†å¥ã€æ‰¾å…³ç³»ã€æ¶æ¡¥æ¢
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            èåˆåçš„ä¿¡æ¯ï¼š
            - fused_content: èåˆåçš„æ ¸å¿ƒå†…å®¹
            - three_layer_graph: ä¸‰å±‚å›¾ç»“æ„
            - entity_triples: å®ä½“-å…³ç³»-å®ä½“ä¸‰å…ƒç»„
        """
        try:
            # æ­¥éª¤1: æ‹†å¥ï¼ŒæŠŠæ–‡æ¡£åˆ‡æˆå•å¥
            all_sentences = []
            sentence_to_source = {}  # è®°å½•å¥å­æ¥æº
            
            for result in search_results:
                content = result.get("content", "")
                doc_id = result.get("doc_id", "")
                
                sentences = self.split_into_sentences(content)
                for sentence in sentences:
                    all_sentences.append(sentence)
                    sentence_to_source[sentence] = doc_id
            
            print(f"ğŸ“ æ‹†å¥å®Œæˆï¼šå…± {len(all_sentences)} ä¸ªå¥å­")
            
            # æ­¥éª¤2: æ‰¾å…³ç³»ï¼Œè¯†åˆ«å¥é—´12ç§ä¿®è¾å…³ç³»
            # è¿™ä¸€æ­¥å·²ç»åœ¨build_three_layer_graphä¸­å®Œæˆ
            
            # æ­¥éª¤3: æ¶æ¡¥æ¢ï¼Œè·¨æ–‡æ¡£å®ä½“å¯¹é½ï¼Œç”Ÿæˆ"å®ä½“-å…³ç³»-å®ä½“"ä¸‰å…ƒç»„
            three_layer_graph = self.build_three_layer_graph(all_sentences, search_results)
            
            # ç”Ÿæˆå®ä½“-å…³ç³»-å®ä½“ä¸‰å…ƒç»„
            entity_triples = []
            core_sentences = three_layer_graph["core_sentences"]
            
            for i, sent_info1 in enumerate(core_sentences):
                entities1 = sent_info1["entities"]
                relation1 = sent_info1["relation"]
                
                # æŸ¥æ‰¾ä¸å½“å‰å¥å­æœ‰å…±åŒå®ä½“çš„å…¶ä»–å¥å­
                for j, sent_info2 in enumerate(core_sentences[i+1:], start=i+1):
                    entities2 = sent_info2["entities"]
                    relation2 = sent_info2["relation"]
                    
                    # æ‰¾å…±åŒå®ä½“
                    common_entities = set(entities1) & set(entities2)
                    if common_entities:
                        # ç”Ÿæˆä¸‰å…ƒç»„
                        for entity in common_entities:
                            # ç¡®å®šå…³ç³»ç±»å‹
                            if relation1 == relation2:
                                relation = relation1
                            else:
                                relation = f"{relation1}-{relation2}"
                            
                            entity_triples.append({
                                "entity1": entities1[0] if entities1 else "",
                                "relation": relation,
                                "entity2": entities2[0] if entities2 else "",
                                "bridge_entity": entity,
                                "sentence1": sent_info1["sentence"],
                                "sentence2": sent_info2["sentence"]
                            })
            
            # èåˆæ ¸å¿ƒå†…å®¹ï¼ˆå»é‡ï¼Œä¿ç•™æ ¸å¿ƒäº‹å®ï¼‰
            fused_content_parts = []
            seen_sentences = set()
            
            # ä¼˜å…ˆä½¿ç”¨coreå¥å±‚çš„å¥å­
            for sent_info in three_layer_graph["core_sentences"]:
                sentence = sent_info["sentence"]
                if sentence not in seen_sentences:
                    fused_content_parts.append(sentence)
                    seen_sentences.add(sentence)
            
            # è¡¥å……subå¥å±‚çš„é‡è¦èƒŒæ™¯ä¿¡æ¯
            for sent_info in three_layer_graph["sub_sentences"][:5]:  # æœ€å¤š5ä¸ªèƒŒæ™¯å¥
                sentence = sent_info["sentence"]
                if sentence not in seen_sentences:
                    fused_content_parts.append(sentence)
                    seen_sentences.add(sentence)
            
            fused_content = "\n".join(fused_content_parts)
            
            return {
                "success": True,
                "fused_content": fused_content,
                "three_layer_graph": three_layer_graph,
                "entity_triples": entity_triples[:10],  # æœ€å¤š10ä¸ªä¸‰å…ƒç»„
                "core_sentences": [s["sentence"] for s in three_layer_graph["core_sentences"]],
                "topic_bridges": three_layer_graph["topic_bridges"]
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"ä¿¡æ¯èåˆå¤±è´¥: {str(e)}",
                "fused_content": "",
                "three_layer_graph": {},
                "entity_triples": []
            }
