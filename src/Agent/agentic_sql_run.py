# -*- coding:utf-8 -*-
"""
Agentic SQLæ™ºèƒ½ä½“ä¸»å…¥å£
æ ¹æ®ç”¨æˆ·æŸ¥è¯¢å’Œæ•°æ®åº“ä¿¡æ¯ï¼Œç”ŸæˆSQLæŸ¥è¯¢è¯­å¥
"""

import traceback

from typing import Dict, Any, Optional, List
from Agent.AgenticSqlAgent.tools.database_tools import (
    query_database_info, 
    query_tables_by_sql_id, 
    query_columns_by_table_id
)
from Agent.AgenticSqlAgent.database_metadata_agent import DatabaseMetadataAgent
from Agent.AgenticSqlAgent.AnalysisSql.database_analysis_agent import DatabaseAnalysisAgent
from Agent.AgenticSqlAgent.agents.query_decomposition_agent import QueryDecompositionAgent
from Agent.AgenticSqlAgent.agents.intent_recognition_agent import IntentRecognitionAgent
# from Agent.AgenticSqlAgent.agents.database_check_agent import DatabaseCheckAgent
from Agent.AgenticSqlAgent.SqlGeneration.sql_generation_flow import SqlGenerationFlow
from Agent.AgenticSqlAgent.RuleLogic.logic_calculation_agent import LogicCalculationAgent
from Sql.schema_vector import SqlSchemaVectorAgent
import json
from Db.sqlite_db import cSingleSqlite


def run_sql_agentic_search(sql_id: str, query: str, user_id: str = None, 
                           step_callback: Optional[callable] = None) -> Dict[str, Any]:
    """
    è¿è¡ŒAgentic SQLæ™ºèƒ½ä½“æœç´¢æµç¨‹
    
    Args:
        sql_id: æ•°æ®åº“è¿æ¥ID
        query: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
        user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
        step_callback: æ­¥éª¤å›è°ƒå‡½æ•°ï¼Œç”¨äºæµå¼è¿”å›æ­¥éª¤ä¿¡æ¯
                       callback(step_name: str, step_data: Dict[str, Any])
        
    Returns:
        å®Œæ•´çš„æœç´¢ç»“æœï¼ŒåŒ…å«ï¼š
        - success: æ˜¯å¦æˆåŠŸ
        - sql: ç”Ÿæˆçš„SQLè¯­å¥
        - intent_analysis: æ„å›¾åˆ†æç»“æœ
        - relevant_tables: ç›¸å…³è¡¨åˆ—è¡¨
        - relevant_columns: ç›¸å…³åˆ—åˆ—è¡¨ï¼ˆæŒ‰è¡¨åˆ†ç»„ï¼‰
        - sql_check: SQLæ£€æŸ¥ç»“æœ
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    """
    
    def _notify_step(step_name: str, step_data: Dict[str, Any]):
        """é€šçŸ¥æ­¥éª¤å®Œæˆ"""
        if step_callback:
            try:
                step_callback(step_name, step_data)
            except Exception as e:
                print(f"âš ï¸ æ­¥éª¤å›è°ƒå¤±è´¥ ({step_name}): {e}")
    
    try:
        print(f"ğŸš€ å¼€å§‹Agentic SQLæ™ºèƒ½ä½“æœç´¢æµç¨‹")
        print(f"   SQL ID: {sql_id}")
        print(f"   ç”¨æˆ·æŸ¥è¯¢: {query}")
        
        # æ­¥éª¤1: è·å–æ•°æ®åº“ä¿¡æ¯ï¼ˆåŒ…æ‹¬è¡¨ç»“æ„å’Œåˆ—ä¿¡æ¯ï¼‰
        print("\nğŸ“Š æ­¥éª¤1: è·å–æ•°æ®åº“ä¿¡æ¯...")
        database_info = query_database_info(sql_id)
        if not database_info:
            error_msg = f"æœªæ‰¾åˆ°æ•°æ®åº“ä¿¡æ¯ (sql_id: {sql_id})"
            _notify_step("step_1_database_info", {
                "success": False,
                "error": error_msg
            })
            return {
                "success": False,
                "error": error_msg
            }
        
        db_name = database_info.get('sql_name', '')
        db_type = database_info.get('sql_type', '')
        print(f"   âœ… æ•°æ®åº“: {db_name} ({db_type})")
        
        # è·å–æ‰€æœ‰è¡¨ä¿¡æ¯
        print("   ğŸ“‹ è·å–è¡¨ç»“æ„ä¿¡æ¯...")
        all_tables = query_tables_by_sql_id(sql_id)
        tables_info = []
        
        for table in all_tables:
            table_id = table.get("table_id", "")
            table_name = table.get("table_name", "")
            table_description = table.get("table_description", "")
            
            if not table_id:
                continue
            
            # è·å–è¡¨çš„åˆ—ä¿¡æ¯
            columns = query_columns_by_table_id(table_id)
            attributes_des_lt = []
            attributes_col_lt = []
            numerics_des_lt = []
            numerics_col_lt = []
            datetime_des_lt = []
            datetime_col_lt = []
            for col in columns:
                col_name = col.get("col_name", "")
                col_info = col.get("col_info", {})
                # è§£æ col_infoï¼ˆå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²ï¼‰
                if isinstance(col_info, str):
                    try:
                        col_info = json.loads(col_info)
                    except:
                        col_info = {}
                        
                col_comment = col_info.get("comment", "") if isinstance(col_info, dict) else ""
                
                col_type_ana = col_info.get("ana_type", "")
                if(col_type_ana == "numeric"):
                    numerics_des_lt.append(col_comment)
                    numerics_col_lt.append(col_name)
                elif(col_type_ana == "attribute"):
                    attributes_des_lt.append(col_comment)
                    attributes_col_lt.append(col_name)
                elif(col_type_ana == "datetime"):
                    datetime_des_lt.append(col_comment)
                    datetime_col_lt.append(col_name)
                    
            
            tables_info.append({
                # "table_id": table_id,
                "table_name": table_name,
                "table_description": table_description,
                "attributes_description": attributes_des_lt,
                "attributes":attributes_col_lt,
                "datetime_description": datetime_des_lt,
                "datetime": datetime_col_lt,
                "numerics_description": numerics_des_lt,
                "numerics": numerics_col_lt
            })
        
        # å°†è¡¨ç»“æ„ä¿¡æ¯æ·»åŠ åˆ° database_info ä¸­
        database_info["tables"] = tables_info
        database_info["tables_count"] = len(tables_info)
        
        print(f"   âœ… è·å–åˆ° {len(tables_info)} ä¸ªè¡¨çš„ç»“æ„ä¿¡æ¯")
        
        _notify_step("step_1_database_info", {
            "success": True,
            "database_name": db_name,
            "database_type": db_type,
            "tables_count": len(tables_info),
            "database_info": database_info
        })
        
        # æ­¥éª¤1.2: æ•°æ®åº“å…ƒæ•°æ®æŸ¥è¯¢æ£€æŸ¥
        print("\nğŸ” æ­¥éª¤1.2: æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“å…ƒæ•°æ®æŸ¥è¯¢...")
        metadata_agent = DatabaseMetadataAgent()
        metadata_result = metadata_agent.process_metadata_query(sql_id, query)
        
        if metadata_result.get("success") and metadata_result.get("is_metadata_query"):
            # æ˜¯å…ƒæ•°æ®æŸ¥è¯¢ï¼Œç›´æ¥è¿”å›ç»“æœ
            metadata_query_result = metadata_result.get("metadata_result", {})
            query_type = metadata_result.get("query_type", "")
            
            print(f"   âœ… è¯†åˆ«ä¸ºæ•°æ®åº“å…ƒæ•°æ®æŸ¥è¯¢: {query_type}")
            
            if metadata_query_result.get("success"):
                # æ„å»ºè¿”å›ç»“æœ
                result_data = metadata_query_result.get("data", {})
                result_message = result_data.get("message", "æŸ¥è¯¢æˆåŠŸ")
                
                _notify_step("step_1_2_metadata_query", {
                    "success": True,
                    "is_metadata_query": True,
                    "query_type": query_type,
                    "result": result_data,
                    "message": result_message
                })
                
                return {
                    "success": True,
                    "is_metadata_query": True,
                    "query_type": query_type,
                    "metadata_result": metadata_query_result,
                    "message": result_message
                }
            else:
                # å…ƒæ•°æ®æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ï¼ˆè¡¨åæˆ–åˆ—åé”™è¯¯ç­‰ï¼‰
                error_msg = metadata_query_result.get("error", "å…ƒæ•°æ®æŸ¥è¯¢æ‰§è¡Œå¤±è´¥")
                error_message = metadata_query_result.get("error_message", error_msg)
                available_tables = metadata_query_result.get("available_tables", [])
                available_columns = metadata_query_result.get("available_columns", [])
                
                print(f"   âš ï¸ å…ƒæ•°æ®æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {error_msg}")
                
                # æ„å»ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                error_result = {
                    "error": error_msg,
                    "error_message": error_message,
                    "query_type": query_type,
                    "table_name": metadata_result.get("table_name", ""),
                    "column_name": metadata_result.get("column_name", "")
                }
                
                if available_tables:
                    error_result["available_tables"] = available_tables
                    error_result["available_tables_count"] = len(available_tables)
                
                if available_columns:
                    error_result["available_columns"] = available_columns
                    error_result["available_columns_count"] = len(available_columns)
                
                _notify_step("step_1_2_metadata_query", {
                    "success": False,
                    "is_metadata_query": True,
                    "error": error_msg,
                    "error_message": error_message,
                    "available_tables": available_tables,
                    "available_columns": available_columns
                })
                
                # å…ƒæ•°æ®æŸ¥è¯¢å¤±è´¥æ—¶ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯å’Œè¯¦ç»†æŸ¥è¯¢ä¿¡æ¯ï¼Œä¸ç»§ç»­åç»­æµç¨‹
                return {
                    "success": False,
                    "is_metadata_query": True,
                    "query_type": query_type,
                    "error": error_msg,
                    "error_message": error_message,
                    "available_tables": available_tables,
                    "available_columns": available_columns,
                    "metadata_result": metadata_query_result
                }
        else:
            # ä¸æ˜¯å…ƒæ•°æ®æŸ¥è¯¢ï¼Œç»§ç»­åç»­æµç¨‹
            print(f"   âœ… ä¸æ˜¯æ•°æ®åº“å…ƒæ•°æ®æŸ¥è¯¢ï¼Œç»§ç»­åç»­æµç¨‹")
            _notify_step("step_1_2_metadata_query", {
                "success": True,
                "is_metadata_query": False,
                "should_continue": True
            })
        
        # æ­¥éª¤1.3: Milvuså‘é‡æœç´¢ï¼ˆé€šè¿‡sql_idä½œä¸ºpartitionæŸ¥è¯¢ç›¸å…³è¡¨ï¼‰
        print("\nğŸ” æ­¥éª¤1.3: Milvuså‘é‡æœç´¢ç›¸å…³è¡¨...")
        table_info_list = []
        try:
            schema_vector_agent = SqlSchemaVectorAgent(sql_id=sql_id)
            # é€šè¿‡sql_idä½œä¸ºpartitionè¿›è¡Œè¯­ä¹‰æœç´¢
            search_result = schema_vector_agent.search_graph_nodes(
                query=query,
                sql_id=sql_id,
                limit=10  # æœ€å¤šè¿”å›10ä¸ªç›¸å…³è¡¨
            )
            
            if search_result.get("success"):
                results = search_result.get("results", [])
                print(f"   âœ… Milvusæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³è¡¨èŠ‚ç‚¹")
                
                # æå–å”¯ä¸€çš„table_id
                table_ids = set()
                for result in results:
                    table_id = result.get("table_id", "")
                    if table_id:
                        table_ids.add(table_id)
                
                print(f"   ğŸ“‹ æå–åˆ° {len(table_ids)} ä¸ªå”¯ä¸€çš„è¡¨ID")
                
                # é€šè¿‡table_idè·å–è¡¨ä¿¡æ¯
                # å…ˆè·å–æ‰€æœ‰è¡¨ä¿¡æ¯ï¼Œç„¶ååŒ¹é…table_id
                all_tables = query_tables_by_sql_id(sql_id)
                table_id_to_table = {t.get("table_id"): t for t in all_tables}
                
                for table_id in table_ids:
                    try:
                        # ä»å·²æŸ¥è¯¢çš„è¡¨ä¿¡æ¯ä¸­è·å–
                        table_info = table_id_to_table.get(table_id)
                        if not table_info:
                            print(f"   âš ï¸ æœªæ‰¾åˆ°è¡¨ID {table_id} çš„ä¿¡æ¯")
                            continue
                        
                        # è·å–åˆ—ä¿¡æ¯
                        columns = query_columns_by_table_id(table_id)
                        
                        # æ„å»ºè¡¨ä¿¡æ¯
                        table_data = {
                            "table_id": table_id,
                            "table_name": table_info.get("table_name", ""),
                            "table_description": table_info.get("table_description", ""),
                            "columns": []
                        }
                        
                        # æ·»åŠ åˆ—ä¿¡æ¯
                        for col in columns:
                            col_info = col.get("col_info", {})
                            table_data["columns"].append({
                                "col_name": col.get("col_name", ""),
                                "col_type": col.get("col_type", ""),
                                "comment": col_info.get("comment", "")
                            })
                        
                        table_info_list.append(table_data)
                        print(f"   âœ… è·å–è¡¨ä¿¡æ¯: {table_data['table_name']} ({len(table_data['columns'])} åˆ—)")
                        
                    except Exception as e:
                        print(f"   âš ï¸ è·å–è¡¨ {table_id} ä¿¡æ¯å¤±è´¥: {e}")
                        continue
                
                _notify_step("step_1_3_milvus_search", {
                    "success": True,
                    "search_results_count": len(results),
                    "table_ids_found": len(table_ids),
                    "table_info_count": len(table_info_list)
                })
            else:
                print(f"   âš ï¸ Milvusæœç´¢å¤±è´¥: {search_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                _notify_step("step_1_3_milvus_search", {
                    "success": False,
                    "error": search_result.get("message", "Milvusæœç´¢å¤±è´¥")
                })
        except Exception as e:
            print(f"   âš ï¸ Milvusæœç´¢å¼‚å¸¸: {e}")
            traceback.print_exc()
            _notify_step("step_1_3_milvus_search", {
                "success": False,
                "error": f"Milvusæœç´¢å¼‚å¸¸: {str(e)}"
            })
        
        # æ­¥éª¤1.5: é—®é¢˜æ‹†è§£ä¸é€»è¾‘åˆ†æï¼ˆå¸¦è¡¨ä¿¡æ¯è¯­ä¹‰æ ¸å¯¹ï¼‰
        print("\nğŸ” æ­¥éª¤1.5: é—®é¢˜æ‹†è§£ä¸é€»è¾‘åˆ†æï¼ˆè¯­ä¹‰æ ¸å¯¹ï¼‰...")
        decomposition_agent = QueryDecompositionAgent()
        # å¦‚æœæ‰¾åˆ°äº†ç›¸å…³è¡¨ä¿¡æ¯ï¼Œä¼ å…¥è¿›è¡Œè¯­ä¹‰æ ¸å¯¹
        if table_info_list:
            print(f"   ğŸ“‹ ä½¿ç”¨ {len(table_info_list)} ä¸ªè¡¨ä¿¡æ¯è¿›è¡Œè¯­ä¹‰æ ¸å¯¹")
            decomposition_result = decomposition_agent.decompose_query(query, table_info_list=table_info_list)
        else:
            print(f"   âš ï¸ æœªæ‰¾åˆ°ç›¸å…³è¡¨ä¿¡æ¯ï¼Œä»…è¿›è¡Œé—®é¢˜æ‹†è§£")
            decomposition_result = decomposition_agent.decompose_query(query)
        
        if not decomposition_result.get("success"):
            error_msg = f"é—®é¢˜æ‹†è§£å¤±è´¥: {decomposition_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            # ç¡®ä¿decomposition_resultåŒ…å«queryå­—æ®µï¼Œä»¥ä¾¿åç»­æ­¥éª¤ä½¿ç”¨
            if "query" not in decomposition_result:
                decomposition_result["query"] = query
            _notify_step("step_1_5_query_decomposition", {
                "success": False,
                "error": error_msg
            })
            # é—®é¢˜æ‹†è§£å¤±è´¥ä¸å½±å“åç»­æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
            print(f"   âš ï¸ {error_msg}ï¼Œç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")
            return {
                "success": False,
                "error": error_msg
            }
        else:
            entities = decomposition_result.get('entities', [])
            metrics = decomposition_result.get('metrics', [])
            time_dims = decomposition_result.get('time_dimensions', [])
            relationships = decomposition_result.get('relationships', [])
            logical_calcs = decomposition_result.get('logical_calculations', [])
            spatial_dims = decomposition_result.get('spatial_dimensions', [])
            set_theory = decomposition_result.get('set_theory_relations', [])
            relational_algebra = decomposition_result.get('relational_algebra', [])
            graph_theory = decomposition_result.get('graph_theory_relations', [])
            logical_reasoning = decomposition_result.get('logical_reasoning', [])
            semantic_network = decomposition_result.get('semantic_network', [])
            math_relations = decomposition_result.get('mathematical_relations', [])
            
            print(f"   âœ… æ‹†è§£ç»“æœ:")
            print(f"      - å®ä½“: {len(entities)} ä¸ª")
            print(f"      - æŒ‡æ ‡: {len(metrics)} ä¸ª")
            print(f"      - æ—¶é—´ç»´åº¦: {len(time_dims)} ä¸ª")
            print(f"      - å…³è”å…³ç³»: {len(relationships)} ä¸ª")
            print(f"      - é€»è¾‘è®¡ç®—: {len(logical_calcs)} ä¸ª")
            print(f"      - ç©ºé—´ç»´åº¦: {len(spatial_dims)} ä¸ª")
            print(f"      - é›†åˆè®ºå…³ç³»: {len(set_theory)} ä¸ª")
            print(f"      - å…³ç³»ä»£æ•°: {len(relational_algebra)} ä¸ª")
            print(f"      - å›¾è®ºå…³ç³»: {len(graph_theory)} ä¸ª")
            print(f"      - é€»è¾‘æ¨ç†: {len(logical_reasoning)} ä¸ª")
            print(f"      - è¯­ä¹‰ç½‘ç»œ: {len(semantic_network)} ä¸ª")
            print(f"      - æ•°å­¦å…³ç³»: {len(math_relations)} ä¸ª")
            
            _notify_step("step_1_5_query_decomposition", {
                "success": True,
                "entities_count": len(entities),
                "metrics_count": len(metrics),
                "time_dimensions_count": len(time_dims),
                "relationships_count": len(relationships),
                "logical_calculations_count": len(logical_calcs),
                "spatial_dimensions_count": len(spatial_dims),
                "set_theory_relations_count": len(set_theory),
                "relational_algebra_count": len(relational_algebra),
                "graph_theory_relations_count": len(graph_theory),
                "logical_reasoning_count": len(logical_reasoning),
                "semantic_network_count": len(semantic_network),
                "mathematical_relations_count": len(math_relations),
                "entities": entities,
                "metrics": metrics,
                "time_dimensions": time_dims,
                "relationships": relationships,
                "logical_calculations": logical_calcs,
                "spatial_dimensions": spatial_dims,
                "set_theory_relations": set_theory,
                "relational_algebra": relational_algebra,
                "graph_theory_relations": graph_theory,
                "logical_reasoning": logical_reasoning,
                "semantic_network": semantic_network,
                "mathematical_relations": math_relations,
                "analysis_summary": decomposition_result.get("analysis_summary", ""),
                "decomposition_result": decomposition_result
            })
        # ç­›é€‰å‡ºéœ€è¦æ ¸å¯¹çš„è¡¨
        print("\nğŸ” ç­›é€‰ç›¸å…³è¡¨...")
        matched_tables = set()  # ä½¿ç”¨seté¿å…é‡å¤
        
        # 1. é€šè¿‡entitiesåœ¨sqliteä¸­LIKEæœç´¢ç›¸å…³è¡¨æ ¼çš„æè¿°
        if entities:
            for entity in entities:
                entity_name = entity.get('entity_name', '') if isinstance(entity, dict) else str(entity)
                if entity_name:
                    # åœ¨è¡¨æè¿°ä¸­æœç´¢
                    tables_by_desc = cSingleSqlite.query_tables_by_description_like(sql_id, entity_name)
                    for table in tables_by_desc:
                        table_id = table.get('table_id', '')
                        if table_id:
                            matched_tables.add(table_id)
                    print(f"   - å®ä½“ '{entity_name}' åŒ¹é…åˆ° {len(tables_by_desc)} ä¸ªè¡¨")
        
        # 2. é€šè¿‡metricsåœ¨sqliteä¸­LIKEæœç´¢ç›¸å…³åˆ—çš„æè¿°
        if metrics:
            for metric in metrics:
                metric_name = metric.get('metric_name', '') if isinstance(metric, dict) else str(metric)
                if metric_name:
                    # åœ¨åˆ—æè¿°ä¸­æœç´¢
                    columns_by_desc = cSingleSqlite.query_columns_by_description_like(sql_id, metric_name)
                    for col in columns_by_desc:
                        table_id = col.get('table_id', '')
                        if table_id:
                            matched_tables.add(table_id)
                    print(f"   - æŒ‡æ ‡ '{metric_name}' åŒ¹é…åˆ° {len(columns_by_desc)} ä¸ªåˆ—ï¼ˆæ¶‰åŠ {len(set(c.get('table_id') for c in columns_by_desc))} ä¸ªè¡¨ï¼‰")
        
        # è·å–åŒ¹é…çš„è¡¨ä¿¡æ¯
        filtered_tables = []
        if matched_tables:
            all_tables = query_tables_by_sql_id(sql_id)
            for table in all_tables:
                if table.get('table_id', '') in matched_tables:
                    filtered_tables.append(table)
            print(f"   âœ… ç­›é€‰å‡º {len(filtered_tables)} ä¸ªç›¸å…³è¡¨")
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„è¡¨ï¼Œä½¿ç”¨æ‰€æœ‰è¡¨
            filtered_tables = query_tables_by_sql_id(sql_id)
            print(f"   âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„è¡¨ï¼Œä½¿ç”¨æ‰€æœ‰è¡¨ï¼ˆ{len(filtered_tables)} ä¸ªï¼‰")
        
        # æ›´æ–°database_infoä¸­çš„tablesï¼ŒåªåŒ…å«ç­›é€‰å‡ºçš„è¡¨
        if filtered_tables:
            # é‡æ–°æ„å»ºtables_infoï¼ŒåªåŒ…å«ç­›é€‰å‡ºçš„è¡¨
            filtered_tables_info = []
            for table in filtered_tables:
                table_id = table.get('table_id', '')
                table_name = table.get('table_name', '')
                table_description = table.get('table_description', '')
                
                # è·å–è¡¨çš„åˆ—ä¿¡æ¯
                columns = query_columns_by_table_id(table_id)
                attributes = []
                attributes_description = []
                datetime_cols = []
                datetime_description = []
                numerics = []
                numerics_description = []
                
                for col in columns:
                    col_name = col.get('col_name', '')
                    col_type = col.get('col_type', '')
                    col_info = col.get('col_info', {})
                    
                    if isinstance(col_info, str):
                        try:
                            col_info = json.loads(col_info)
                        except:
                            col_info = {}
                    
                    col_comment = col_info.get('comment', '') if isinstance(col_info, dict) else ''
                    col_type_ana = col_info.get('ana_type', '')
                    
                    if col_type_ana == 'numeric':
                        numerics.append(col_name)
                        numerics_description.append(col_comment if col_comment else col_name)
                    elif col_type_ana == 'attribute':
                        attributes.append(col_name)
                        attributes_description.append(col_comment if col_comment else col_name)
                    elif col_type_ana == 'datetime':
                        datetime_cols.append(col_name)
                        datetime_description.append(col_comment if col_comment else col_name)
                
                filtered_tables_info.append({
                    'table_id': table_id,
                    'table_name': table_name,
                    'table_description': table_description,
                    'attributes': attributes,
                    'attributes_description': attributes_description,
                    'datetime': datetime_cols,
                    'datetime_description': datetime_description,
                    'numerics': numerics,
                    'numerics_description': numerics_description
                })
            
            # æ›´æ–°database_info
            database_info['tables'] = filtered_tables_info
        
        # æ­¥éª¤2: æ„å›¾è¯†åˆ«
        print("\nğŸ§  æ­¥éª¤2: æ„å›¾è¯†åˆ«...")
        intent_agent = IntentRecognitionAgent()
        intent_analysis = intent_agent.analyze_intent(decomposition_result, database_info)
        
        if not intent_analysis.get("success"):
            error_msg = f"æ„å›¾è¯†åˆ«å¤±è´¥: {intent_analysis.get('error', 'æœªçŸ¥é”™è¯¯')}"
            _notify_step("step_2_intent_recognition", {
                "success": False,
                "error": error_msg
            })
            return {
                "success": False,
                "error": error_msg,
                "intent_analysis": intent_analysis
            }
        
        primary_entities = intent_analysis.get('primary_entities', [])
        entity_attributes = intent_analysis.get('entity_attributes', [])
        entity_metrics = intent_analysis.get('entity_metrics', [])
        time_dimensions = intent_analysis.get('time_dimensions', [])
        relationships = intent_analysis.get('relationships', [])
        relevant_tables = intent_analysis.get('relevant_tables', [])
        relevant_columns = intent_analysis.get('relevant_columns', [])
        
        print(f"   âœ… è¯†åˆ«åˆ°:")
        print(f"      - æœ¬æºå®ä½“: {len(primary_entities)} ä¸ª")
        print(f"      - å®ä½“å±æ€§: {len(entity_attributes)} ä¸ª")
        print(f"      - å®ä½“æŒ‡æ ‡: {len(entity_metrics)} ä¸ª")
        print(f"      - æ—¶é—´ç»´åº¦: {len(time_dimensions)} ä¸ª")
        print(f"      - å…³è”å…³ç³»: {len(relationships)} ä¸ª")
        print(f"      - ç›¸å…³è¡¨: {len(relevant_tables)} ä¸ª")
        print(f"      - ç›¸å…³åˆ—: {len(relevant_columns)} ä¸ª")
        
        # æ‰“å°ç›¸å…³è¡¨å
        if relevant_tables:
            table_names = [t.get('table_name', '') for t in relevant_tables]
            print(f"      - è¡¨å: {', '.join(table_names[:5])}")
        
        # æ‰“å°ç›¸å…³åˆ—åï¼ˆæŒ‰è¡¨åˆ†ç»„ï¼‰
        if relevant_columns:
            columns_by_table = {}
            for col in relevant_columns:
                table_name = col.get('table_name', '')
                col_name = col.get('col_name', '')
                if table_name and col_name:
                    if table_name not in columns_by_table:
                        columns_by_table[table_name] = []
                    columns_by_table[table_name].append(col_name)
            
            for table_name, cols in list(columns_by_table.items())[:3]:
                print(f"      - {table_name} è¡¨çš„åˆ—: {', '.join(cols[:5])}")
        
        _notify_step("step_2_intent_recognition", {
            "success": True,
            "query_type": intent_analysis.get('search_strategy', ''),
            "primary_entities_count": len(primary_entities),
            "entity_attributes_count": len(entity_attributes),
            "entity_metrics_count": len(entity_metrics),
            "time_dimensions_count": len(time_dimensions),
            "relationships_count": len(relationships),
            "relevant_tables_count": len(relevant_tables),
            "relevant_columns_count": len(relevant_columns),
            "primary_entities": primary_entities,
            "entity_attributes": entity_attributes,
            "entity_metrics": entity_metrics,
            "time_dimensions": time_dimensions,
            "relationships": relationships,
            "relevant_tables": relevant_tables,
            "relevant_columns": relevant_columns,
            "intent_analysis": intent_analysis
        })
        
        # æ­¥éª¤3: SQLç”Ÿæˆæµç¨‹ï¼ˆåŒ…å«ç”Ÿæˆã€æ£€æµ‹ã€çº é”™ã€ä¼˜åŒ–ã€å†æ£€æµ‹ã€æ ¸å¯¹ï¼‰
        # æ³¨æ„ï¼šå·²å–æ¶ˆè¡¨æ ¸å¯¹æ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨æ„å›¾è¯†åˆ«æ­¥éª¤ä¸­æ‰¾åˆ°çš„ç›¸å…³è¡¨
        print("\nğŸ’» æ­¥éª¤3: SQLç”Ÿæˆæµç¨‹...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³è¡¨
        if not relevant_tables:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ°ç›¸å…³è¡¨ï¼Œæ— æ³•ç”ŸæˆSQL",
                "intent_analysis": intent_analysis
            }
        
        sql_flow = SqlGenerationFlow(max_retries=3)
        sql_flow_result = sql_flow.run_flow(
            query, intent_analysis, relevant_tables, sql_id, database_info, None,
            step_callback=lambda step_name, step_data: _notify_step(step_name, step_data)
        )
        
        if not sql_flow_result.get("success"):
            error_msg = f"SQLç”Ÿæˆæµç¨‹å¤±è´¥: {sql_flow_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            _notify_step("step_3_sql_generation", {
                "success": False,
                "error": error_msg
            })
            return {
                "success": False,
                "error": error_msg,
                "intent_analysis": intent_analysis,
                "relevant_tables": relevant_tables,
                "sql_flow_result": sql_flow_result
            }
        
        generated_sql = sql_flow_result.get("sql", "")
        final_execution_result = sql_flow_result.get("final_execution_result", {})
        is_satisfied = sql_flow_result.get("is_satisfied", True)
        generation_columns_used = sql_flow_result.get("generation_columns_used", [])  # SQLç”Ÿæˆæ™ºèƒ½ä½“è¿”å›çš„åˆ—ä¿¡æ¯
        
        print(f"   âœ… æœ€ç»ˆSQL: {generated_sql}")
        print(f"   âœ… æ‰§è¡Œç»“æœ: {'æˆåŠŸ' if final_execution_result.get('executed', False) else 'å¤±è´¥'}")
        print(f"   âœ… æ»¡è¶³åº¦: {sql_flow_result.get('satisfaction_score', 1.0):.2f}")
        
        # ä»SQLç”Ÿæˆæ™ºèƒ½ä½“è¿”å›çš„åˆ—ä¿¡æ¯ä¸­æ„å»º columns_with_description
        columns_with_description = []
        tables_used_in_sql = set()  # ä½¿ç”¨seté¿å…é‡å¤
        
        # æ„å»ºè¡¨ååˆ°è¡¨IDçš„æ˜ å°„ï¼Œç”¨äºè·å–åˆ—ç±»å‹
        table_name_to_id = {}
        table_name_to_info = {}
        for table in relevant_tables:
            table_id = table.get("table_id", "")
            table_name = table.get("table_name", "")
            if table_name:
                table_name_to_id[table_name] = table_id
                table_name_to_info[table_name] = table
        
        # ä»generation_columns_usedä¸­æå–åˆ—ä¿¡æ¯
        for col_info in generation_columns_used:
            table_name = col_info.get("table_name", "")
            col_name = col_info.get("col_name", "")
            col_description = col_info.get("col_description", col_name)
            
            if table_name:
                tables_used_in_sql.add(table_name)
            
            # è·å–åˆ—ç±»å‹ï¼ˆä»è¡¨ä¿¡æ¯ä¸­æŸ¥æ‰¾ï¼‰
            col_type = "unknown"
            if table_name and table_name in table_name_to_id:
                table_id = table_name_to_id[table_name]
                columns = query_columns_by_table_id(table_id)
                for col in columns:
                    if col.get("col_name", "") == col_name:
                        col_type = col.get("col_type", "unknown")
                        break
            
            # æ„å»º table.col æ ¼å¼çš„åˆ—å
            col_name_with_table = f"{table_name}.{col_name}" if table_name else col_name
            
            columns_with_description.append({
                "table_name": table_name,
                                "col_name": col_name,
                "col_name_with_table": col_name_with_table,
                "col_type": col_type,
                "col_description": col_description,
                "col_comment": col_description
            })
        
        # å¦‚æœgeneration_columns_usedä¸ºç©ºï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹æ³•ï¼ˆé€šè¿‡SQLå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if not columns_with_description:
            print("   âš ï¸ SQLç”Ÿæˆæ™ºèƒ½ä½“æœªè¿”å›åˆ—ä¿¡æ¯ï¼Œä½¿ç”¨å­—ç¬¦ä¸²åŒ¹é…æ–¹æ³•")
            for table in relevant_tables:
                table_id = table.get("table_id", "")
                table_name = table.get("table_name", "")
                
                if table_name:
                    tables_used_in_sql.add(table_name)
                
                # è·å–è¯¥è¡¨çš„æ‰€æœ‰åˆ—ä¿¡æ¯
                columns = query_columns_by_table_id(table_id)
                for col in columns:
                    col_name = col.get("col_name", "")
                    col_info = col.get("col_info", {})
                    
                    if isinstance(col_info, str):
                        try:
                            col_info = json.loads(col_info)
                        except:
                            col_info = {}
                    
                    col_comment = col_info.get("comment", "") if isinstance(col_info, dict) else ""
                    col_type = col.get("col_type", "")
                    
                    # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åœ¨SQLä¸­è¢«ä½¿ç”¨ï¼ˆé€šè¿‡æ£€æŸ¥åˆ—åæ˜¯å¦åœ¨SQLä¸­ï¼‰
                    if col_name and col_name.lower() in generated_sql.lower():
                        # æ„å»º table.col æ ¼å¼çš„åˆ—å
                        col_name_with_table = f"{table_name}.{col_name}"
                        columns_with_description.append({
                            "table_name": table_name,
                            "col_name": col_name,
                            "col_name_with_table": col_name_with_table,
                            "col_type": col_type,
                            "col_description": col_comment if col_comment else col_name,
                            "col_comment": col_comment
                        })
        
        tables_used_in_sql = list(tables_used_in_sql)
        
        # ä»decomposition_resultä¸­è·å–é€»è¾‘è®¡ç®—ä¿¡æ¯
        logical_calculations = []
        if decomposition_result and decomposition_result.get("success"):
            logical_calcs = decomposition_result.get("logical_calculations", [])
            for lc in logical_calcs:
                logical_calculations.append({
                    "logical_operation": lc.get("logical_operation", ""),
                    "operands": lc.get("operands", []),
                    "description": lc.get("description", "")
                })
        
        _notify_step("step_3_sql_generation", {
            "success": True,
            "sql": generated_sql,
            "sql_type": sql_flow_result.get("sql_type", database_info.get("sql_type", "mysql")),
            "execution_result": final_execution_result,
            "is_satisfied": is_satisfied,
            "satisfaction_score": sql_flow_result.get("satisfaction_score", 1.0),
            "columns_with_description": columns_with_description,
            "logical_calculations": logical_calculations
        })
        
        # æ„å»º table.col æ ¼å¼çš„åˆ—ååˆ—è¡¨ï¼ˆç”¨äºè¿”å›æ•°æ®ï¼‰
        columns_with_table_prefix = []
        for col in columns_with_description:
            col_name_with_table = col.get("col_name_with_table", f"{col.get('table_name', '')}.{col.get('col_name', '')}")
            columns_with_table_prefix.append(col_name_with_table)
        
        # æ›´æ–° execution_result ä¸­çš„åˆ—åï¼Œä½¿ç”¨ table.col æ ¼å¼
        if final_execution_result and final_execution_result.get("executed"):
            original_columns = final_execution_result.get("columns", [])
            # å°†åŸå§‹åˆ—åæ˜ å°„åˆ° table.col æ ¼å¼
            updated_columns = []
            column_mapping = {}  # åŸå§‹åˆ—å -> table.col æ ¼å¼çš„æ˜ å°„
            
            for col in columns_with_description:
                original_col_name = col.get("col_name", "")
                col_name_with_table = col.get("col_name_with_table", "")
                if original_col_name and col_name_with_table:
                    column_mapping[original_col_name.lower()] = col_name_with_table
            
            # æ›´æ–°åˆ—ååˆ—è¡¨
            for orig_col in original_columns:
                mapped_col = column_mapping.get(orig_col.lower(), orig_col)
                updated_columns.append(mapped_col)
            
            # æ›´æ–°æ•°æ®å­—å…¸ä¸­çš„é”®å
            updated_data = []
            if final_execution_result.get("data"):
                for row in final_execution_result.get("data", []):
                    updated_row = {}
                    for i, orig_col in enumerate(original_columns):
                        mapped_col = column_mapping.get(orig_col.lower(), orig_col)
                        # ä»åŸå§‹è¡Œä¸­è·å–å€¼
                        if isinstance(row, dict):
                            value = row.get(orig_col)
                        else:
                            value = row[i] if i < len(row) else None
                        updated_row[mapped_col] = value
                    updated_data.append(updated_row)
            
            # æ›´æ–° execution_result
            final_execution_result["columns"] = updated_columns
            final_execution_result["data"] = updated_data
        
        # è¿”å›æœ€ç»ˆç»“æœï¼ˆåŒ…å«åˆ—æè¿°å’Œè®¡ç®—ä¿¡æ¯ï¼‰
        result = {
            "success": True,
            "sql": generated_sql,
            "sql_type": database_info.get("sql_type", "mysql"),
            "execution_result": final_execution_result,
            "is_satisfied": is_satisfied,
            "satisfaction_score": sql_flow_result.get("satisfaction_score", 1.0),
            "columns_with_description": columns_with_description,  # ç”ŸæˆSQLä½¿ç”¨çš„åˆ—åŠå…¶æè¿°
            "columns_with_table_prefix": columns_with_table_prefix,  # table.col æ ¼å¼çš„åˆ—ååˆ—è¡¨
            "logical_calculations": logical_calculations,  # åˆ†æç”¨æˆ·æ„å›¾æ—¶éœ€è¦çš„è®¡ç®—
            "tables_used": tables_used_in_sql,  # ä½¿ç”¨çš„è¡¨ååˆ—è¡¨
            "database_info": {
                "sql_id": sql_id,
                "sql_name": database_info.get("sql_name", ""),
                "sql_type": database_info.get("sql_type", "")
            }
        }
        
        print("\nâœ… Agentic SQLæ™ºèƒ½ä½“æœç´¢æµç¨‹å®Œæˆ")
        
        # é€šçŸ¥æœ€ç»ˆç»“æœ
        _notify_step("step_final_result", {
            "success": True,
            "sql": generated_sql,
            "sql_type": database_info.get("sql_type", "mysql")
        })
        
        return result
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": f"æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}"
        }


def run_logic_calculation(csv_file_path: str, query: str, logical_calculations: list,
                          columns_desc: list, columns_types: list, sql: str) -> Dict[str, Any]:
    """
    æ‰§è¡Œé€»è¾‘è®¡ç®—
    
    Args:
        csv_file_path: CSVæ–‡ä»¶è·¯å¾„
        query: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
        logical_calculations: é€»è¾‘è®¡ç®—è§„åˆ™åˆ—è¡¨
        columns_desc: åˆ—æè¿°åˆ—è¡¨ï¼ˆtable.col æ ¼å¼ï¼‰
        columns_types: åˆ—ç±»å‹åˆ—è¡¨
        sql: åŸå§‹SQLè¯­å¥
        
    Returns:
        é€»è¾‘è®¡ç®—ç»“æœï¼ŒåŒ…å«æœ€ç»ˆè§£è¯»
    """
    try:
        agent = LogicCalculationAgent()
        result = agent.calculate_logic(
            csv_file_path=csv_file_path,
            query=query,
            logical_calculations=logical_calculations,
            columns_desc=columns_desc,
            columns_types=columns_types,
            sql=sql
        )
        
        # å¦‚æœè®¡ç®—æˆåŠŸï¼Œæ·»åŠ æœ€ç»ˆç»¼åˆè§£è¯»
        if result.get("success"):
            final_interpretation = agent.interpret_final_result(
                query=query,
                logical_calculations=logical_calculations,
                calculation_result=result.get("calculation_result", {}),
                interpretation=result.get("interpretation", {}),
                tools_used=result.get("tools_used", []),
                columns_desc=columns_desc
            )
            result["final_interpretation"] = final_interpretation
        
        return result
    except Exception as e:
        traceback.print_exc()
        return {
            "success": False,
            "error": f"é€»è¾‘è®¡ç®—æ‰§è¡Œå¤±è´¥: {str(e)}"
        }


def analyze_database_descriptions(des_list: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    åˆ†ææ•°æ®åº“æè¿°æ–‡æœ¬ï¼Œè¿›è¡Œåˆ†ç±»

    Args:
        des_list: æè¿°åˆ—è¡¨ï¼Œæ ¼å¼ä¸º:
            [{"table_id": "", "title": "", "content": ""}]

    Returns:
        åˆ†ç±»åˆ†æç»“æœ
    """
    try:
        agent = DatabaseAnalysisAgent()
        result = agent.analyze_database_descriptions(des_list)

        if not result.get("success"):
            return {
                "success": False,
                "error": result.get("error", "æ•°æ®åº“æè¿°åˆ†æå¤±è´¥")
            }

        return result

    except Exception as e:
        traceback.print_exc()
        return {
            "success": False,
            "error": f"æ•°æ®åº“æè¿°åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}"
        }
