# -*- coding:utf-8 -*-
"""
è¡¨æ ¼æ–‡ä»¶åˆ†ææ™ºèƒ½ä½“ä¸»å…¥å£
æ”¯æŒ CSVã€XLSXã€XLS æ–‡ä»¶çš„æ™ºèƒ½åˆ†æ
"""

import os
import pandas as pd
import json
import time
import uuid
import logging
import numpy as np
from typing import Dict, Any, Optional, Generator, List
from pathlib import Path

from Config.llm_config import get_chat_tongyi

# å¯¼å…¥æ‰€æœ‰å·¥ä½œæ™ºèƒ½ä½“
from .file_understanding_agent import FileUnderstandingAgent
from .data_type_analysis_agent import DataTypeAnalysisAgent
from .statistics_planning_agent import StatisticsPlanningAgent
from .statistics_calculation_agent import StatisticsCalculationAgent
from .correlation_analysis_agent import CorrelationAnalysisAgent
from .semantic_analysis_agent import SemanticAnalysisAgent
from .result_interpretation_agent import ResultInterpretationAgent

# ğŸ¯ ç»Ÿä¸€ç®¡ç†ï¼šé€šè¿‡ echarts_run.py è°ƒç”¨ echarts æ™ºèƒ½ä½“
from Agent.echarts_run import query_echarts

# å¯¼å…¥è¾…åŠ©æ™ºèƒ½ä½“æµ
from .react_agent import ReActAgent
from .code_act_agent import OHCodeActAgent
from .dummy_react_agent import DummyReactAgent
from .supervision_agent import SupervisionAgent

logger = logging.getLogger(__name__)

# åˆå§‹åŒ–LLM
llm = get_chat_tongyi(temperature=0.3, enable_thinking=False)


def _convert_to_json_serializable(obj: Any) -> Any:
    """
    å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
    å¤„ç† pandas/numpy ç±»å‹ï¼ˆint64, float64ç­‰ï¼‰
    """
    if isinstance(obj, (np.integer, pd.Int64Dtype)):
        return int(obj)
    elif isinstance(obj, (np.floating, pd.Float64Dtype)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, pd.Series):
        return obj.tolist()
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict(orient='records')
    elif isinstance(obj, dict):
        return {key: _convert_to_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [_convert_to_json_serializable(item) for item in obj]
    elif pd.isna(obj):
        return None
    else:
        return obj


def _extract_sheet_indicators(sheet_name: str, sheet_stats: Dict[str, Any]) -> List[str]:
    """
    ä»å·¥ä½œè¡¨çš„ç»Ÿè®¡ç»“æœä¸­æå–å…·ä½“çš„ç»Ÿè®¡æŒ‡æ ‡ç”¨äºæ˜¾ç¤º
    è¿”å›åŒ…å«å…·ä½“æ•°å€¼çš„æŒ‡æ ‡æè¿°åˆ—è¡¨
    
    Args:
        sheet_name: å·¥ä½œè¡¨åç§°
        sheet_stats: å·¥ä½œè¡¨çš„ç»Ÿè®¡ç»“æœ
        
    Returns:
        åŒ…å«å…·ä½“æ•°å€¼çš„æŒ‡æ ‡æè¿°åˆ—è¡¨
    """
    indicators = []
    
    if not sheet_stats or not isinstance(sheet_stats, dict):
        return indicators
    
    # 1. æå–æè¿°æ€§ç»Ÿè®¡æŒ‡æ ‡ï¼ˆåŒ…å«å…·ä½“æ•°å€¼ï¼‰
    if "descriptive_statistics" in sheet_stats:
        desc_stats = sheet_stats["descriptive_statistics"]
        for col_name, stats in list(desc_stats.items())[:5]:  # å‰5åˆ—
            if isinstance(stats, dict):
                mean_val = stats.get("mean")
                median_val = stats.get("median")
                std_val = stats.get("std")
                min_val = stats.get("min")
                max_val = stats.get("max")
                
                if mean_val is not None:
                    # æ ¼å¼åŒ–æ•°å€¼
                    mean_str = f"{mean_val:.2f}" if isinstance(mean_val, (int, float)) else str(mean_val)
                    median_str = f"{median_val:.2f}" if isinstance(median_val, (int, float)) and median_val is not None else str(median_val) if median_val is not None else "N/A"
                    std_str = f"{std_val:.2f}" if isinstance(std_val, (int, float)) and std_val is not None else str(std_val) if std_val is not None else "N/A"
                    min_str = f"{min_val:.2f}" if isinstance(min_val, (int, float)) and min_val is not None else str(min_val) if min_val is not None else "N/A"
                    max_str = f"{max_val:.2f}" if isinstance(max_val, (int, float)) and max_val is not None else str(max_val) if max_val is not None else "N/A"
                    
                    indicators.append(f"{sheet_name}.{col_name}: å‡å€¼={mean_str}, ä¸­ä½æ•°={median_str}, æ ‡å‡†å·®={std_str}, èŒƒå›´=[{min_str}, {max_str}]")
    
    # 2. æå–ç›¸å…³æ€§æŒ‡æ ‡ï¼ˆåŒ…å«å…·ä½“ç›¸å…³ç³»æ•°ï¼‰
    if "correlation_analysis" in sheet_stats:
        corr_analysis = sheet_stats["correlation_analysis"]
        if isinstance(corr_analysis, dict):
            strong_corrs = corr_analysis.get("strong_correlations", [])
            for corr in strong_corrs[:3]:  # å‰3ä¸ªå¼ºç›¸å…³
                if isinstance(corr, dict):
                    var1 = corr.get("variable1", "")
                    var2 = corr.get("variable2", "")
                    corr_value = corr.get("correlation", 0)
                    if var1 and var2 and corr_value is not None:
                        indicators.append(f"{sheet_name}: {var1} â†” {var2} (r={corr_value:.3f})")
    
    # 3. æå–é¢‘ç‡æŒ‡æ ‡ï¼ˆåŒ…å«topå€¼ï¼‰
    if "frequency_analysis" in sheet_stats:
        freq_analysis = sheet_stats["frequency_analysis"]
        if isinstance(freq_analysis, dict):
            for col_name, freq in list(freq_analysis.items())[:3]:  # å‰3åˆ—
                if isinstance(freq, dict):
                    unique_count = freq.get("unique_count")
                    total_count = freq.get("total_count")
                    top_10 = freq.get("top_10", {})
                    
                    if unique_count is not None and total_count is not None:
                        top_value = ""
                        if isinstance(top_10, dict) and top_10:
                            top_item = list(top_10.items())[0]
                            top_value = f", æœ€é«˜é¢‘å€¼={top_item[0]} (å‡ºç°{top_item[1]}æ¬¡)"
                        
                        indicators.append(f"{sheet_name}.{col_name}: å”¯ä¸€å€¼æ•°={unique_count}/{total_count}{top_value}")
    
    # 4. æå–åˆ†å¸ƒæŒ‡æ ‡ï¼ˆåŒ…å«ååº¦å’Œå³°åº¦ï¼‰
    if "distribution_analysis" in sheet_stats:
        dist_analysis = sheet_stats["distribution_analysis"]
        if isinstance(dist_analysis, dict):
            for col_name, dist in list(dist_analysis.items())[:3]:  # å‰3åˆ—
                if isinstance(dist, dict):
                    skewness = dist.get("skewness")
                    kurtosis = dist.get("kurtosis")
                    dist_type = dist.get("distribution_type", "")
                    
                    if skewness is not None or kurtosis is not None:
                        skew_str = f"{skewness:.3f}" if isinstance(skewness, (int, float)) and skewness is not None else "N/A"
                        kurt_str = f"{kurtosis:.3f}" if isinstance(kurtosis, (int, float)) and kurtosis is not None else "N/A"
                        dist_type_str = f", åˆ†å¸ƒç±»å‹={dist_type}" if dist_type else ""
                        indicators.append(f"{sheet_name}.{col_name}: ååº¦={skew_str}, å³°åº¦={kurt_str}{dist_type_str}")
    
    return indicators


def _extract_chart_indicators(statistics_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»ç»Ÿè®¡ç»“æœä¸­æå–ç”¨äºç”Ÿæˆå›¾è¡¨çš„å…³é”®æŒ‡æ ‡
    åªä¿ç•™å¿…è¦çš„ç»Ÿè®¡æŒ‡æ ‡ï¼Œä¸åŒ…å«å®Œæ•´çš„æ•°æ®çŸ©é˜µ
    
    Args:
        statistics_result: å®Œæ•´çš„ç»Ÿè®¡è®¡ç®—ç»“æœ
        
    Returns:
        ç²¾ç®€åçš„ç»Ÿè®¡æŒ‡æ ‡ï¼ŒåªåŒ…å«ç”¨äºç”Ÿæˆå›¾è¡¨çš„å…³é”®ä¿¡æ¯
    """
    try:
        chart_indicators = {
            "calculations": {}
        }
        
        calculations = statistics_result.get("calculations", {})
        
        for sheet_name, sheet_stats in calculations.items():
            if not sheet_stats or isinstance(sheet_stats, dict) and "error" in sheet_stats:
                continue
            
            simplified_sheet = {}
            
            # 1. æè¿°æ€§ç»Ÿè®¡ - åªä¿ç•™å…³é”®æŒ‡æ ‡
            if "descriptive_statistics" in sheet_stats:
                desc_stats = sheet_stats["descriptive_statistics"]
                simplified_desc = {}
                for col_name, stats in list(desc_stats.items())[:20]:  # åªä¿ç•™å‰20åˆ—
                    if isinstance(stats, dict):
                        simplified_desc[col_name] = {
                            k: v for k, v in stats.items() 
                            if k in ["mean", "median", "std", "min", "max", "count", "q25", "q50", "q75"]
                        }
                if simplified_desc:
                    simplified_sheet["descriptive_statistics"] = simplified_desc
            
            # 2. ç›¸å…³æ€§åˆ†æ - åªä¿ç•™å¼ºç›¸å…³å…³ç³»ï¼Œä¸ä¿ç•™å®Œæ•´çŸ©é˜µ
            if "correlation_analysis" in sheet_stats:
                corr_analysis = sheet_stats["correlation_analysis"]
                if isinstance(corr_analysis, dict):
                    simplified_corr = {
                        "strong_correlations": corr_analysis.get("strong_correlations", [])[:20]  # åªä¿ç•™å‰20ä¸ªå¼ºç›¸å…³
                    }
                    # ä¸åŒ…å« correlation_matrixï¼Œå› ä¸ºå®ƒå¯èƒ½éå¸¸å¤§
                    if simplified_corr.get("strong_correlations"):
                        simplified_sheet["correlation_analysis"] = simplified_corr
            
            # 3. é¢‘ç‡åˆ†æ - åªä¿ç•™ top_10 æ±‡æ€»
            if "frequency_analysis" in sheet_stats:
                freq_analysis = sheet_stats["frequency_analysis"]
                simplified_freq = {}
                for col_name, freq in list(freq_analysis.items())[:10]:  # åªä¿ç•™å‰10åˆ—
                    if isinstance(freq, dict):
                        simplified_freq[col_name] = {
                            "unique_count": freq.get("unique_count"),
                            "total_count": freq.get("total_count"),
                            "top_10": freq.get("top_10", {})  # åªä¿ç•™ top_10ï¼Œä¸ä¿ç•™å®Œæ•´é¢‘ç‡åˆ†å¸ƒ
                        }
                if simplified_freq:
                    simplified_sheet["frequency_analysis"] = simplified_freq
            
            # 4. åˆ†å¸ƒåˆ†æ - åªä¿ç•™å…³é”®æŒ‡æ ‡
            if "distribution_analysis" in sheet_stats:
                dist_analysis = sheet_stats["distribution_analysis"]
                if isinstance(dist_analysis, dict):
                    simplified_dist = {}
                    for col_name, dist in list(dist_analysis.items())[:10]:  # åªä¿ç•™å‰10åˆ—
                        if isinstance(dist, dict):
                            simplified_dist[col_name] = {
                                k: v for k, v in dist.items() 
                                if k in ["skewness", "kurtosis", "distribution_type"]
                            }
                    if simplified_dist:
                        simplified_sheet["distribution_analysis"] = simplified_dist
            
            if simplified_sheet:
                chart_indicators["calculations"][sheet_name] = simplified_sheet
        
        logger.info(f"âœ… æå–å›¾è¡¨æŒ‡æ ‡å®Œæˆï¼Œç²¾ç®€å‰å·¥ä½œè¡¨æ•°: {len(calculations)}, ç²¾ç®€å: {len(chart_indicators['calculations'])}")
        return chart_indicators
        
    except Exception as e:
        logger.error(f"âŒ æå–å›¾è¡¨æŒ‡æ ‡å¤±è´¥: {e}")
        # è¿”å›ç©ºç»“æœï¼Œé¿å…å½±å“ä¸»æµç¨‹
        return {"calculations": {}}


def _create_chunk(_id: str, content: str, created: int, model: str, chunk_type: str = "text") -> Dict[str, Any]:
    """åˆ›å»ºchunkçš„è¾…åŠ©æ–¹æ³•"""
    return {
        "id": _id,
        "object": "chat.completion.chunk",
        "created": created,
        "model": model,
        "choices": [{
            "index": 0,
            "delta": {
                "content": content,
                "type": chunk_type
            },
            "finish_reason": None
        }]
    }


def read_table_file(file_path: str) -> Dict[str, Any]:
    """
    è¯»å–è¡¨æ ¼æ–‡ä»¶ï¼ˆCSVã€XLSXã€XLSï¼‰
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸ï¼š
        - file_type: æ–‡ä»¶ç±»å‹ (csv/xlsx/xls)
        - sheets: å·¥ä½œè¡¨åˆ—è¡¨ï¼ˆExcelæ–‡ä»¶ï¼‰
        - data: æ•°æ®å­—å…¸ {sheet_name: DataFrame}
        - columns_info: åˆ—ä¿¡æ¯ {sheet_name: [åˆ—ååˆ—è¡¨]}
    """
    file_extension = Path(file_path).suffix.lower()
    
    result = {
        "file_type": file_extension[1:],  # å»æ‰ç‚¹å·
        "sheets": [],
        "data": {},
        "columns_info": {},
        "file_path": file_path
    }
    
    try:
        if file_extension == ".csv":
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(file_path, encoding='utf-8')
            result["sheets"] = ["Sheet1"]
            result["data"]["Sheet1"] = df
            result["columns_info"]["Sheet1"] = df.columns.tolist()
            
        elif file_extension in [".xlsx", ".xls"]:
            # è¯»å–Excelæ–‡ä»¶
            excel_file = pd.ExcelFile(file_path)
            result["sheets"] = excel_file.sheet_names
            
            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                result["data"][sheet_name] = df
                result["columns_info"][sheet_name] = df.columns.tolist()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}")
            
        logger.info(f"âœ… æˆåŠŸè¯»å–è¡¨æ ¼æ–‡ä»¶: {file_path}, å…± {len(result['sheets'])} ä¸ªå·¥ä½œè¡¨")
        return result
        
    except Exception as e:
        logger.error(f"âŒ è¯»å–è¡¨æ ¼æ–‡ä»¶å¤±è´¥: {e}")
        raise


def run_table_analysis_stream(file_path: str, query: str = "", 
                              step_callback: Optional[callable] = None) -> Generator[Dict[str, Any], None, None]:
    """
    è¿è¡Œè¡¨æ ¼æ–‡ä»¶åˆ†ææ™ºèƒ½ä½“æµç¨‹ï¼ˆæµå¼è¿”å›ï¼‰
    
    Args:
        file_path: è¡¨æ ¼æ–‡ä»¶è·¯å¾„ï¼ˆCSVã€XLSXã€XLSï¼‰
        query: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜ï¼ˆå¯é€‰ï¼‰
        step_callback: æ­¥éª¤å›è°ƒå‡½æ•° step_callback(step_name: str, step_data: Dict[str, Any])
        
    Yields:
        OpenAIæ ¼å¼çš„æµå¼å“åº”å—
    """
    _id = f"table-analysis-{uuid.uuid4().hex[:16]}"
    created = int(time.time())
    model = "table-analysis-model"
    
    def _notify_step(step_name: str, step_data: Dict[str, Any]):
        """é€šçŸ¥æ­¥éª¤å®Œæˆ"""
        if step_callback:
            try:
                # è½¬æ¢ step_data ä¸­çš„ pandas/numpy ç±»å‹ä¸º JSON å¯åºåˆ—åŒ–ç±»å‹
                serializable_data = _convert_to_json_serializable(step_data)
                step_callback(step_name, serializable_data)
            except Exception as e:
                logger.error(f"âš ï¸ æ­¥éª¤å›è°ƒå¤±è´¥ ({step_name}): {e}")
    
    def _supervise_step(step_name: str, step_result: Any, previous_steps: List[Dict[str, Any]], 
                       task_context: Dict[str, Any]) -> Dict[str, Any]:
        """ç›‘ç£æ­¥éª¤æ‰§è¡Œ"""
        try:
            # å‡†å¤‡æ­¥éª¤ç»“æœï¼ˆå¦‚æœæ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åŒ…è£…ï¼‰
            if isinstance(step_result, dict):
                result_dict = step_result
            else:
                result_dict = {"result": step_result}
            
            # è°ƒç”¨ç›‘ç£æ™ºèƒ½ä½“
            supervision_result = supervision_agent.supervise_step(
                step_name=step_name,
                step_result=result_dict,
                previous_steps=previous_steps,
                task_context=task_context
            )
            
            supervision_results.append({
                "step": step_name,
                "supervision": supervision_result
            })
            
            # å¦‚æœå‘ç°é—®é¢˜ï¼Œè®°å½•è­¦å‘Š
            overall = supervision_result.get("overall", {})
            if overall.get("status") == "fail":
                logger.error(f"âŒ æ­¥éª¤ {step_name} ç›‘ç£è¯„ä¼°å¤±è´¥: {overall.get('summary')}")
            elif overall.get("status") == "warning":
                logger.warning(f"âš ï¸ æ­¥éª¤ {step_name} ç›‘ç£è¯„ä¼°è­¦å‘Š: {overall.get('summary')}")
            
            return supervision_result
            
        except Exception as e:
            logger.error(f"âŒ ç›‘ç£æ£€æŸ¥å¤±è´¥ ({step_name}): {e}")
            return {}
    
    try:
        # æ­¥éª¤0: è¯»å–æ–‡ä»¶
        _notify_step("step_0_file_reading", {"status": "started"})
        file_info = read_table_file(file_path)
        _notify_step("step_0_file_reading", {
            "success": True,
            "file_type": file_info["file_type"],
            "sheets_count": len(file_info["sheets"]),
            "sheets": file_info["sheets"]
        })
        
        # ç”Ÿæˆåˆå§‹å“åº”
        yield {
            "id": _id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "content": f"ğŸ“Š å¼€å§‹åˆ†æè¡¨æ ¼æ–‡ä»¶: {os.path.basename(file_path)}\n",
                    "type": "text"
                },
                "finish_reason": None
            }]
        }
        
        # åˆå§‹åŒ–æ‰€æœ‰æ™ºèƒ½ä½“
        file_understanding_agent = FileUnderstandingAgent()
        data_type_analysis_agent = DataTypeAnalysisAgent()
        statistics_planning_agent = StatisticsPlanningAgent()
        statistics_calculation_agent = StatisticsCalculationAgent()
        correlation_analysis_agent = CorrelationAnalysisAgent()
        semantic_analysis_agent = SemanticAnalysisAgent()
        result_interpretation_agent = ResultInterpretationAgent()
        # ğŸ¯ ç»Ÿä¸€ç®¡ç†ï¼šé€šè¿‡ echarts_run.py è°ƒç”¨ echarts æ™ºèƒ½ä½“
        # ä¸éœ€è¦åˆå§‹åŒ– echarts_agentï¼Œç›´æ¥ä½¿ç”¨ echarts_run.py ä¸­çš„å‡½æ•°
        
        # è¾…åŠ©æ™ºèƒ½ä½“æµ
        react_agent = ReActAgent(max_iterations=3)
        code_act_agent = OHCodeActAgent()
        dummy_react_agent = DummyReactAgent()
        supervision_agent = SupervisionAgent()  # ğŸ¯ ç›‘ç£æ™ºèƒ½ä½“
        
        step_results = []
        supervision_results = []  # å­˜å‚¨ç›‘ç£ç»“æœ
        
        # åˆå§‹åŒ–å˜é‡ï¼Œé¿å…åç»­æ­¥éª¤å› å˜é‡æœªå®šä¹‰è€Œå¤±è´¥
        file_understanding_result = None
        data_type_analysis_result = None
        statistics_plan_result = None
        statistics_result = None
        correlation_analysis_result = None
        semantic_analysis_result = None
        
        # ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆç”¨äºç›‘ç£æ™ºèƒ½ä½“ï¼‰
        task_context = {
            "file_path": file_path,
            "file_name": os.path.basename(file_path),
            "query": query,
            "file_info": file_info
        }
        
        # æ­¥éª¤1: æ–‡ä»¶ç†è§£æ™ºèƒ½ä½“
        _notify_step("step_1_file_understanding", {"status": "started"})
        yield _create_chunk(_id, "ğŸ” æ­¥éª¤1: æ–‡ä»¶ç†è§£æ™ºèƒ½ä½“ - åˆ†ææ–‡ä»¶ç»“æ„å’Œç”¨æˆ·éœ€æ±‚\n", created, model)
        
        try:
            file_understanding_result = file_understanding_agent.analyze(file_info, query)
            _notify_step("step_1_file_understanding", {"success": True, "result": file_understanding_result})
            step_results.append({"step": "file_understanding", "success": True})
            
            # ğŸ¯ ç›‘ç£æ£€æŸ¥
            _supervise_step("file_understanding", file_understanding_result, [], task_context)
            
            yield _create_chunk(_id, f"âœ… å®Œæˆ\n- è¯†åˆ«åˆ° {len(file_understanding_result.get('key_columns', []))} ä¸ªå…³é”®åˆ—\n", created, model)
        except Exception as e:
            _notify_step("step_1_file_understanding", {"success": False, "error": str(e)})
            step_results.append({"step": "file_understanding", "success": False, "error": str(e)})
            yield _create_chunk(_id, f"âŒ å¤±è´¥: {str(e)}\n", created, model)
        
        # æ­¥éª¤2: æ•°æ®ç±»å‹åˆ†ææ™ºèƒ½ä½“
        _notify_step("step_2_data_type_analysis", {"status": "started"})
        yield _create_chunk(_id, "\nğŸ“Š æ­¥éª¤2: æ•°æ®ç±»å‹åˆ†ææ™ºèƒ½ä½“ - åˆ†æåˆ—çš„æ•°æ®ç±»å‹å’Œæ•°æ®é‡\n", created, model)
        
        try:
            data_type_analysis_result = data_type_analysis_agent.analyze(file_info)
            _notify_step("step_2_data_type_analysis", {"success": True, "result": data_type_analysis_result})
            step_results.append({"step": "data_type_analysis", "success": True})
            
            # ğŸ¯ ç›‘ç£æ£€æŸ¥
            _supervise_step("data_type_analysis", data_type_analysis_result, step_results, task_context)
            
            total_cols = sum(len(s.get("columns_analysis", [])) for s in data_type_analysis_result.get("sheets_analysis", []))
            yield _create_chunk(_id, f"âœ… å®Œæˆ\n- å…±åˆ†æ {total_cols} ä¸ªåˆ—\n", created, model)
        except Exception as e:
            _notify_step("step_2_data_type_analysis", {"success": False, "error": str(e)})
            step_results.append({"step": "data_type_analysis", "success": False, "error": str(e)})
            yield _create_chunk(_id, f"âŒ å¤±è´¥: {str(e)}\n", created, model)
        
        # æ­¥éª¤3: ç»Ÿè®¡è®¡ç®—è§„åˆ’æ™ºèƒ½ä½“
        _notify_step("step_3_statistics_planning", {"status": "started"})
        yield _create_chunk(_id, "\nğŸ“‹ æ­¥éª¤3: ç»Ÿè®¡è®¡ç®—è§„åˆ’æ™ºèƒ½ä½“ - è§„åˆ’ç»Ÿè®¡è®¡ç®—æ–¹æ¡ˆ\n", created, model)
        
        try:
            if file_understanding_result and data_type_analysis_result:
                statistics_plan_result = statistics_planning_agent.plan(
                    file_understanding_result, data_type_analysis_result
                )
                _notify_step("step_3_statistics_planning", {"success": True, "result": statistics_plan_result})
                step_results.append({"step": "statistics_planning", "success": True})

                # ğŸ¯ ç›‘ç£æ£€æŸ¥
                _supervise_step("statistics_planning", statistics_plan_result, step_results, task_context)

                plans_count = len(statistics_plan_result.get("statistics_plan", {}).get("sheets_plans", []))
                yield _create_chunk(_id, f"âœ… å®Œæˆ\n- ä¸º {plans_count} ä¸ªå·¥ä½œè¡¨åˆ¶å®šäº†ç»Ÿè®¡è®¡åˆ’\n", created, model)
            else:
                raise ValueError("ç¼ºå°‘å‰ç½®æ­¥éª¤ç»“æœï¼ˆæ–‡ä»¶ç†è§£æˆ–æ•°æ®ç±»å‹åˆ†æï¼‰")
        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡è®¡ç®—è§„åˆ’å¤±è´¥: {e}", exc_info=True)
            _notify_step("step_3_statistics_planning", {"success": False, "error": str(e)})
            step_results.append({"step": "statistics_planning", "success": False, "error": str(e)})
            yield _create_chunk(_id, f"âŒ å¤±è´¥: {str(e)}\n", created, model)
            # åˆ›å»ºé»˜è®¤è§„åˆ’ï¼Œä»¥ä¾¿åç»­æ­¥éª¤å¯ä»¥ç»§ç»­
            statistics_plan_result = {
                "statistics_plan": {
                    "overall_strategy": "é»˜è®¤ç»Ÿè®¡ç­–ç•¥",
                    "sheets_plans": []
                },
                "recommendations": []
            }
        
        # æ­¥éª¤4: æ•°ç†ç»Ÿè®¡æ™ºèƒ½ä½“
        _notify_step("step_4_statistics_calculation", {"status": "started"})
        yield _create_chunk(_id, "\nğŸ”¢ æ­¥éª¤4: æ•°ç†ç»Ÿè®¡æ™ºèƒ½ä½“ - æ‰§è¡Œç»Ÿè®¡è®¡ç®—\n", created, model)
        
        try:
            if statistics_plan_result:
                # ğŸ¯ ä¼ é€’ file_understanding_result ç”¨äºä¸šåŠ¡è¯­ä¹‰
                statistics_result = statistics_calculation_agent.calculate(
                    file_info,
                    statistics_plan_result,
                    file_understanding_result
                )

                # ğŸ¯ ç›‘ç£æ£€æŸ¥
                _supervise_step("statistics_calculation", statistics_result, step_results, task_context)

                # ğŸ¯ æ•°æ®éªŒè¯ï¼šæ£€æŸ¥ç»Ÿè®¡è®¡ç®—ç»“æœ
                calculations = statistics_result.get("calculations", {})
                calc_count = len(calculations)

                # ğŸ¯ æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº† ECharts ç»“æ„
                echarts_structures = statistics_result.get("echarts_structures", {})
                if echarts_structures:
                    echarts_count = sum(len(structs) for structs in echarts_structures.values())
                    logger.info(f"âœ… æ­¥éª¤4ç”Ÿæˆäº† {echarts_count} ä¸ª ECharts ç»“æ„")
                    yield _create_chunk(_id, f"- ç”Ÿæˆäº† {echarts_count} ä¸ª ECharts ç»“æ„æ•°æ®\n", created, model)

                success_message = f"âœ… å®Œæˆ\n- å®Œæˆäº† {calc_count} ä¸ªå·¥ä½œè¡¨çš„ç»Ÿè®¡è®¡ç®—\n"

                if calc_count == 0:
                    logger.warning("âš ï¸ ç»Ÿè®¡è®¡ç®—ç»“æœä¸ºç©ºï¼Œcalculations å­—å…¸ä¸ºç©º")
                    yield _create_chunk(_id, "âš ï¸ è­¦å‘Šï¼šç»Ÿè®¡è®¡ç®—ç»“æœä¸ºç©º\n", created, model)
                else:
                    # ğŸ¯ æå–å¹¶æ˜¾ç¤ºå…·ä½“çš„ç»Ÿè®¡æŒ‡æ ‡ï¼ˆä¸èƒ½ç©ºæ˜¾ç¤ºæè¿°ï¼‰
                    indicators_summary = []
                    empty_count = 0

                    for sheet_name, sheet_stats in calculations.items():
                        if not sheet_stats or (isinstance(sheet_stats, dict) and len(sheet_stats) == 0):
                            empty_count += 1
                            logger.warning(f"âš ï¸ å·¥ä½œè¡¨ {sheet_name} çš„ç»Ÿè®¡ç»“æœä¸ºç©º")
                        elif isinstance(sheet_stats, dict) and "error" in sheet_stats:
                            empty_count += 1
                            logger.error(f"âŒ å·¥ä½œè¡¨ {sheet_name} ç»Ÿè®¡è®¡ç®—å‡ºé”™: {sheet_stats.get('error')}")
                        else:
                            # ğŸ¯ è®°å½•æ¯ä¸ªå·¥ä½œè¡¨åŒ…å«çš„ç»Ÿè®¡ç±»å‹
                            stat_types = list(sheet_stats.keys()) if isinstance(sheet_stats, dict) else []
                            logger.info(f"âœ… å·¥ä½œè¡¨ {sheet_name} ç»Ÿè®¡è®¡ç®—å®Œæˆï¼ŒåŒ…å«ç»Ÿè®¡ç±»å‹: {', '.join(stat_types)}")

                            # ğŸ¯ æå–å…·ä½“çš„ç»Ÿè®¡æŒ‡æ ‡æ•°å€¼ç”¨äºæ˜¾ç¤º
                            sheet_indicators = _extract_sheet_indicators(sheet_name, sheet_stats)
                            if sheet_indicators:
                                indicators_summary.extend(sheet_indicators)

                    if empty_count == calc_count:
                        logger.error("âŒ æ‰€æœ‰å·¥ä½œè¡¨çš„ç»Ÿè®¡ç»“æœéƒ½ä¸ºç©ºæˆ–å‡ºé”™")
                        success_message = f"âš ï¸ éƒ¨åˆ†å®Œæˆ\n- {calc_count} ä¸ªå·¥ä½œè¡¨ä¸­æ‰€æœ‰ç»Ÿè®¡éƒ½å¤±è´¥\n"
                    elif empty_count > 0:
                        logger.warning(f"âš ï¸ {empty_count}/{calc_count} ä¸ªå·¥ä½œè¡¨çš„ç»Ÿè®¡ç»“æœä¸ºç©ºæˆ–å‡ºé”™")
                        success_message = f"âš ï¸ éƒ¨åˆ†å®Œæˆ\n- {calc_count - empty_count}/{calc_count} ä¸ªå·¥ä½œè¡¨ç»Ÿè®¡æˆåŠŸ\n"

                    # ğŸ¯ è¾“å‡ºå…·ä½“çš„ç»Ÿè®¡æŒ‡æ ‡ï¼ˆåŒ…å«æ•°æ®ä¾æ®ï¼‰
                    if indicators_summary:
                        yield _create_chunk(_id, f"\nğŸ“Š å…³é”®ç»Ÿè®¡æŒ‡æ ‡ï¼š\n", created, model)
                        for indicator in indicators_summary[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ªå…³é”®æŒ‡æ ‡
                            yield _create_chunk(_id, f"- {indicator}\n", created, model)

                # ğŸ¯ æˆåŠŸé€šçŸ¥å’Œè®°å½•
                _notify_step("step_4_statistics_calculation", {"success": True, "result": statistics_result})
                step_results.append({"step": "statistics_calculation", "success": True})

                yield _create_chunk(_id, success_message, created, model)
            else:
                raise ValueError("ç¼ºå°‘ç»Ÿè®¡è®¡ç®—è§„åˆ’ç»“æœ")
        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}", exc_info=True)
            _notify_step("step_4_statistics_calculation", {"success": False, "error": str(e)})
            step_results.append({"step": "statistics_calculation", "success": False, "error": str(e)})
            yield _create_chunk(_id, f"âŒ å¤±è´¥: {str(e)}\n", created, model)
            # åˆ›å»ºé»˜è®¤ç»“æœï¼Œä»¥ä¾¿åç»­æ­¥éª¤å¯ä»¥ç»§ç»­
            statistics_result = {"calculations": {}}
        
        # æ­¥éª¤5: å…³è”åˆ†ææ™ºèƒ½ä½“
        _notify_step("step_5_correlation_analysis", {"status": "started"})
        yield _create_chunk(_id, "\nğŸ”— æ­¥éª¤5: å…³è”åˆ†ææ™ºèƒ½ä½“ - è¿›è¡Œå…³è”åˆ†æ\n", created, model)
        
        try:
            if statistics_result and data_type_analysis_result:
                correlation_analysis_result = correlation_analysis_agent.analyze(
                    statistics_result, data_type_analysis_result
                )
                _notify_step("step_5_correlation_analysis", {"success": True, "result": correlation_analysis_result})
                step_results.append({"step": "correlation_analysis", "success": True})

                # ğŸ¯ ç›‘ç£æ£€æŸ¥
                _supervise_step("correlation_analysis", correlation_analysis_result, step_results, task_context)

                strong_corrs = len(correlation_analysis_result.get("strong_correlations", []))
                yield _create_chunk(_id, f"âœ… å®Œæˆ\n- å‘ç° {strong_corrs} ä¸ªå¼ºç›¸å…³å…³ç³»\n", created, model)
            else:
                raise ValueError("ç¼ºå°‘å‰ç½®æ­¥éª¤ç»“æœï¼ˆç»Ÿè®¡è®¡ç®—æˆ–æ•°æ®ç±»å‹åˆ†æï¼‰")
        except Exception as e:
            logger.error(f"âŒ å…³è”åˆ†æå¤±è´¥: {e}", exc_info=True)
            _notify_step("step_5_correlation_analysis", {"success": False, "error": str(e)})
            step_results.append({"step": "correlation_analysis", "success": False, "error": str(e)})
            yield _create_chunk(_id, f"âŒ å¤±è´¥: {str(e)}\n", created, model)
            # åˆ›å»ºé»˜è®¤ç»“æœ
            correlation_analysis_result = {"strong_correlations": [], "moderate_correlations": []}
        
        # æ­¥éª¤6: è¯­ä¹‰åˆ†ææ™ºèƒ½ä½“
        _notify_step("step_6_semantic_analysis", {"status": "started"})
        yield _create_chunk(_id, "\nğŸ§  æ­¥éª¤6: è¯­ä¹‰åˆ†ææ™ºèƒ½ä½“ - ç†è§£åˆ—çš„è¯­ä¹‰å¹¶è¿›è¡Œæ·±åº¦åˆ†æ\n", created, model)
        
        try:
            if (file_understanding_result and data_type_analysis_result and 
                statistics_result and correlation_analysis_result):
                semantic_analysis_result = semantic_analysis_agent.analyze(
                    file_understanding_result,
                    data_type_analysis_result,
                    statistics_result,
                    correlation_analysis_result
                )
                _notify_step("step_6_semantic_analysis", {"success": True, "result": semantic_analysis_result})
                step_results.append({"step": "semantic_analysis", "success": True})

                # ğŸ¯ ç›‘ç£æ£€æŸ¥
                _supervise_step("semantic_analysis", semantic_analysis_result, step_results, task_context)

                patterns = len(semantic_analysis_result.get("semantic_analysis", {}).get("business_patterns", []))
                yield _create_chunk(_id, f"âœ… å®Œæˆ\n- è¯†åˆ«äº† {patterns} ä¸ªä¸šåŠ¡æ¨¡å¼\n", created, model)
            else:
                raise ValueError("ç¼ºå°‘å‰ç½®æ­¥éª¤ç»“æœ")
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰åˆ†æå¤±è´¥: {e}", exc_info=True)
            _notify_step("step_6_semantic_analysis", {"success": False, "error": str(e)})
            step_results.append({"step": "semantic_analysis", "success": False, "error": str(e)})
            yield _create_chunk(_id, f"âŒ å¤±è´¥: {str(e)}\n", created, model)
            # åˆ›å»ºé»˜è®¤ç»“æœ
            semantic_analysis_result = {"semantic_analysis": {"business_patterns": []}}
        
        # æ­¥éª¤7: ç»“æœè§£è¯»æ™ºèƒ½ä½“
        _notify_step("step_7_result_interpretation", {"status": "started"})
        yield _create_chunk(_id, "\nğŸ“ æ­¥éª¤7: ç»“æœè§£è¯»æ™ºèƒ½ä½“ - ç»¼åˆè§£è¯»åˆ†æç»“æœ\n", created, model)
        
        try:
            if (file_understanding_result and data_type_analysis_result and 
                statistics_result and correlation_analysis_result and semantic_analysis_result):
                interpretation_text = result_interpretation_agent.interpret(
                query if query else "åˆ†æè¡¨æ ¼æ•°æ®",
                file_understanding_result,
                data_type_analysis_result,
                statistics_result,
                correlation_analysis_result,
                semantic_analysis_result
                )
                _notify_step("step_7_result_interpretation", {"success": True, "result": interpretation_text})
                step_results.append({"step": "result_interpretation", "success": True})
                
                # ğŸ¯ ç›‘ç£æ£€æŸ¥
                _supervise_step("result_interpretation", {"interpretation": interpretation_text}, step_results, task_context)
            
                # æµå¼è¾“å‡ºè§£è¯»ç»“æœ
                yield _create_chunk(_id, "âœ… å®Œæˆ\n\n", created, model)
                yield _create_chunk(_id, interpretation_text + "\n\n", created, model)
            else:
                raise ValueError("ç¼ºå°‘å‰ç½®æ­¥éª¤ç»“æœ")
        except Exception as e:
            logger.error(f"âŒ ç»“æœè§£è¯»å¤±è´¥: {e}", exc_info=True)
            _notify_step("step_7_result_interpretation", {"success": False, "error": str(e)})
            step_results.append({"step": "result_interpretation", "success": False, "error": str(e)})
            yield _create_chunk(_id, f"âŒ å¤±è´¥: {str(e)}\n", created, model)
        
        # æ­¥éª¤8: EChartsç”Ÿæˆæ™ºèƒ½ä½“ï¼ˆç»Ÿä¸€ä½¿ç”¨ EchartsAgentï¼‰
        _notify_step("step_8_echarts_generation", {"status": "started"})
        yield _create_chunk(_id, "\nğŸ“ˆ æ­¥éª¤8: EChartsç”Ÿæˆæ™ºèƒ½ä½“ - ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨\n", created, model)
        
        try:
            if (statistics_result and correlation_analysis_result and
                semantic_analysis_result):
                # ğŸ¯ æ•°æ®éªŒè¯ï¼šæ£€æŸ¥ç»Ÿè®¡ç»“æœæ˜¯å¦åŒ…å«å®é™…æ•°æ®
                calculations = statistics_result.get("calculations", {})
                if not calculations:
                    logger.warning("âš ï¸ statistics_result.calculations ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
                    yield _create_chunk(_id, "âš ï¸ è­¦å‘Šï¼šç»Ÿè®¡è®¡ç®—ç»“æœä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨\n", created, model)
                    raise ValueError("ç»Ÿè®¡è®¡ç®—ç»“æœä¸ºç©º")
                
                # æ£€æŸ¥æ¯ä¸ªå·¥ä½œè¡¨çš„ç»Ÿè®¡ç»“æœæ˜¯å¦ä¸ºç©º
                empty_sheets = []
                for sheet_name, sheet_stats in calculations.items():
                    if not sheet_stats or (isinstance(sheet_stats, dict) and len(sheet_stats) == 0):
                        empty_sheets.append(sheet_name)
                    elif isinstance(sheet_stats, dict) and "error" in sheet_stats:
                        logger.warning(f"âš ï¸ å·¥ä½œè¡¨ {sheet_name} ç»Ÿè®¡è®¡ç®—å‡ºé”™: {sheet_stats.get('error')}")
                        empty_sheets.append(sheet_name)
                
                if len(empty_sheets) == len(calculations):
                    logger.error("âŒ æ‰€æœ‰å·¥ä½œè¡¨çš„ç»Ÿè®¡ç»“æœéƒ½ä¸ºç©º")
                    yield _create_chunk(_id, "âŒ é”™è¯¯ï¼šæ‰€æœ‰å·¥ä½œè¡¨çš„ç»Ÿè®¡ç»“æœéƒ½ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨\n", created, model)
                    raise ValueError("æ‰€æœ‰å·¥ä½œè¡¨çš„ç»Ÿè®¡ç»“æœéƒ½ä¸ºç©º")
                
                if empty_sheets:
                    logger.warning(f"âš ï¸ ä»¥ä¸‹å·¥ä½œè¡¨çš„ç»Ÿè®¡ç»“æœä¸ºç©º: {', '.join(empty_sheets)}")
                
                charts = []
                
                # ğŸ¯ ä¼˜å…ˆä½¿ç”¨æ­¥éª¤4ç”Ÿæˆçš„ ECharts ç»“æ„ï¼ˆåŸºäºç»Ÿè®¡æŒ‡æ ‡ç”Ÿæˆï¼Œä¸æ˜¯åŸå§‹æ•°æ®ï¼‰
                echarts_structures = statistics_result.get("echarts_structures", {})
                if echarts_structures:
                    logger.info(f"âœ… ä¼˜å…ˆä½¿ç”¨æ­¥éª¤4ç”Ÿæˆçš„ ECharts ç»“æ„ï¼Œå…± {sum(len(s) for s in echarts_structures.values())} ä¸ª")
                    for sheet_name, structures in echarts_structures.items():
                        for struct in structures:
                            if struct.get("echarts_config"):
                                charts.append({
                                    "type": struct.get("chart_type", "bar"),
                                    "title": struct.get("title", f"{sheet_name} - å›¾è¡¨"),
                                    "description": f"åŸºäºç»Ÿè®¡æŒ‡æ ‡ç”Ÿæˆï¼ˆ{struct.get('type', 'unknown')}ï¼‰",
                                    "echarts_config": struct["echarts_config"],
                                    "source": "statistics_calculation"
                                })
                    logger.info(f"âœ… ä»æ­¥éª¤4æ·»åŠ äº† {len(charts)} ä¸ª ECharts å›¾è¡¨")
                
                # è®°å½•å·²ç”Ÿæˆçš„å›¾è¡¨æ ‡é¢˜ï¼Œé¿å…é‡å¤
                existing_titles = {chart.get("title", "") for chart in charts}
                
                # ğŸ¯ æå–ç”¨äºç”Ÿæˆå›¾è¡¨çš„å…³é”®æŒ‡æ ‡ï¼ˆä¸åŒ…å«å®Œæ•´æ•°æ®çŸ©é˜µï¼‰
                # åªæå–ç»Ÿè®¡æŒ‡æ ‡ï¼Œä¸åŒ…å«ç›¸å…³æ€§çŸ©é˜µã€å®Œæ•´é¢‘ç‡åˆ†å¸ƒç­‰å¤§æ•°æ®
                chart_indicators = _extract_chart_indicators(statistics_result)

                # ğŸ¯ æ£€æŸ¥æå–çš„æŒ‡æ ‡æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®
                def _has_valid_chart_data(indicators: Dict[str, Any]) -> bool:
                    """æ£€æŸ¥å›¾è¡¨æŒ‡æ ‡æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®"""
                    if not indicators or not isinstance(indicators, dict):
                        return False

                    calculations = indicators.get("calculations", {})
                    if not calculations:
                        return False

                    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å·¥ä½œè¡¨åŒ…å«æœ‰æ•ˆæ•°æ®
                    for sheet_name, sheet_stats in calculations.items():
                        if not sheet_stats or not isinstance(sheet_stats, dict):
                            continue

                        # æ£€æŸ¥æè¿°æ€§ç»Ÿè®¡æ˜¯å¦æœ‰æ•°æ®
                        desc_stats = sheet_stats.get("descriptive_statistics", {})
                        if desc_stats and isinstance(desc_stats, dict):
                            for col_name, stats in desc_stats.items():
                                if isinstance(stats, dict) and any(stats.get(key) is not None for key in ["mean", "median", "std", "min", "max"]):
                                    return True

                        # æ£€æŸ¥ç›¸å…³æ€§åˆ†ææ˜¯å¦æœ‰æ•°æ®
                        corr_stats = sheet_stats.get("correlation_analysis", {})
                        if corr_stats and isinstance(corr_stats, dict):
                            strong_corrs = corr_stats.get("strong_correlations", [])
                            if strong_corrs and len(strong_corrs) > 0:
                                return True

                        # æ£€æŸ¥é¢‘ç‡åˆ†ææ˜¯å¦æœ‰æ•°æ®
                        freq_stats = sheet_stats.get("frequency_analysis", {})
                        if freq_stats and isinstance(freq_stats, dict):
                            for col_name, freq_data in freq_stats.items():
                                if isinstance(freq_data, dict):
                                    total_count = freq_data.get("total_count", 0)
                                    unique_count = freq_data.get("unique_count", 0)
                                    top_10 = freq_data.get("top_10", {})
                                    if total_count > 0 or unique_count > 0 or (top_10 and len(top_10) > 0):
                                        return True

                    return False

                has_valid_data = _has_valid_chart_data(chart_indicators)
                if not has_valid_data:
                    logger.warning("âš ï¸ æå–çš„å›¾è¡¨æŒ‡æ ‡ä¸ºç©ºæˆ–ä¸åŒ…å«æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
                    # ç›´æ¥è·³è½¬åˆ°æœ€ç»ˆè¾“å‡ºï¼Œä¸ç”Ÿæˆä»»ä½•å›¾è¡¨
                    _notify_step("step_8_echarts_generation", {"success": True, "charts": [], "reason": "no_valid_data"})
                    step_results.append({"step": "echarts_generation", "success": True})

                    yield _create_chunk(_id, f"âœ… å®Œæˆ\nâš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ç»Ÿè®¡æŒ‡æ ‡æ•°æ®ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ\n", created, model)

                    # ç›‘ç£æ™ºèƒ½ä½“æ£€æŸ¥
                    supervision_result = dummy_react_agent.supervise(
                        f"åˆ†æè¡¨æ ¼æ–‡ä»¶: {os.path.basename(file_path)}",
                        {"file_info": file_info},
                        step_results
                    )

                    yield _create_chunk(_id, f"\nâœ… è¡¨æ ¼æ–‡ä»¶åˆ†æå®Œæˆï¼è¿›åº¦: {supervision_result.get('progress', 0)*100:.1f}%\n", created, model)

                    # å‘é€å®Œæˆæ ‡è®°
                    yield {
                        "id": _id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {},
                            "finish_reason": "stop"
                        }]
                    }
                    return
                
                # ğŸ¯ éªŒè¯ï¼šç¡®è®¤ä¸åŒ…å«å®Œæ•´æ•°æ®çŸ©é˜µ
                for sheet_name, sheet_stats in chart_indicators.get("calculations", {}).items():
                    if "correlation_analysis" in sheet_stats:
                        corr = sheet_stats["correlation_analysis"]
                        if isinstance(corr, dict):
                            if "correlation_matrix" in corr:
                                logger.error(f"âŒ é”™è¯¯ï¼šç²¾ç®€åçš„æ•°æ®ä»åŒ…å« correlation_matrixï¼å·¥ä½œè¡¨: {sheet_name}")
                                # å¼ºåˆ¶ç§»é™¤
                                corr.pop("correlation_matrix", None)
                                logger.warning(f"âš ï¸ å·²å¼ºåˆ¶ç§»é™¤ correlation_matrix")
                            if "strong_correlations" not in corr:
                                logger.warning(f"âš ï¸ å·¥ä½œè¡¨ {sheet_name} çš„ç›¸å…³æ€§åˆ†æä¸­æ²¡æœ‰ strong_correlations")
                    
                    if "frequency_analysis" in sheet_stats:
                        freq = sheet_stats["frequency_analysis"]
                        for col_name, freq_data in freq.items():
                            if isinstance(freq_data, dict) and "frequency" in freq_data:
                                logger.error(f"âŒ é”™è¯¯ï¼šç²¾ç®€åçš„æ•°æ®ä»åŒ…å«å®Œæ•´ frequency å­—å…¸ï¼å·¥ä½œè¡¨: {sheet_name}, åˆ—: {col_name}")
                                # å¼ºåˆ¶ç§»é™¤
                                freq_data.pop("frequency", None)
                                logger.warning(f"âš ï¸ å·²å¼ºåˆ¶ç§»é™¤å®Œæ•´ frequency å­—å…¸")
                
                # ğŸ¯ æ•°æ®åºåˆ—åŒ–ï¼šä½¿ç”¨ _convert_to_json_serializable ç¡®ä¿æ•°æ®å¯åºåˆ—åŒ–
                serializable_indicators = _convert_to_json_serializable(chart_indicators)
                data_str = json.dumps(serializable_indicators, ensure_ascii=False, default=str)
                
                # ğŸ¯ æœ€ç»ˆéªŒè¯ï¼šç¡®è®¤æ•°æ®ä¸­ä¸åŒ…å« correlation_matrix å’Œå®Œæ•´ frequency
                if "correlation_matrix" in data_str:
                    logger.error("âŒ ä¸¥é‡é”™è¯¯ï¼šåºåˆ—åŒ–åçš„æ•°æ®ä»åŒ…å« correlation_matrixï¼")
                    # å°è¯•ç§»é™¤
                    data_dict = json.loads(data_str)
                    for sheet_stats in data_dict.get("calculations", {}).values():
                        if isinstance(sheet_stats, dict) and "correlation_analysis" in sheet_stats:
                            sheet_stats["correlation_analysis"].pop("correlation_matrix", None)
                    data_str = json.dumps(data_dict, ensure_ascii=False, default=str)
                    logger.warning("âš ï¸ å·²ä»åºåˆ—åŒ–æ•°æ®ä¸­ç§»é™¤ correlation_matrix")
                
                # ğŸ¯ æ•°æ®éªŒè¯ï¼šæ£€æŸ¥åºåˆ—åŒ–åçš„æ•°æ®æ˜¯å¦ä¸ºç©º
                if not data_str or data_str == "{}" or data_str == '{"calculations": {}}':
                    logger.error("âŒ æå–çš„å›¾è¡¨æŒ‡æ ‡ä¸ºç©º")
                    yield _create_chunk(_id, "âŒ é”™è¯¯ï¼šæå–çš„å›¾è¡¨æŒ‡æ ‡ä¸ºç©º\n", created, model)
                    raise ValueError("æå–çš„å›¾è¡¨æŒ‡æ ‡ä¸ºç©º")
                
                # ğŸ¯ æ•°æ®å¤§å°æ£€æŸ¥ï¼šå¦‚æœä»ç„¶å¤ªå¤§ï¼Œè¿›ä¸€æ­¥ç²¾ç®€
                if len(data_str) > 50000:  # 50KBï¼ˆæ›´ä¸¥æ ¼çš„é™åˆ¶ï¼‰
                    logger.warning(f"âš ï¸ å›¾è¡¨æŒ‡æ ‡ä»ç„¶è¿‡å¤§ï¼ˆ{len(data_str)}å­—ç¬¦ï¼‰ï¼Œè¿›è¡Œè¿›ä¸€æ­¥ç²¾ç®€")
                    # è¿›ä¸€æ­¥ç²¾ç®€ï¼šåªä¿ç•™æœ€å…³é”®çš„æŒ‡æ ‡
                    ultra_simplified = {
                        "calculations": {}
                    }
                    for sheet_name, sheet_stats in chart_indicators.get("calculations", {}).items():
                        ultra_sheet = {}
                        # åªä¿ç•™æè¿°æ€§ç»Ÿè®¡çš„å…³é”®æŒ‡æ ‡
                        if "descriptive_statistics" in sheet_stats:
                            desc = sheet_stats["descriptive_statistics"]
                            ultra_sheet["descriptive_statistics"] = {
                                col: {k: v for k, v in stats.items() if k in ["mean", "median", "std"]}
                                for col, stats in list(desc.items())[:10]  # åªä¿ç•™å‰10åˆ—
                            }
                        # åªä¿ç•™å¼ºç›¸å…³å…³ç³»
                        if "correlation_analysis" in sheet_stats:
                            corr = sheet_stats["correlation_analysis"]
                            if corr.get("strong_correlations"):
                                ultra_sheet["correlation_analysis"] = {
                                    "strong_correlations": corr["strong_correlations"][:10]  # åªä¿ç•™å‰10ä¸ª
                                }
                        if ultra_sheet:
                            ultra_simplified["calculations"][sheet_name] = ultra_sheet
                    
                    serializable_indicators = _convert_to_json_serializable(ultra_simplified)
                    data_str = json.dumps(serializable_indicators, ensure_ascii=False, default=str)
                    logger.info(f"âœ… è¿›ä¸€æ­¥ç²¾ç®€åé•¿åº¦: {len(data_str)} å­—ç¬¦")
                
                # ğŸ¯ æœ€ç»ˆç¡®è®¤ï¼šè®°å½•æ•°æ®å¤§å°å’Œå†…å®¹æ‘˜è¦
                logger.info(f"ğŸ“Š å‡†å¤‡ç”Ÿæˆå›¾è¡¨ï¼Œå›¾è¡¨æŒ‡æ ‡é•¿åº¦: {len(data_str)} å­—ç¬¦ï¼ˆåŸå§‹ç»Ÿè®¡ç»“æœå·²ç²¾ç®€ï¼‰ï¼Œå·¥ä½œè¡¨æ•°: {len(calculations)}")
                
                # éªŒè¯æ•°æ®å†…å®¹æ‘˜è¦
                try:
                    data_dict = json.loads(data_str)
                    for sheet_name, sheet_stats in data_dict.get("calculations", {}).items():
                        if "correlation_analysis" in sheet_stats:
                            corr = sheet_stats["correlation_analysis"]
                            has_matrix = "correlation_matrix" in corr if isinstance(corr, dict) else False
                            strong_corr_count = len(corr.get("strong_correlations", [])) if isinstance(corr, dict) else 0
                            logger.info(f"âœ… å·¥ä½œè¡¨ {sheet_name} ç›¸å…³æ€§åˆ†æï¼šåŒ…å«çŸ©é˜µ={has_matrix}ï¼Œå¼ºç›¸å…³å…³ç³»æ•°={strong_corr_count}")
                            if has_matrix:
                                logger.error(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæ•°æ®ä¸­ä»åŒ…å« correlation_matrixï¼")
                        if "frequency_analysis" in sheet_stats:
                            freq = sheet_stats["frequency_analysis"]
                            freq_cols = len(freq) if isinstance(freq, dict) else 0
                            logger.info(f"âœ… å·¥ä½œè¡¨ {sheet_name} é¢‘ç‡åˆ†æï¼šåˆ—æ•°={freq_cols}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ•°æ®éªŒè¯å¤±è´¥: {e}")
                
                # 1. ğŸ¯ åŸºäºç»Ÿè®¡ç»“æœç”Ÿæˆæ™ºèƒ½å›¾è¡¨æ¨è
                # ğŸ¯ æ€§èƒ½ä¼˜åŒ–ï¼šé™åˆ¶å›¾è¡¨ç”Ÿæˆæ•°é‡ï¼Œé¿å…è¿‡å¤š LLM è°ƒç”¨
                # åŸºäºç»Ÿè®¡ç»“æœçš„ç±»å‹æ™ºèƒ½é€‰æ‹©å›¾è¡¨
                smart_recommendations = []

                # æ£€æŸ¥æ˜¯å¦æœ‰æè¿°æ€§ç»Ÿè®¡
                if any("descriptive_statistics" in sheet_stats for sheet_stats in calculations.values() if sheet_stats):
                    smart_recommendations.append({
                        "title": "æè¿°æ€§ç»Ÿè®¡åˆ†æ",
                        "description": "å±•ç¤ºå„åˆ—çš„ç»Ÿè®¡ç‰¹å¾",
                        "query": "ç”Ÿæˆæè¿°æ€§ç»Ÿè®¡çš„æŸ±çŠ¶å›¾æˆ–ç®±çº¿å›¾ï¼Œå±•ç¤ºå„æ•°å€¼åˆ—çš„å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ç­‰ç»Ÿè®¡ç‰¹å¾",
                        "priority": "high"
                    })

                # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ€§åˆ†æ
                if any("correlation_analysis" in sheet_stats for sheet_stats in calculations.values() if sheet_stats):
                    smart_recommendations.append({
                        "title": "ç›¸å…³æ€§çƒ­åŠ›å›¾",
                        "description": "å±•ç¤ºå˜é‡é—´çš„ç›¸å…³å…³ç³»",
                        "query": "ç”Ÿæˆå˜é‡é—´ç›¸å…³æ€§çš„çƒ­åŠ›å›¾ï¼Œçªå‡ºæ˜¾ç¤ºå¼ºç›¸å…³å…³ç³»",
                        "priority": "high"
                    })

                # æ£€æŸ¥æ˜¯å¦æœ‰é¢‘ç‡åˆ†æ
                if any("frequency_analysis" in sheet_stats for sheet_stats in calculations.values() if sheet_stats):
                    smart_recommendations.append({
                        "title": "é¢‘ç‡åˆ†å¸ƒåˆ†æ",
                        "description": "å±•ç¤ºåˆ†ç±»å˜é‡çš„é¢‘ç‡åˆ†å¸ƒ",
                        "query": "ç”Ÿæˆåˆ†ç±»å˜é‡é¢‘ç‡åˆ†å¸ƒçš„æŸ±çŠ¶å›¾æˆ–é¥¼å›¾",
                        "priority": "medium"
                    })

                # é™åˆ¶ä¸ºæœ€å¤š2ä¸ªå›¾è¡¨
                selected_recommendations = smart_recommendations[:2]
                logger.info(f"âœ… åŸºäºç»Ÿè®¡ç»“æœæ™ºèƒ½æ¨èï¼Œå…± {len(selected_recommendations)} ä¸ªå›¾è¡¨")

                for rec in selected_recommendations:
                        chart_title = rec.get("title", "å›¾è¡¨")
                        # å¦‚æœå·²ç»å­˜åœ¨ï¼Œè·³è¿‡
                        if chart_title in existing_titles:
                            logger.info(f"â­ï¸ è·³è¿‡é‡å¤å›¾è¡¨: {chart_title}ï¼ˆå·²åœ¨æ­¥éª¤4ç”Ÿæˆï¼‰")
                            continue
                        
                        try:
                            # ğŸ¯ ä½¿ç”¨æ™ºèƒ½æ¨èçš„æŸ¥è¯¢
                            enhanced_query = rec.get("query", "")

                            # ğŸ¯ é€šè¿‡ echarts_run.py è°ƒç”¨ echarts æ™ºèƒ½ä½“
                            echarts_result = query_echarts(chart_indicators, enhanced_query)
                            if echarts_result and "echarts_config" in echarts_result:
                                echarts_config = echarts_result["echarts_config"]
                                if echarts_config and isinstance(echarts_config, dict) and len(echarts_config) > 0:
                                    charts.append({
                                        "type": rec.get("chart_type", "bar"),
                                        "title": chart_title,
                                        "description": rec.get("description", ""),
                                        "echarts_config": echarts_config,
                                        "source": "summary_analysis"
                                    })
                                    existing_titles.add(chart_title)
                                    continue
                        except Exception as e:
                            logger.warning(f"âš ï¸ è°ƒç”¨EChartsAgentç”Ÿæˆæ±‡æ€»æ¨èå›¾è¡¨å¤±è´¥: {e}")
                
                # 2. ä»å…³è”åˆ†æç»“æœç”Ÿæˆå›¾è¡¨ï¼ˆå¦‚æœæ­¥éª¤4å’Œæ±‡æ€»åˆ†æéƒ½æ²¡æœ‰ç”Ÿæˆï¼‰
                # ğŸ¯ æ€§èƒ½ä¼˜åŒ–ï¼šåªåœ¨æ²¡æœ‰è¶³å¤Ÿå›¾è¡¨æ—¶æ‰ç”Ÿæˆå…³è”åˆ†æå›¾è¡¨
                if len(charts) < 3:  # å¦‚æœå·²æœ‰å›¾è¡¨å°‘äº3ä¸ªï¼Œæ‰ç”Ÿæˆå…³è”åˆ†æå›¾è¡¨
                    correlation_charts = correlation_analysis_result.get("recommended_charts", [])[:1]  # æœ€å¤š1ä¸ª
                    for chart_rec in correlation_charts:
                        chart_title = chart_rec.get("title", "å›¾è¡¨")
                        # å¦‚æœå·²ç»å­˜åœ¨ï¼Œè·³è¿‡
                        if chart_title in existing_titles:
                            logger.info(f"â­ï¸ è·³è¿‡é‡å¤å›¾è¡¨: {chart_title}ï¼ˆå·²åœ¨æ­¥éª¤4ç”Ÿæˆï¼‰")
                            continue
                        try:
                            chart_type = chart_rec.get("chart_type", "")
                            title = chart_rec.get("title", "å›¾è¡¨")
                            query = f"ç”Ÿæˆ{title}çš„{chart_type}å›¾è¡¨"

                            # ğŸ¯ é€šè¿‡ echarts_run.py è°ƒç”¨ echarts æ™ºèƒ½ä½“
                            echarts_result = query_echarts(chart_indicators, query)
                            if echarts_result and "echarts_config" in echarts_result:
                                echarts_config = echarts_result["echarts_config"]
                                if echarts_config and isinstance(echarts_config, dict) and len(echarts_config) > 0:
                                    charts.append({
                                        "type": chart_type,
                                        "title": title,
                                        "description": chart_rec.get("description", ""),
                                        "echarts_config": echarts_config
                                    })
                                    continue
                        except Exception as e:
                            logger.warning(f"âš ï¸ è°ƒç”¨EChartsAgentç”Ÿæˆæ¨èå›¾è¡¨å¤±è´¥: {e}")

                        # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸºæœ¬é…ç½®
                        charts.append({
                            "type": chart_rec.get("chart_type", ""),
                            "title": chart_rec.get("title", "å›¾è¡¨"),
                            "description": chart_rec.get("description", ""),
                            "config": {
                                "chart_type": chart_rec.get("chart_type", ""),
                                "title": chart_rec.get("title", "å›¾è¡¨")
                            }
                        })

                # 3. ä»è¯­ä¹‰åˆ†æç»“æœç”Ÿæˆå›¾è¡¨ï¼ˆå¦‚æœæ­¥éª¤4å’Œæ±‡æ€»åˆ†æéƒ½æ²¡æœ‰ç”Ÿæˆï¼‰
                # ğŸ¯ æ€§èƒ½ä¼˜åŒ–ï¼šåªåœ¨å›¾è¡¨æ•°é‡ä¸è¶³æ—¶æ‰ç”Ÿæˆè¯­ä¹‰åˆ†æå›¾è¡¨
                if len(charts) < 4:  # å¦‚æœå·²æœ‰å›¾è¡¨å°‘äº4ä¸ªï¼Œæ‰ç”Ÿæˆè¯­ä¹‰åˆ†æå›¾è¡¨
                    semantic_analyses = semantic_analysis_result.get("semantic_analysis", {}).get("recommended_analysis", [])[:1]  # æœ€å¤š1ä¸ª
                    for analysis_rec in semantic_analyses:
                        chart_type = analysis_rec.get("expected_chart", "bar")
                        title = f"{analysis_rec.get('analysis_type', 'åˆ†æ')} - {', '.join(analysis_rec.get('target_columns', []))}"
                        # å¦‚æœå·²ç»å­˜åœ¨ï¼Œè·³è¿‡
                        if title in existing_titles:
                            logger.info(f"â­ï¸ è·³è¿‡é‡å¤å›¾è¡¨: {title}ï¼ˆå·²åœ¨æ­¥éª¤4ç”Ÿæˆï¼‰")
                            continue

                        try:
                            query = f"ç”Ÿæˆ{title}çš„{chart_type}å›¾è¡¨"

                            # ğŸ¯ é€šè¿‡ echarts_run.py è°ƒç”¨ echarts æ™ºèƒ½ä½“
                            echarts_result = query_echarts(chart_indicators, query)
                            if echarts_result and "echarts_config" in echarts_result:
                                echarts_config = echarts_result["echarts_config"]
                                if echarts_config and isinstance(echarts_config, dict) and len(echarts_config) > 0:
                                    charts.append({
                                        "type": chart_type,
                                        "title": title,
                                        "description": analysis_rec.get("reason", ""),
                                        "echarts_config": echarts_config
                                    })
                                    continue
                        except Exception as e:
                            logger.warning(f"âš ï¸ è°ƒç”¨EChartsAgentç”Ÿæˆè¯­ä¹‰å›¾è¡¨å¤±è´¥: {e}")

                        # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸºæœ¬é…ç½®
                        charts.append({
                            "type": analysis_rec.get("expected_chart", "bar"),
                            "title": f"{analysis_rec.get('analysis_type', 'åˆ†æ')} - {', '.join(analysis_rec.get('target_columns', []))}",
                            "description": analysis_rec.get("reason", ""),
                            "config": {
                                "chart_type": analysis_rec.get("expected_chart", "bar"),
                                "title": f"{analysis_rec.get('analysis_type', 'åˆ†æ')} - {', '.join(analysis_rec.get('target_columns', []))}"
                            }
                        })

                # 4. ä»ç»Ÿè®¡ç»“æœç”Ÿæˆé»˜è®¤å›¾è¡¨ï¼ˆå¦‚æœæ­¥éª¤4å’Œæ±‡æ€»åˆ†æéƒ½æ²¡æœ‰ç”Ÿæˆï¼‰
                # ğŸ¯ æ€§èƒ½ä¼˜åŒ–ï¼šåªåœ¨å›¾è¡¨æ•°é‡ä¸è¶³æ—¶æ‰ç”Ÿæˆé»˜è®¤å›¾è¡¨
                if len(charts) < 5:  # å¦‚æœå·²æœ‰å›¾è¡¨å°‘äº5ä¸ªï¼Œæ‰ç”Ÿæˆé»˜è®¤å›¾è¡¨
                    default_sheets = list(statistics_result.get("calculations", {}).items())[:2]  # æœ€å¤šå¤„ç†2ä¸ªå·¥ä½œè¡¨
                    for sheet_name, sheet_stats in default_sheets:
                        # æè¿°æ€§ç»Ÿè®¡ - æŸ±çŠ¶å›¾
                        desc_title = f"{sheet_name} - æè¿°æ€§ç»Ÿè®¡"
                        if desc_title in existing_titles:
                            logger.info(f"â­ï¸ è·³è¿‡é‡å¤å›¾è¡¨: {desc_title}ï¼ˆå·²åœ¨æ­¥éª¤4ç”Ÿæˆï¼‰")
                        elif "descriptive_statistics" in sheet_stats:
                            try:
                                query = f"ç”Ÿæˆ{sheet_name}çš„æè¿°æ€§ç»Ÿè®¡æŸ±çŠ¶å›¾ï¼Œå±•ç¤ºå„åˆ—çš„å‡å€¼ã€ä¸­ä½æ•°ç­‰ç»Ÿè®¡æŒ‡æ ‡"
                                # ğŸ¯ é€šè¿‡ echarts_run.py è°ƒç”¨ echarts æ™ºèƒ½ä½“
                                echarts_result = query_echarts(chart_indicators, query)
                                if echarts_result and "echarts_config" in echarts_result:
                                    echarts_config = echarts_result["echarts_config"]
                                    if echarts_config and isinstance(echarts_config, dict) and len(echarts_config) > 0:
                                        charts.append({
                                            "type": "bar",
                                            "title": f"{sheet_name} - æè¿°æ€§ç»Ÿè®¡",
                                            "description": "å±•ç¤ºå„åˆ—çš„å‡å€¼ã€ä¸­ä½æ•°ç­‰ç»Ÿè®¡æŒ‡æ ‡",
                                            "echarts_config": echarts_config
                                        })
                                        continue
                            except Exception as e:
                                logger.warning(f"âš ï¸ è°ƒç”¨EChartsAgentç”Ÿæˆæè¿°æ€§ç»Ÿè®¡å›¾è¡¨å¤±è´¥: {e}")

                            charts.append({
                                "type": "bar",
                                "title": f"{sheet_name} - æè¿°æ€§ç»Ÿè®¡",
                                "description": "å±•ç¤ºå„åˆ—çš„å‡å€¼ã€ä¸­ä½æ•°ç­‰ç»Ÿè®¡æŒ‡æ ‡",
                                "config": {
                                    "chart_type": "bar",
                                    "title": f"{sheet_name} - æè¿°æ€§ç»Ÿè®¡"
                                }
                            })

                        # ç›¸å…³æ€§åˆ†æ - çƒ­åŠ›å›¾
                        corr_title = f"{sheet_name} - ç›¸å…³æ€§çƒ­åŠ›å›¾"
                        if corr_title in existing_titles:
                            logger.info(f"â­ï¸ è·³è¿‡é‡å¤å›¾è¡¨: {corr_title}ï¼ˆå·²åœ¨æ­¥éª¤4ç”Ÿæˆï¼‰")
                        elif "correlation_analysis" in sheet_stats:
                            try:
                                query = f"ç”Ÿæˆ{sheet_name}çš„ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼Œå±•ç¤ºå˜é‡é—´çš„ç›¸å…³æ€§"
                                # ğŸ¯ é€šè¿‡ echarts_run.py è°ƒç”¨ echarts æ™ºèƒ½ä½“
                                echarts_result = query_echarts(chart_indicators, query)
                                if echarts_result and "echarts_config" in echarts_result:
                                    echarts_config = echarts_result["echarts_config"]
                                    if echarts_config and isinstance(echarts_config, dict) and len(echarts_config) > 0:
                                        charts.append({
                                            "type": "heatmap",
                                            "title": f"{sheet_name} - ç›¸å…³æ€§çƒ­åŠ›å›¾",
                                            "description": "å±•ç¤ºå˜é‡é—´çš„ç›¸å…³æ€§",
                                            "echarts_config": echarts_config
                                        })
                                        continue
                            except Exception as e:
                                logger.warning(f"âš ï¸ è°ƒç”¨EChartsAgentç”Ÿæˆç›¸å…³æ€§çƒ­åŠ›å›¾å¤±è´¥: {e}")

                            charts.append({
                                "type": "heatmap",
                                "title": f"{sheet_name} - ç›¸å…³æ€§çƒ­åŠ›å›¾",
                                "description": "å±•ç¤ºå˜é‡é—´çš„ç›¸å…³æ€§",
                                "config": {
                                    "chart_type": "heatmap",
                                    "title": f"{sheet_name} - ç›¸å…³æ€§çƒ­åŠ›å›¾"
                                }
                            })
                
                _notify_step("step_8_echarts_generation", {"success": True, "result": charts})
                step_results.append({"step": "echarts_generation", "success": True})

                # ğŸ¯ ç›‘ç£æ£€æŸ¥
                _supervise_step("echarts_generation", {"charts": charts, "count": len(charts)}, step_results, task_context)
                
                logger.info(f"âœ… EChartså›¾è¡¨ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(charts)} ä¸ªå›¾è¡¨")
                yield _create_chunk(_id, f"âœ… å®Œæˆ\n- ç”Ÿæˆäº† {len(charts)} ä¸ªå›¾è¡¨é…ç½®\n", created, model)
                
                # è¾“å‡ºå›¾è¡¨é…ç½®ï¼ˆä»¥ ECharts æ ¼å¼ï¼‰
                if charts:
                    yield _create_chunk(_id, "\n## ğŸ“Š ç”Ÿæˆçš„å›¾è¡¨\n\n", created, model)
                    for i, chart in enumerate(charts, 1):
                        # å…ˆè¾“å‡ºå›¾è¡¨æè¿°æ–‡æœ¬
                        chart_desc = f"{i}. **{chart.get('title', 'å›¾è¡¨')}** ({chart.get('type', 'unknown')})\n"
                        yield _create_chunk(_id, chart_desc, created, model)
                        
                        # è·å–å›¾è¡¨é…ç½®
                        chart_config = chart.get("config", {})
                        echarts_config = chart.get("echarts_config")
                        
                        # å¦‚æœå·²ç»æœ‰å®Œæ•´çš„ echarts_configï¼Œç›´æ¥ä½¿ç”¨
                        if echarts_config:
                            echarts_option = echarts_config
                        else:
                            # å¦åˆ™ï¼Œä» config æ„å»ºåŸºæœ¬çš„ ECharts é…ç½®
                            chart_type = chart.get("type", "bar")
                            title = chart.get("title", "å›¾è¡¨")
                            
                            # æ„å»ºåŸºæœ¬çš„ ECharts option
                            echarts_option = {
                                "title": {
                                    "text": title,
                                    "left": "center"
                                },
                                "tooltip": {
                                    "trigger": "axis" if chart_type in ["bar", "line"] else "item"
                                },
                                "xAxis": {
                                    "type": "category" if chart_type in ["bar", "line"] else None,
                                    "data": []
                                },
                                "yAxis": {
                                    "type": "value"
                                },
                                "series": [{
                                    "name": title,
                                    "type": chart_type,
                                    "data": []
                                }]
                            }
                        
                        # å°† ECharts é…ç½®è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ï¼Œä½¿ç”¨ option= æ ¼å¼
                        echarts_json = f"option={json.dumps(echarts_option, ensure_ascii=False)}"
                        
                        # å‘é€ ECharts chunkï¼ˆä½¿ç”¨ echarts ç±»å‹ï¼‰
                        yield _create_chunk(_id, echarts_json, created, model, chunk_type="echarts")
            else:
                raise ValueError("ç¼ºå°‘å‰ç½®æ­¥éª¤ç»“æœ")
        except Exception as e:
            logger.error(f"âŒ EChartsç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            _notify_step("step_8_echarts_generation", {"success": False, "error": str(e)})
            step_results.append({"step": "echarts_generation", "success": False, "error": str(e)})
            yield _create_chunk(_id, f"âŒ å¤±è´¥: {str(e)}\n", created, model)
        
        # ç›‘ç£æ™ºèƒ½ä½“æ£€æŸ¥
        supervision_result = dummy_react_agent.supervise(
            f"åˆ†æè¡¨æ ¼æ–‡ä»¶: {os.path.basename(file_path)}",
            {"file_info": file_info},
            step_results
        )
        
        yield _create_chunk(_id, f"\nâœ… è¡¨æ ¼æ–‡ä»¶åˆ†æå®Œæˆï¼è¿›åº¦: {supervision_result.get('progress', 0)*100:.1f}%\n", created, model)
        
        # å‘é€å®Œæˆæ ‡è®°
        yield {
            "id": _id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        
    except Exception as e:
        logger.error(f"âŒ è¡¨æ ¼æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        error_text = f"âŒ è¡¨æ ¼æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}"
        
        yield {
            "id": _id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "content": error_text,
                    "type": "text"
                },
                "finish_reason": "stop"
            }]
        }
