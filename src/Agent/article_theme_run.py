import asyncio
import re
import json
from langgraph.graph import StateGraph, START, END
from Agent.ArticleThemeAgent.theme_agent import AgentState
from Agent.ArticleThemeAgent.theme_agent import preprocess_text
from Agent.ArticleThemeAgent.theme_agent import extract_title
from Agent.ArticleThemeAgent.theme_agent import extract_summary
from Agent.ArticleThemeAgent.theme_agent import extract_toc
from Agent.ArticleThemeAgent.theme_agent import extract_metadata
from Agent.ArticleThemeAgent.theme_agent import classify_doctype
from Agent.ArticleThemeAgent.theme_agent import extract_authors


# After all parallel nodes are done, end the graph
# We need a joining node to wait for all parallel tasks to complete.
# Let's add a simple collector node.
def collector_node(state: AgentState) -> AgentState:
    """
    A simple node that acts as a synchronization point for the parallel branches.
    
    Regarding state merging: LangGraph automatically merges the state updates from
    the parallel nodes. When each node updates a *different* key in the AgentState
    (e.g., one updates 'summary', another updates 'toc'), the updates are combined
    into the final state dictionary.
    
    If multiple nodes were to update the *same* key, we would need a special
    reducer function (like the `operator.add` we use for the 'error' key) to tell
    LangGraph how to combine the values.
    
    This node itself doesn't need to perform any merging logic; its purpose is
    simply to ensure that all parallel extraction tasks have completed before
    the graph proceeds to the END.
    """
    print("---(Node: Collector)---")
    print("   All extraction tasks complete. State has been merged.")
    return {}

async def run(sample_text):
    # --- 1. Define the Graph ---
    # Create a new graph
    workflow = StateGraph(AgentState)
    
    # Add the nodes to the graph
    workflow.add_node("preprocess", preprocess_text)
    workflow.add_node("extract_title", extract_title)
    workflow.add_node("extract_summary", extract_summary)
    workflow.add_node("extract_toc", extract_toc)
    workflow.add_node("extract_metadata", extract_metadata)
    workflow.add_node("classify_doctype", classify_doctype)
    workflow.add_node("extract_authors", extract_authors)
    
    # --- 2. Define the Edges ---
    # The graph starts with the preprocessing node
    workflow.add_edge(START, "preprocess")
    
    # After preprocessing, run all extraction nodes in parallel
    workflow.add_edge("preprocess", "extract_title")
    workflow.add_edge("preprocess", "extract_summary")
    workflow.add_edge("preprocess", "extract_toc")
    workflow.add_edge("preprocess", "extract_metadata")
    workflow.add_edge("preprocess", "classify_doctype")
    workflow.add_edge("preprocess", "extract_authors")
    
    workflow.add_node("collector", collector_node)
    workflow.add_edge("extract_title", "collector")
    workflow.add_edge("extract_summary", "collector")
    workflow.add_edge("extract_toc", "collector")
    workflow.add_edge("extract_metadata", "collector")
    workflow.add_edge("classify_doctype", "collector")
    workflow.add_edge("extract_authors", "collector")
    
    # The collector node transitions to the end
    workflow.add_edge("collector", END)
    
    # --- 3. Compile the Graph ---
    app = workflow.compile()
    
    initial_state = {"text": sample_text}
    
    final_state = app.invoke(initial_state)
    # return final_state  # Return the final state dictionary
    # Use json.dumps for pretty printing the dictionary
    # Ensure ensure_ascii=False to correctly display Chinese characters
    final_state_json = json.dumps(final_state, indent=2, ensure_ascii=False)
    return final_state_json

def clean_json_string(json_str: str) -> str:
    """
    æ¸…ç† JSON å­—ç¬¦ä¸²ï¼Œå»é™¤ä»£ç å—æ ‡è®°å’Œå¤šä½™çš„ç©ºç™½
    
    Args:
        json_str: å¯èƒ½åŒ…å«ä»£ç å—æ ‡è®°çš„ JSON å­—ç¬¦ä¸²
        
    Returns:
        str: æ¸…ç†åçš„ JSON å­—ç¬¦ä¸²
    """
    if not json_str:
        return json_str
    
    # å»é™¤é¦–å°¾ç©ºç™½
    json_str = json_str.strip()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤å¼€å¤´çš„ ```json æˆ– ``` æ ‡è®°
    # åŒ¹é…å¼€å¤´çš„ ```json æˆ– ```ï¼ˆå¯èƒ½åŒ…å«æ¢è¡Œç¬¦å’Œç©ºç™½ï¼‰
    json_str = re.sub(r'^```(?:json)?\s*\n?', '', json_str, flags=re.IGNORECASE | re.MULTILINE)
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤ç»“å°¾çš„ ``` æ ‡è®°ï¼ˆå¯èƒ½åŒ…å«æ¢è¡Œç¬¦å’Œç©ºç™½ï¼‰
    json_str = re.sub(r'\n?\s*```\s*$', '', json_str, flags=re.MULTILINE)
    
    # å†æ¬¡å»é™¤é¦–å°¾ç©ºç™½å’Œæ¢è¡Œç¬¦
    json_str = json_str.strip()
    
    return json_str

def run_sync(sample_text):
    # result = asyncio.create_task(run(sample_text))
    # loop = asyncio.get_event_loop()
    # if loop.is_running():
    #     # If the event loop is already running, we need to run the async task in a different way
    #     result = loop.create_task(run(sample_text))
    # else:
    #     # If the event loop is not running, we can run the async task directly
    #     result = loop.run_until_complete(run(sample_text))   
    # print("Text length:", len(sample_text))
    # print("Result type:", type(result))
    # print("Result:", result)
    # return result  # Wait for the async task to complete and return the result
    res_j = asyncio.run(run(sample_text))
    # æ¸…ç† JSON å­—ç¬¦ä¸²ï¼Œå»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
    cleaned_json = clean_json_string(res_j)
    result = json.loads(cleaned_json)
    return result

# # --- 4. Define a Sample Input Text ---
# # A comprehensive sample document for testing
# sample_text = """
# # åŸå¸‚å¤§è„‘å»ºè®¾ä¸è¿è¥ç®¡ç†æ ‡å‡†
#
# **å‘å¸ƒæ—¥æœŸ**: 2023-11-01
# **ç”Ÿæ•ˆæ—¥æœŸ**: 2023-12-01
# **ä½œåºŸæ—¥æœŸ**: 2033-12-01
#
# **èµ·è‰å•ä½**: æœªæ¥åŸå¸‚ç ”ç©¶ä¸­å¿ƒ
# **ä½œè€…**: å¼ ä¸‰, æå››
#
# **é€‚ç”¨èŒƒå›´**: æœ¬æ ‡å‡†é€‚ç”¨äºä¸­åäººæ°‘å…±å’Œå›½å››å·çœæˆéƒ½å¸‚çš„åŸå¸‚å¤§è„‘é¡¹ç›®ã€‚
#
# ---
#
# ## **æ‘˜è¦**
#
# æœ¬æ–‡æ¡£è§„å®šäº†åŸå¸‚å¤§è„‘ï¼ˆCity Brainï¼‰é¡¹ç›®çš„å»ºè®¾ã€è¿è¥å’Œç®¡ç†çš„ç›¸å…³æ ‡å‡†ä¸è¦æ±‚ï¼Œæ—¨åœ¨ç¡®ä¿é¡¹ç›®çš„è§„èŒƒæ€§ã€å®‰å…¨æ€§å’Œé«˜æ•ˆæ€§ã€‚
#
# ---
#
# ## **ç›®å½•**
#
# 1.  **å¼•è¨€**
#     1.1. èƒŒæ™¯
#     1.2. ç›®çš„
# 2.  **æ ¸å¿ƒæŠ€æœ¯è¦æ±‚**
#     2.1. æ•°æ®èåˆå¹³å°
#     2.2. AIç®—æ³•å¼•æ“
#     2.3. å®‰å…¨ä½“ç³»
# 3.  **è¿è¥ç®¡ç†è§„èŒƒ**
#     3.1. ç»„ç»‡æ¶æ„
#     3.2. åº”æ€¥é¢„æ¡ˆ
# 4.  **é™„å½•**
#     4.1. åè¯è§£é‡Š
#
# ---
#
# ## **1. å¼•è¨€**
#
# ### **1.1. èƒŒæ™¯**
#
# éšç€ä¿¡æ¯æŠ€æœ¯çš„é£é€Ÿå‘å±•ï¼ŒåŸå¸‚ç®¡ç†é¢ä¸´ç€å‰æ‰€æœªæœ‰çš„æœºé‡ä¸æŒ‘æˆ˜ã€‚
#
# ### **1.2. ç›®çš„**
#
# æœ¬æ ‡å‡†çš„ç›®çš„æ˜¯ä¸ºäº†ç»Ÿä¸€å’Œè§„èŒƒæˆéƒ½å¸‚åŸå¸‚å¤§è„‘çš„å»ºè®¾ä¸è¿è¥æµç¨‹ã€‚
#
# ... (æ­£æ–‡å†…å®¹çœç•¥) ...
# """

# # --- 5. Run the Graph ---
# if __name__ == "__main__":
#     print("ğŸš€ Starting the document analysis process...")
#
#     # The initial state for the graph
#     initial_state = {"text": sample_text}
#
#     # Invoke the graph with the initial state
#     # The `stream` method provides real-time updates from each node
#     final_state = app.invoke(initial_state)
#
#     # Print the final, structured output
#     print("\n\nâœ… Document analysis complete!")
#     print("--- Final Result ---")
#
#     # Use json.dumps for pretty printing the dictionary
#     # Ensure ensure_ascii=False to correctly display Chinese characters
#     final_state_json = json.dumps(final_state, indent=2, ensure_ascii=False)
#     print(final_state_json)
#
#     # You can also access individual keys
#     # print("\n--- Extracted Summary ---")
#     # print(final_state.get('summary'))
