"""
Elasticsearch æ§åˆ¶å±‚
æä¾›å¯¹ Elasticsearch æ•°æ®åº“çš„é«˜çº§æ“ä½œå’Œä¸šåŠ¡é€»è¾‘å°è£…
"""

import json
import time
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from Db.elastic_db import get_elasticsearch_instance, ElasticSearchDB
from Config.elasticsearch_config import is_elasticsearch_enabled
from Config.embedding_config import get_embeddings, get_vector_length
import logging

logger = logging.getLogger(__name__)


class CControl():
    """Elasticsearch æ§åˆ¶å±‚"""

    def __init__(self):
        """åˆå§‹åŒ–æ§åˆ¶å™¨"""
        self.enabled = is_elasticsearch_enabled()
        if self.enabled:
            self.es_db: ElasticSearchDB = get_elasticsearch_instance()
            # åˆå§‹åŒ– embedding æ¨¡å‹ç”¨äºç”Ÿæˆå‘é‡
            try:
                self.embeddings = get_embeddings()
                self.vector_dimension = get_vector_length()
                logger.info(f"Elasticsearch å‘é‡æœç´¢å·²å¯ç”¨ï¼Œå‘é‡ç»´åº¦: {self.vector_dimension}")
            except Exception as e:
                logger.warning(f"æ— æ³•åˆå§‹åŒ– embedding æ¨¡å‹: {e}ï¼Œå°†ä»…ä½¿ç”¨å…¨æ–‡æœç´¢")
                self.embeddings = None
                self.vector_dimension = None
        else:
            self.es_db = None
            self.embeddings = None
            self.vector_dimension = None
            logger.info("Elasticsearchå·²ç¦ç”¨ï¼ˆELASTICSEARCG_FLAG=Falseï¼‰ï¼Œè·³è¿‡åˆå§‹åŒ–")
        self.index_name = "knowledge_base"  # é»˜è®¤ç´¢å¼•åç§°

    def split_text_with_overlap(self, text: str, chunk_size: int = 1024, overlap: int = 128) -> List[Dict[str, Any]]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å‰²æˆé‡å çš„æ®µè½

        Args:
            text: åŸå§‹æ–‡æœ¬
            chunk_size: æ¯ä¸ªæ®µè½çš„å­—ç¬¦æ•°
            overlap: æ®µè½ä¹‹é—´çš„é‡å å­—ç¬¦æ•°

        Returns:
            List[Dict[str, Any]]: æ®µè½åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«æ–‡æœ¬å†…å®¹å’Œä½ç½®ä¿¡æ¯
        """
        if not text or len(text) <= chunk_size:
            return [{
                "content": text,
                "start_pos": 0,
                "end_pos": len(text),
                "chunk_index": 0,
                "total_chunks": 1
            }]

        chunks = []
        start = 0
        chunk_index = 0
        text_length = len(text)

        while start < text_length:
            # è®¡ç®—ç»“æŸä½ç½®
            end = min(start + chunk_size, text_length)

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ®µï¼Œå°è¯•åœ¨å¥å­è¾¹ç•Œç»“æŸ
            if end < text_length:
                # å¯»æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸç¬¦
                sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n', '. ', '! ', '? ']
                best_end = end

                for ending in sentence_endings:
                    last_ending = text.rfind(ending, start, end + 50)
                    if last_ending != -1 and last_ending > start + chunk_size // 2:
                        best_end = last_ending + len(ending)
                        break

                end = min(best_end, text_length)

            # æå–æ®µè½å†…å®¹
            chunk_content = text[start:end]
            chunks.append({
                "content": chunk_content.strip(),
                "start_pos": start,
                "end_pos": end,
                "chunk_index": chunk_index,
                "total_chunks": 0  # ç¨åæ›´æ–°
            })

            chunk_index += 1

            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = max(end - overlap, start + 1)

            # é˜²æ­¢æ— é™å¾ªç¯
            if start >= text_length:
                break

        # æ›´æ–°æ€»æ®µè½æ•°
        total_chunks = len(chunks)
        for chunk in chunks:
            chunk["total_chunks"] = total_chunks

        return chunks

    def set_index_name(self, index_name: str):
        """
        è®¾ç½®ç´¢å¼•åç§°

        Args:
            index_name: ç´¢å¼•åç§°
        """
        self.index_name = index_name

    def save_document_to_elastic(self, knowledge_id: str, file_id: str,
                                user_id: str, permission_level: str,
                                title: str, content: str, **kwargs) -> bool:
        """
        ä¿å­˜æ–‡æ¡£åˆ° Elasticsearch

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID
            user_id: ç”¨æˆ·ID
            permission_level: æƒé™çº§åˆ«
            title: æ–‡æ¡£æ ‡é¢˜
            content: æ–‡æ¡£å†…å®¹
            **kwargs: å…¶ä»–å¯é€‰å­—æ®µ

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        if not self.enabled:
            logger.debug("Elasticsearchå·²ç¦ç”¨ï¼Œè·³è¿‡ä¿å­˜æ–‡æ¡£æ“ä½œ")
            return False
            
        try:
            # ç”Ÿæˆæ–‡æ¡£ID (åŸºäºæ–‡ä»¶IDå’ŒçŸ¥è¯†åº“ID)
            doc_id = f"{knowledge_id}_{file_id}"

            # æ„å»ºæ–‡æ¡£
            document = {
                "knowledge_id": knowledge_id,
                "file_id": file_id,
                "user_id": user_id,
                "permission_level": permission_level,
                "title": title,
                "content": content,
                "upload_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            }

            # æ·»åŠ å¯é€‰å­—æ®µ
            for key, value in kwargs.items():
                if value is not None:
                    document[key] = value

            # è®¡ç®—å†…å®¹å“ˆå¸Œï¼Œç”¨äºå»é‡
            content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
            document["content_hash"] = content_hash

            # ä¿å­˜åˆ° Elasticsearch
            success = self.es_db.index_document(self.index_name, doc_id, document)

            if success:
                logger.info(f"æ–‡æ¡£ {doc_id} ä¿å­˜åˆ° Elasticsearch æˆåŠŸ")
            else:
                logger.error(f"æ–‡æ¡£ {doc_id} ä¿å­˜åˆ° Elasticsearch å¤±è´¥")

            return success

        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æ¡£åˆ° Elasticsearch å¤±è´¥: {e}")
            return False

    def save_markdown_content(self, knowledge_id: str, file_id: str,
                             user_id: str, permission_level: str,
                             file_name: str, markdown_content: str,
                             summary: str = "", authors: str = "",
                             category: str = "") -> bool:
        """
        ä¿å­˜ Markdown å†…å®¹åˆ° Elasticsearchï¼ˆæ”¯æŒæ–‡æœ¬åˆ†æ®µå’Œçˆ¶å­å…³ç³»ï¼‰

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID
            user_id: ç”¨æˆ·ID
            permission_level: æƒé™çº§åˆ«
            file_name: æ–‡ä»¶å
            markdown_content: Markdown å†…å®¹
            summary: æ‘˜è¦
            authors: ä½œè€…
            category: åˆ†ç±»

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        if not self.enabled:
            logger.debug("Elasticsearchå·²ç¦ç”¨ï¼Œè·³è¿‡ä¿å­˜Markdownå†…å®¹æ“ä½œ")
            return False
            
        try:
            # 1. ä¿å­˜çˆ¶æ–‡æ¡£ï¼ˆæ–‡ä»¶åŸºæœ¬ä¿¡æ¯ï¼‰
            parent_doc_id = f"{knowledge_id}_{file_id}"
            parent_document = {
                "knowledge_id": knowledge_id,
                "file_id": file_id,
                "user_id": user_id,
                "permission_level": permission_level,
                "title": file_name,
                "content": markdown_content[:2000] if len(markdown_content) > 2000 else markdown_content,  # çˆ¶æ–‡æ¡£åªä¿å­˜éƒ¨åˆ†å†…å®¹
                "full_content_length": len(markdown_content),
                "summary": summary,
                "authors": authors,
                "category": category,
                "upload_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
                "doc_type": "parent",  # æ ‡è®°ä¸ºçˆ¶æ–‡æ¡£
                "has_children": True
            }

            # è®¡ç®—çˆ¶æ–‡æ¡£å†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(markdown_content.encode('utf-8')).hexdigest()
            parent_document["content_hash"] = content_hash

            # ç”Ÿæˆå‘é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.embeddings:
                try:
                    # ç”Ÿæˆæ ‡é¢˜å‘é‡
                    title_text = file_name
                    # ç¡®ä¿æ ‡é¢˜æ˜¯å­—ç¬¦ä¸²ç±»å‹ä¸”ä¸ä¸ºç©º
                    if not isinstance(title_text, str):
                        title_text = str(title_text) if title_text is not None else ""
                    if title_text.strip():
                        parent_document["title_vector"] = self.embeddings.embed_query(title_text)
                    else:
                        logger.warning(f"çˆ¶æ–‡æ¡£ {parent_doc_id} æ ‡é¢˜ä¸ºç©ºï¼Œè·³è¿‡æ ‡é¢˜å‘é‡ç”Ÿæˆ")
                    
                    # ç”Ÿæˆå†…å®¹å‘é‡ï¼ˆä½¿ç”¨å‰2000å­—ç¬¦ï¼‰
                    content_text = markdown_content[:2000] if len(markdown_content) > 2000 else markdown_content
                    # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²ç±»å‹ä¸”ä¸ä¸ºç©º
                    if not isinstance(content_text, str):
                        content_text = str(content_text) if content_text is not None else ""
                    if content_text.strip():
                        parent_document["content_vector"] = self.embeddings.embed_query(content_text)
                    else:
                        logger.warning(f"çˆ¶æ–‡æ¡£ {parent_doc_id} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡å†…å®¹å‘é‡ç”Ÿæˆ")
                    logger.debug(f"å·²ä¸ºçˆ¶æ–‡æ¡£ {parent_doc_id} ç”Ÿæˆå‘é‡")
                except Exception as e:
                    logger.warning(f"ç”Ÿæˆçˆ¶æ–‡æ¡£å‘é‡å¤±è´¥: {e}ï¼Œå°†ä»…ä½¿ç”¨å…¨æ–‡æœç´¢")
                    import traceback
                    logger.debug(f"å‘é‡ç”Ÿæˆé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

            # ä¿å­˜çˆ¶æ–‡æ¡£
            parent_success = self.es_db.index_document(self.index_name, parent_doc_id, parent_document)
            if not parent_success:
                logger.error(f"ä¿å­˜çˆ¶æ–‡æ¡£å¤±è´¥: {parent_doc_id}")
                return False

            # 2. åˆ†æ®µå¤„ç†æ–‡æœ¬å†…å®¹
            chunks = self.split_text_with_overlap(markdown_content, chunk_size=1024, overlap=128)

            # 3. ä¿å­˜å­æ–‡æ¡£ï¼ˆå„ä¸ªæ®µè½ï¼‰
            child_documents = []
            for chunk in chunks:
                child_doc_id = f"{parent_doc_id}_chunk_{chunk['chunk_index']}"

                child_document = {
                    "knowledge_id": knowledge_id,
                    "file_id": file_id,
                    "user_id": user_id,
                    "permission_level": permission_level,
                    "title": f"{file_name} (æ®µè½ {chunk['chunk_index'] + 1}/{chunk['total_chunks']})",
                    "content": chunk["content"],
                    "chunk_index": chunk["chunk_index"],
                    "total_chunks": chunk["total_chunks"],
                    "start_pos": chunk["start_pos"],
                    "end_pos": chunk["end_pos"],
                    "parent_id": parent_doc_id,  # çˆ¶æ–‡æ¡£ID
                    "summary": summary,
                    "authors": authors,
                    "category": category,
                    "upload_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
                    "doc_type": "child",  # æ ‡è®°ä¸ºå­æ–‡æ¡£
                    "content_hash": hashlib.md5(chunk["content"].encode('utf-8')).hexdigest()
                }

                # ç”Ÿæˆå‘é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.embeddings:
                    try:
                        # ç”Ÿæˆæ ‡é¢˜å‘é‡
                        chunk_title = child_document.get("title", "")
                        # ç¡®ä¿æ ‡é¢˜æ˜¯å­—ç¬¦ä¸²ç±»å‹ä¸”ä¸ä¸ºç©º
                        if not isinstance(chunk_title, str):
                            chunk_title = str(chunk_title) if chunk_title is not None else ""
                        if chunk_title.strip():
                            child_document["title_vector"] = self.embeddings.embed_query(chunk_title)
                        else:
                            logger.warning(f"å­æ–‡æ¡£ {child_doc_id} æ ‡é¢˜ä¸ºç©ºï¼Œè·³è¿‡æ ‡é¢˜å‘é‡ç”Ÿæˆ")
                        
                        # ç”Ÿæˆå†…å®¹å‘é‡
                        chunk_content = chunk.get("content", "")
                        # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²ç±»å‹ä¸”ä¸ä¸ºç©º
                        if not isinstance(chunk_content, str):
                            chunk_content = str(chunk_content) if chunk_content is not None else ""
                        if chunk_content.strip():
                            child_document["content_vector"] = self.embeddings.embed_query(chunk_content)
                        else:
                            logger.warning(f"å­æ–‡æ¡£ {child_doc_id} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡å†…å®¹å‘é‡ç”Ÿæˆ")
                    except Exception as e:
                        logger.warning(f"ç”Ÿæˆå­æ–‡æ¡£ {child_doc_id} å‘é‡å¤±è´¥: {e}")
                        import traceback
                        logger.debug(f"å‘é‡ç”Ÿæˆé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

                child_documents.append((child_doc_id, child_document))

            # æ‰¹é‡ä¿å­˜å­æ–‡æ¡£
            if child_documents:
                batch_success = self.es_db.bulk_index_documents(self.index_name, child_documents)
                if not batch_success:
                    logger.error(f"æ‰¹é‡ä¿å­˜å­æ–‡æ¡£å¤±è´¥: {file_id}")
                    return False

            logger.info(f"æˆåŠŸä¿å­˜æ–‡æ¡£ {file_name}: 1ä¸ªçˆ¶æ–‡æ¡£ + {len(child_documents)}ä¸ªå­æ–‡æ¡£")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜ Markdown å†…å®¹å¤±è´¥: {e}")
            return False

    def search_documents(self, knowledge_id: str, user_id: str,
                        permission_flag: bool, search_query: str = "",
                        size: int = 10, use_hybrid_search: bool = True) -> List[Dict[str, Any]]:
        """
        æœç´¢æ–‡æ¡£ï¼ˆæ ¹æ®æƒé™æ§åˆ¶ï¼Œæ”¯æŒ Hybrid Searchï¼‰
        
        Elasticsearch 9.2.4+ æ”¯æŒæ··åˆæœç´¢ï¼Œç»“åˆå…¨æ–‡æœç´¢å’ŒAIé©±åŠ¨çš„å‘é‡æœç´¢

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            user_id: ç”¨æˆ·ID
            permission_flag: æƒé™æ ‡å¿— (True: å¯è®¿é—®æ‰€æœ‰, False: ä»…å…¬å¼€)
            search_query: æœç´¢æŸ¥è¯¢å†…å®¹
            size: è¿”å›ç»“æœæ•°é‡
            use_hybrid_search: æ˜¯å¦ä½¿ç”¨æ··åˆæœç´¢ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.enabled:
            logger.debug("Elasticsearchå·²ç¦ç”¨ï¼Œè·³è¿‡æœç´¢æ–‡æ¡£æ“ä½œ")
            return []
            
        try:
            # å¦‚æœå¯ç”¨æ··åˆæœç´¢ï¼Œç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = None
            if use_hybrid_search and search_query and self.embeddings:
                try:
                    # ç¡®ä¿æŸ¥è¯¢æ–‡æœ¬æ˜¯å­—ç¬¦ä¸²ç±»å‹ä¸”ä¸ä¸ºç©º
                    if not isinstance(search_query, str):
                        search_query = str(search_query) if search_query is not None else ""
                    if search_query.strip():
                        query_vector = self.embeddings.embed_query(search_query)
                        logger.debug(f"å·²ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œç»´åº¦: {len(query_vector)}")
                    else:
                        logger.warning("æŸ¥è¯¢æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡å‘é‡ç”Ÿæˆ")
                        use_hybrid_search = False
                except Exception as e:
                    logger.warning(f"ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {e}ï¼Œå°†ä»…ä½¿ç”¨å…¨æ–‡æœç´¢")
                    import traceback
                    logger.debug(f"å‘é‡ç”Ÿæˆé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                    use_hybrid_search = False
            
            results = self.es_db.search_by_knowledge_and_permission(
                index_name=self.index_name,
                knowledge_id=knowledge_id,
                user_id=user_id,
                permission_flag=permission_flag,
                search_query=search_query,
                size=size,
                use_hybrid_search=use_hybrid_search,
                query_vector=query_vector
            )
            logger.info(f"æœç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœï¼ˆæ··åˆæœç´¢: {use_hybrid_search}ï¼‰")
            return results

        except Exception as e:
            logger.error(f"æœç´¢æ–‡æ¡£å¤±è´¥: {e}")
            return []

    def get_document_by_id(self, knowledge_id: str, file_id: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®çŸ¥è¯†åº“IDå’Œæ–‡ä»¶IDè·å–æ–‡æ¡£

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID

        Returns:
            Optional[Dict[str, Any]]: æ–‡æ¡£å†…å®¹
        """
        if not self.enabled:
            logger.debug("Elasticsearchå·²ç¦ç”¨ï¼Œè·³è¿‡è·å–æ–‡æ¡£æ“ä½œ")
            return None
            
        doc_id = f"{knowledge_id}_{file_id}"
        return self.es_db.get_document(self.index_name, doc_id)

    def update_document(self, knowledge_id: str, file_id: str,
                       updates: Dict[str, Any]) -> bool:
        """
        æ›´æ–°æ–‡æ¡£

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID
            updates: æ›´æ–°å†…å®¹

        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        if not self.enabled:
            logger.debug("Elasticsearchå·²ç¦ç”¨ï¼Œè·³è¿‡æ›´æ–°æ–‡æ¡£æ“ä½œ")
            return False
            
        doc_id = f"{knowledge_id}_{file_id}"
        return self.es_db.update_document(self.index_name, doc_id, updates)

    def delete_document(self, knowledge_id: str, file_id: str) -> bool:
        """
        åˆ é™¤æ–‡æ¡£

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID

        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        if not self.enabled:
            logger.debug("Elasticsearchå·²ç¦ç”¨ï¼Œè·³è¿‡åˆ é™¤æ–‡æ¡£æ“ä½œ")
            return False
            
        doc_id = f"{knowledge_id}_{file_id}"
        return self.es_db.delete_document(self.index_name, doc_id)

    def get_knowledge_documents(self, knowledge_id: str, user_id: str,
                               permission_flag: bool, size: int = 100) -> List[Dict[str, Any]]:
        """
        è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£ï¼ˆæ ¹æ®æƒé™ï¼‰

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            user_id: ç”¨æˆ·ID
            permission_flag: æƒé™æ ‡å¿—
            size: è¿”å›ç»“æœæ•°é‡

        Returns:
            List[Dict[str, Any]]: æ–‡æ¡£åˆ—è¡¨
        """
        if not self.enabled:
            logger.debug("Elasticsearchå·²ç¦ç”¨ï¼Œè·³è¿‡è·å–çŸ¥è¯†åº“æ–‡æ¡£æ“ä½œ")
            return []
            
        # ä½¿ç”¨ç©ºçš„æœç´¢æŸ¥è¯¢æ¥è·å–æ‰€æœ‰æ–‡æ¡£
        return self.search_documents(knowledge_id, user_id, permission_flag, "", size)

    def search_similar_documents(self, knowledge_id: str, user_id: str,
                                permission_flag: bool, query_text: str,
                                size: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            user_id: ç”¨æˆ·ID
            permission_flag: æƒé™æ ‡å¿—
            query_text: æŸ¥è¯¢æ–‡æœ¬
            size: è¿”å›ç»“æœæ•°é‡

        Returns:
            List[Dict[str, Any]]: ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        if not self.enabled:
            logger.debug("Elasticsearchå·²ç¦ç”¨ï¼Œè·³è¿‡æœç´¢ç›¸ä¼¼æ–‡æ¡£æ“ä½œ")
            return []
            
        return self.search_documents(knowledge_id, user_id, permission_flag, query_text, size)

    def get_document_with_chunks(self, knowledge_id: str, file_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æ–‡æ¡£åŠå…¶æ‰€æœ‰æ®µè½ä¿¡æ¯

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID

        Returns:
            Optional[Dict[str, Any]]: åŒ…å«çˆ¶æ–‡æ¡£å’Œæ‰€æœ‰å­æ–‡æ¡£çš„ä¿¡æ¯
        """
        try:
            parent_doc_id = f"{knowledge_id}_{file_id}"

            # è·å–çˆ¶æ–‡æ¡£
            parent_doc = self.es_db.get_document(self.index_name, parent_doc_id)
            if not parent_doc:
                return None

            # è·å–æ‰€æœ‰å­æ–‡æ¡£
            child_docs = self.es_db.get_child_documents(self.index_name, parent_doc_id)

            return {
                "parent_document": parent_doc,
                "child_documents": child_docs,
                "total_chunks": len(child_docs)
            }

        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£æ®µè½ä¿¡æ¯å¤±è´¥: {e}")
            return None

    def search_with_context(self, knowledge_id: str, user_id: str, permission_flag: bool,
                          search_query: str, context_size: int = 1, size: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢æ–‡æ¡£å¹¶æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå‰åæ®µè½ï¼‰

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            user_id: ç”¨æˆ·ID
            permission_flag: æƒé™æ ‡å¿—
            search_query: æœç´¢æŸ¥è¯¢
            context_size: ä¸Šä¸‹æ–‡æ®µè½æ•°é‡
            size: è¿”å›ç»“æœæ•°é‡

        Returns:
            List[Dict[str, Any]]: åŒ…å«ä¸Šä¸‹æ–‡çš„æœç´¢ç»“æœ
        """
        try:
            # å…ˆè¿›è¡Œæ™®é€šæœç´¢
            results = self.search_documents(knowledge_id, user_id, permission_flag, search_query, size)

            # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            enriched_results = []
            for result in results:
                enriched_result = result.copy()

                # å¦‚æœæ˜¯å­æ–‡æ¡£ï¼Œè·å–ä¸Šä¸‹æ–‡æ®µè½
                if result.get("doc_type") == "child" and result.get("parent_id"):
                    parent_id = result["parent_id"]
                    chunk_index = result.get("chunk_index", 0)

                    # è·å–æ‰€æœ‰ç›¸å…³æ®µè½
                    child_docs = self.es_db.get_child_documents(self.index_name, parent_id)

                    # æŒ‰chunk_indexæ’åº
                    child_docs.sort(key=lambda x: x.get("chunk_index", 0))

                    # è·å–ä¸Šä¸‹æ–‡æ®µè½
                    context_chunks = []
                    start_idx = max(0, chunk_index - context_size)
                    end_idx = min(len(child_docs), chunk_index + context_size + 1)

                    for i in range(start_idx, end_idx):
                        chunk = child_docs[i]
                        context_chunks.append({
                            "index": i,
                            "content": chunk.get("content", ""),
                            "is_target": (i == chunk_index)
                        })

                    enriched_result["context_chunks"] = context_chunks
                    enriched_result["has_context"] = True
                else:
                    enriched_result["has_context"] = False

                enriched_results.append(enriched_result)

            return enriched_results

        except Exception as e:
            logger.error(f"ä¸Šä¸‹æ–‡æœç´¢å¤±è´¥: {e}")
            return results  # è¿”å›åŸå§‹ç»“æœ

    def get_document_stats(self, knowledge_id: str = None) -> Dict[str, Any]:
        """
        è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯

        Args:
            knowledge_id: çŸ¥è¯†åº“IDï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç»Ÿè®¡ç‰¹å®šçŸ¥è¯†åº“ï¼‰

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if knowledge_id:
                # ç»Ÿè®¡ç‰¹å®šçŸ¥è¯†åº“ï¼ˆæ³¨æ„ï¼šå­—æ®µæ˜¯ text ç±»å‹ï¼Œä½¿ç”¨ match æŸ¥è¯¢ï¼‰
                query = {"match": {"knowledge_id": {"query": knowledge_id, "operator": "and"}}}
                total_docs = self.es_db.get_document_count(self.index_name, query)

                # ç»Ÿè®¡å…¬å¼€æ–‡æ¡£æ•°é‡
                public_query = {
                    "bool": {
                        "must": [
                            {"match": {"knowledge_id": {"query": knowledge_id, "operator": "and"}}},
                            {"match": {"permission_level": {"query": "public", "operator": "and"}}}
                        ]
                    }
                }
                public_docs = self.es_db.get_document_count(self.index_name, public_query)

                # ç»Ÿè®¡ç§æœ‰æ–‡æ¡£æ•°é‡
                private_query = {
                    "bool": {
                        "must": [
                            {"match": {"knowledge_id": {"query": knowledge_id, "operator": "and"}}},
                            {"match": {"permission_level": {"query": "private", "operator": "and"}}}
                        ]
                    }
                }
                private_docs = self.es_db.get_document_count(self.index_name, private_query)

                return {
                    "knowledge_id": knowledge_id,
                    "total_documents": total_docs,
                    "public_documents": public_docs,
                    "private_documents": private_docs
                }
            else:
                # ç»Ÿè®¡æ‰€æœ‰æ–‡æ¡£
                total_docs = self.es_db.get_document_count(self.index_name)
                return {
                    "total_documents": total_docs
                }

        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}

    def batch_save_documents(self, documents: List[Tuple[str, str, Dict[str, Any]]]) -> bool:
        """
        æ‰¹é‡ä¿å­˜æ–‡æ¡£

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ [(knowledge_id, file_id, document_data), ...]

        Returns:
            bool: æ‰¹é‡ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            bulk_docs = []
            for knowledge_id, file_id, doc_data in documents:
                doc_id = f"{knowledge_id}_{file_id}"
                bulk_docs.append((doc_id, doc_data))

            return self.es_db.bulk_index_documents(self.index_name, bulk_docs)

        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜æ–‡æ¡£å¤±è´¥: {e}")
            return False

    def check_knowledge_and_user(self, knowledge_id: str, user_id: str) -> bool:
        """
        æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æƒé™è®¿é—®çŸ¥è¯†åº“

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            user_id: ç”¨æˆ·ID

        Returns:
            bool: True è¡¨ç¤ºæœ‰æƒé™è®¿é—®æ‰€æœ‰æ•°æ®ï¼ŒFalse è¡¨ç¤ºåªèƒ½è®¿é—®å…¬å¼€æ•°æ®
        """
        # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…ä¸šåŠ¡é€»è¾‘å®ç°æƒé™æ£€æŸ¥
        # ä¾‹å¦‚ï¼šæ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¯çŸ¥è¯†åº“çš„åˆ›å»ºè€…æˆ–åä½œè€…
        # ç›®å‰ç®€åŒ–ä¸ºï¼šæ£€æŸ¥çŸ¥è¯†åº“IDæ˜¯å¦ä»¥ç”¨æˆ·IDå¼€å¤´ï¼ˆè¡¨ç¤ºç”¨æˆ·åˆ›å»ºçš„çŸ¥è¯†åº“ï¼‰

        # å®é™…å®ç°åº”è¯¥æŸ¥è¯¢æ•°æ®åº“æ£€æŸ¥ç”¨æˆ·æƒé™
        # è¿™é‡Œæä¾›ä¸€ä¸ªç¤ºä¾‹å®ç°
        try:
            # ç¤ºä¾‹ï¼šå¦‚æœçŸ¥è¯†åº“IDåŒ…å«ç”¨æˆ·IDï¼Œåˆ™è®¤ä¸ºç”¨æˆ·æœ‰å®Œå…¨è®¿é—®æƒé™
            return user_id in knowledge_id or knowledge_id.startswith(f"kb_{user_id}")
        except Exception as e:
            logger.error(f"æƒé™æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def reindex_document(self, knowledge_id: str, file_id: str) -> bool:
        """
        é‡æ–°ç´¢å¼•æ–‡æ¡£ï¼ˆç”¨äºæ›´æ–°ç´¢å¼•ç»“æ„åï¼‰

        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID

        Returns:
            bool: é‡æ–°ç´¢å¼•æ˜¯å¦æˆåŠŸ
        """
        try:
            # è·å–ç°æœ‰æ–‡æ¡£
            doc = self.get_document_by_id(knowledge_id, file_id)
            if not doc:
                logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨ï¼Œæ— æ³•é‡æ–°ç´¢å¼•: {knowledge_id}_{file_id}")
                return False

            # åˆ é™¤æ—§æ–‡æ¡£
            self.delete_document(knowledge_id, file_id)

            # é‡æ–°ä¿å­˜æ–‡æ¡£ï¼ˆä¼šè‡ªåŠ¨åˆ›å»ºç´¢å¼•ï¼‰
            doc_id = f"{knowledge_id}_{file_id}"
            return self.es_db.index_document(self.index_name, doc_id, doc)

        except Exception as e:
            logger.error(f"é‡æ–°ç´¢å¼•æ–‡æ¡£å¤±è´¥: {e}")
            return False

    def delete_all_elasticsearch(self) -> bool:
        """
        åˆ é™¤ Elasticsearch ä¸­çš„æ‰€æœ‰ç´¢å¼•å’Œæ•°æ®

        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹åˆ é™¤æ‰€æœ‰ Elasticsearch ç´¢å¼•å’Œæ•°æ®...")
            success = self.es_db.delete_all_indices()
            if success:
                logger.info("âœ… æˆåŠŸåˆ é™¤æ‰€æœ‰ Elasticsearch ç´¢å¼•å’Œæ•°æ®")
            else:
                logger.error("âŒ åˆ é™¤æ‰€æœ‰ Elasticsearch ç´¢å¼•å’Œæ•°æ®å¤±è´¥")
            return success
        except Exception as e:
            logger.error(f"åˆ é™¤æ‰€æœ‰ Elasticsearch æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

    def delete_file_elasticsearch_data(self, file_id: str) -> bool:
        """
        æ ¹æ®æ–‡ä»¶IDåˆ é™¤ Elasticsearch ä¸­çš„æ‰€æœ‰ç›¸å…³æ•°æ®

        Args:
            file_id: æ–‡ä»¶ID

        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹åˆ é™¤æ–‡ä»¶ {file_id} çš„ Elasticsearch æ•°æ®...")
            success = self.es_db.delete_documents_by_file_id(self.index_name, file_id)
            if success:
                logger.info(f"âœ… æˆåŠŸåˆ é™¤æ–‡ä»¶ {file_id} çš„ Elasticsearch æ•°æ®")
            else:
                logger.error(f"âŒ åˆ é™¤æ–‡ä»¶ {file_id} çš„ Elasticsearch æ•°æ®å¤±è´¥")
            return success
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡ä»¶ {file_id} çš„ Elasticsearch æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

    def delete_knowledge_elasticsearch_data(self, knowledge_id: str) -> bool:
        """
        æ ¹æ®çŸ¥è¯†åº“IDåˆ é™¤ Elasticsearch ä¸­çš„æ‰€æœ‰ç›¸å…³æ•°æ®

        Args:
            knowledge_id: çŸ¥è¯†åº“ID

        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹åˆ é™¤çŸ¥è¯†åº“ {knowledge_id} çš„ Elasticsearch æ•°æ®...")
            success = self.es_db.delete_documents_by_knowledge_id(self.index_name, knowledge_id)
            if success:
                logger.info(f"âœ… æˆåŠŸåˆ é™¤çŸ¥è¯†åº“ {knowledge_id} çš„ Elasticsearch æ•°æ®")
            else:
                logger.error(f"âŒ åˆ é™¤çŸ¥è¯†åº“ {knowledge_id} çš„ Elasticsearch æ•°æ®å¤±è´¥")
            return success
        except Exception as e:
            logger.error(f"åˆ é™¤çŸ¥è¯†åº“ {knowledge_id} çš„ Elasticsearch æ•°æ®æ—¶å‡ºé”™: {e}")
            return False


# å…¨å±€å®ä¾‹
elastic_controller = CControl()

def get_elastic_controller() -> CControl:
    """
    è·å– Elasticsearch æ§åˆ¶å™¨å®ä¾‹

    Returns:
        ElasticSearchController: Elasticsearch æ§åˆ¶å™¨å®ä¾‹
    """
    return elastic_controller


# if __name__ == "__main__":
#     # æµ‹è¯•è¿æ¥
#     controller = get_elastic_controller()
#     print("ğŸ§ª Elasticsearch æ§åˆ¶å±‚æµ‹è¯•")

#     # æµ‹è¯•æƒé™æ£€æŸ¥
#     flag = controller.check_knowledge_and_user("kb_user123", "user123")
#     print(f"âœ… æƒé™æ£€æŸ¥ç»“æœ: {flag}")

#     # æµ‹è¯•æ–‡æ¡£ç»Ÿè®¡
#     stats = controller.get_document_stats()
#     print(f"ğŸ“Š æ–‡æ¡£ç»Ÿè®¡: {stats}")
