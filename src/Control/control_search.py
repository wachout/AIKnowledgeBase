# -*- coding:utf-8 -*-

import time

from typing import List, Dict, Any
from Agent.emb_graph_chat_run import emb_graph_chat_run
from Agent.emb_query_run import run_agent
from Agent.emb_graph_chat_run import emb_graph_chat_stream_run
from Agent import entity_relation_split_run
import logging
import threading

# æœç´¢ç›¸å…³æ¨¡å—
from Config.embedding_config import get_embeddings
from Control.control_milvus import CControl as MilvusController
from Control.control_graph import CControl as ControlGraph
from Control.control_elastic import get_elastic_controller
from Db.sqlite_db import cSingleSqlite
from Emb.xinference_embedding import cSingleEmb
from Utils import utils

# åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # è®¾ç½®æ—¥å¿—çº§åˆ«
logger_lock = threading.Lock()

def thread_safe_log(level_func, message, *args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—è®°å½•å‡½æ•°"""
    with logger_lock:
        level_func(message, *args, **kwargs)

class CControl():
    
    def __init__(self):
        self.milvus_obj = MilvusController()
        self.graph_obj = ControlGraph()
        
    def search_graph_emb(self, query_text, graph_data, milvus_data):
        print("query_text:", query_text)
        print("graph_data:", graph_data)
        print("milvus_data:", milvus_data)
        result = emb_graph_chat_run(query_text, graph_data, milvus_data)
        print("æœ€ç»ˆç»“æœ:", result)
        return result
    
    def search_emb(self, query_text, database_code, index_params, limit):
        chat_history = []
        result = run_agent(query_text, chat_history, database_code, index_params, limit)
        return result
        
    def stream_openai_chat(self, query_text, graph_data, milvus_data):
        """
        å®ç°stream_openai_chatæ–¹æ³•ï¼Œæä¾›ä¸OpenAIå…¼å®¹çš„æµå¼èŠå¤©æ¥å£
        """
        logger.info(f"stream_openai_chat called with query_text: {query_text}")
        logger.info(f"graph_data: {graph_data}")
        logger.info(f"milvus_data: {milvus_data}")
        
        # ä½¿ç”¨ç°æœ‰çš„emb_graph_chat_stream_runæ–¹æ³•
        logger.info("Calling emb_graph_chat_stream_run")
        stream_result = emb_graph_chat_stream_run(query_text, graph_data, milvus_data)
        logger.info(f"emb_graph_chat_stream_run returned: {type(stream_result)}")
        
        # åŒ…è£…ç»“æœä»¥ç¬¦åˆOpenAIæ ¼å¼
        
        _id = f"chatcmpl-{int(time.time())}"
        created = int(time.time())
        model = "emb-graph-chat-model"
        
        chunk_count = 0
        has_data = False
        # é€ä¸ªyieldç¬¦åˆOpenAIæ ¼å¼çš„æ•°æ®å—ï¼Œ{'content': '', 'additional_kwargs': {}, 'id': 'run--88fb5a1b-6ca5-450b-ae11-278c87f9e463'}
        for chunk in stream_result:
            chunk_count += 1
            has_data = True
            logger.info(f"Processing chunk #{chunk_count} in stream_openai_chat: {chunk}")
            if("id" in chunk.keys()):
                _id = chunk["id"]
            yield {
                "id": _id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [
                    {
                        "index": 0,
                        "delta": {
                            "content": chunk["content"],
                            "type": "text"
                        },
                        "finish_reason": None
                    }
                ]
            }
        
        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè®°å½•è­¦å‘Š
        if not has_data:
            logger.warning("No data received from emb_graph_chat_stream_run")
            yield {
                "id": _id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [
                    {
                        "index": 0,
                        "delta": {
                            "content": "No data returned from chat processing",
                            "type": "text"
                        },
                        "finish_reason": None
                    }
                ]
            }
        else:
            logger.info(f"Total chunks processed in stream_openai_chat: {chunk_count}")
        
        # å‘é€ç»“æŸæ ‡è®°
        logger.info("Sending finish message")
        yield {
            "id": _id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model,
            "choices": [
                {
                    "index": 0,
                    "delta": {
                        "content": "",
                        "type": "text"
                    },
                    "finish_reason": "stop"
                }
            ]
        }
        logger.info("Finish message sent")
    
    def process_text_knowledge(self, text_knowledge: List[Dict[str, Any]]) -> str:
        """å¤„ç†æ–‡æœ¬çŸ¥è¯†åº“æ•°æ® - ä¿ç•™å®Œæ•´æ–‡æœ¬æ®µè½"""
        processed_content = []
        for item in text_knowledge:
            title = item.get("title", "æœªçŸ¥æ ‡é¢˜")
            content = item.get("content", "")
            score = item.get("score", 0.0)
            
            # ä¿ç•™å®Œæ•´æ–‡æœ¬ï¼ŒåŒ…æ‹¬å¯èƒ½åŒ…å«è¡¨æ ¼ã€å›¾ç‰‡åŠå…¶è¯´æ˜çš„å†…å®¹
            processed_item = f"æ ‡é¢˜: {title}\nç›¸å…³åº¦: {score}\nå†…å®¹:\n{content}"
            processed_content.append(processed_item)
        
        return "\n\n".join(processed_content)
    
    def process_graph_knowledge(self, graph_knowledge: List[List[Dict[str, Any]]]) -> str:
        """å¤„ç†å›¾æ•°æ®åº“çŸ¥è¯†"""
        processed_content = []
        
        for edge_group in graph_knowledge:
            for edge in edge_group:
                # æå–å®ä½“ä¿¡æ¯
                start_node = edge.get("start_node", {})
                end_node = edge.get("end_node", {})
                relation = edge.get("relation", {})
                
                # æ„å»ºå®ä½“å…³ç³»è¡¨ç¤º
                start_entity = f"å®ä½“1: {start_node.get('entity_id', 'æœªçŸ¥')} ({start_node.get('entity_type', 'æœªçŸ¥ç±»å‹')})"
                end_entity = f"å®ä½“2: {end_node.get('entity_id', 'æœªçŸ¥')} ({end_node.get('entity_type', 'æœªçŸ¥ç±»å‹')})"
                
                # æå–å…³ç³»æè¿°
                relation_desc = relation.get('description', '')
                relation_keywords = relation.get('keywords', '')
                
                # åˆå¹¶èŠ‚ç‚¹æè¿°ä¿¡æ¯
                start_desc = start_node.get('description', '')
                end_desc = end_node.get('description', '')
                
                # å¦‚æœå­˜åœ¨chunksä¿¡æ¯ï¼Œå°†å…¶æ·»åŠ åˆ°å¤„ç†åçš„å†…å®¹ä¸­
                start_chunks = start_node.get('chunks', [])
                end_chunks = end_node.get('chunks', [])
                start_titles = start_node.get('titles', [])
                end_titles = end_node.get('titles', [])
                
                # æ„å»ºå®Œæ•´çš„å…³ç³»è¡¨ç¤º
                graph_item = (
                    f"{start_entity}\n{end_entity}\nå…³ç³»æè¿°: {relation_desc}\n"
                    f"å…³ç³»å…³é”®è¯: {relation_keywords}\n"
                )
                
                # æ·»åŠ èŠ‚ç‚¹æè¿°
                if start_desc:
                    graph_item += f"å®ä½“1æè¿°: {start_desc}\n"
                if end_desc:
                    graph_item += f"å®ä½“2æè¿°: {end_desc}\n"
                
                # æ·»åŠ chunksä¿¡æ¯ - å¯èƒ½åŒ…å«è¡¨æ ¼å’Œå›¾ç‰‡ä¿¡æ¯
                if start_chunks:
                    graph_item += f"å®ä½“1ç›¸å…³æ–‡æœ¬æ®µè½: {', '.join(start_chunks)}\n"
                if end_chunks:
                    graph_item += f"å®ä½“2ç›¸å…³æ–‡æœ¬æ®µè½: {', '.join(end_chunks)}\n"
                if start_titles:
                    graph_item += f"å®ä½“1ç›¸å…³æ–‡æ¡£æ ‡é¢˜: {', '.join(start_titles)}\n"
                if end_titles:
                    graph_item += f"å®ä½“2ç›¸å…³æ–‡æ¡£æ ‡é¢˜: {', '.join(end_titles)}\n"
                
                processed_content.append(graph_item)
        
        return "\n\n".join(processed_content)
    
    # ============================================================================
    # Milvus å‘é‡æœç´¢
    # ============================================================================
    
    def check_knowledge_and_user(self, knowledge_id: str, user_id: str) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æƒé™è®¿é—®çŸ¥è¯†åº“
        
        Args:
            knowledge_id: çŸ¥è¯†åº“ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            æ˜¯å¦æœ‰æƒé™
        """
        param = {"knowledge_id": knowledge_id, "user_id": user_id}
        result = cSingleSqlite.search_knowledge_base_by_id_and_user_id(param)
        return result
    
    def query_milvus(self, param: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åœ¨ Milvus ä¸­æœç´¢å‘é‡æ•°æ®
        
        Args:
            param: æœç´¢å‚æ•°
                - query: æŸ¥è¯¢æ–‡æœ¬
                - knowledge_id: çŸ¥è¯†åº“ID
                - user_id: ç”¨æˆ·ID
                - top_k: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤5ï¼‰
                - metric_type: åº¦é‡ç±»å‹ï¼ˆé»˜è®¤IPï¼‰
                - index_type: ç´¢å¼•ç±»å‹ï¼ˆé»˜è®¤HNSWï¼‰
                
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        query_text = param["query"]
        database_code = param["knowledge_id"]
        user_id = param["user_id"]
        
        flag = True
        if not self.check_knowledge_and_user(database_code, user_id):
            flag = False
        
        top_k = param.get("top_k", 5)
        metric_type = param.get("metric_type", "IP")
        index_type = param.get("index_type", "HNSW")
        
        index_params = {
            "index_type": index_type,
            "metric_type": metric_type,
            "params": {"nlist": 128}
        }
        
        # æœç´¢Milvusï¼ˆå¦‚æœå¯ç”¨ï¼‰
        from Config.milvus_config import is_milvus_enabled
        if not is_milvus_enabled():
            logger.debug("Milvuså·²ç¦ç”¨ï¼Œè·³è¿‡æœç´¢æ“ä½œ")
            return []
            
        embedding = get_embeddings()
        result = self.milvus_obj.search_content(database_code, query_text, 
                                                embedding, index_params, 
                                                top_k, flag)
        
        return result
    
    def query_elasticsearch(self, param: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åœ¨ Elasticsearch ä¸­æœç´¢æ–‡æ¡£
        
        Args:
            param: æœç´¢å‚æ•°
                - query: æŸ¥è¯¢æ–‡æœ¬
                - knowledge_id: çŸ¥è¯†åº“ID
                - user_id: ç”¨æˆ·ID
                - flag: æƒé™æ ‡å¿—ï¼ˆé»˜è®¤Trueï¼‰
                - size: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤10ï¼‰
                
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        from Config.elasticsearch_config import is_elasticsearch_enabled
        if not is_elasticsearch_enabled():
            return []
            
        try:
            elastic_controller = get_elastic_controller()
            
            query_text = param["query"]
            knowledge_id = param["knowledge_id"]
            user_id = param.get("user_id", "")
            flag = param.get("flag", True)
            size = param.get("size", 10)

            # æœç´¢æ–‡æ¡£
            hits = elastic_controller.search_similar_documents(
                knowledge_id=knowledge_id,
                user_id=user_id,
                permission_flag=flag,
                query_text=query_text,
                size=size
            )
            
            # æ ¼å¼åŒ–æœç´¢ç»“æœ
            results = []
            for hit in hits:
                result = {
                    "title": hit.get("title", ""),
                    "content": hit.get("content", ""),
                    "score": hit.get("_score", hit.get("score", 0)),
                    "source": hit.get("file_name", ""),
                    "search_engine": "elasticsearch",
                    "metadata": {
                        "file_id": hit.get("file_id", ""),
                        "knowledge_id": hit.get("knowledge_id", ""),
                        "permission_level": hit.get("permission_level", ""),
                        "user_id": hit.get("user_id", ""),
                        "create_time": hit.get("create_time", "")
                    }
                }
                file_id = hit.get("file_id", "")
                if file_id:
                    file_detail = cSingleSqlite.search_file_detail_info_by_file_id(file_id)
                    result["file_detail"] = file_detail
                results.append(result)

            return results

        except Exception as e:
            print(f"âŒ Elasticsearchæœç´¢å¤±è´¥: {e}")
            return []
    
    def search_milvus_formatted(self, param: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åœ¨ Milvus ä¸­æœç´¢å¹¶è¿”å›ç»Ÿä¸€æ ¼å¼çš„ç»“æœ
        
        Args:
            param: æœç´¢å‚æ•°
                - query: æŸ¥è¯¢æ–‡æœ¬
                - knowledge_id: çŸ¥è¯†åº“ID
                - user_id: ç”¨æˆ·ID
                - top_k: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤10ï¼‰
                
        Returns:
            ç»Ÿä¸€æ ¼å¼çš„æœç´¢ç»“æœåˆ—è¡¨
        """
        results = self.query_milvus(param)
        
        # æ ¼å¼åŒ–ä¸ºç»Ÿä¸€æ ¼å¼
        formatted_results = []
        for hit in results:
            result = {
                "title": hit.get("title", ""),
                "content": hit.get("content", ""),
                "score": hit.get("score", hit.get("distance", 0)),
                "source": hit.get("source", ""),
                "search_engine": "milvus",
                "metadata": hit.get("metadata", {}),
            }
            doc_id = hit.get("partition", "")
            if doc_id:
                file_detail = cSingleSqlite.search_file_detail_info_by_file_id(doc_id)
                result["file_detail"] = file_detail
            formatted_results.append(result)
        
        return formatted_results
    
    # ============================================================================
    # Neo4j å›¾æ•°æ®åº“æœç´¢
    # ============================================================================
    
    def query_graph_neo4j(self, param: Dict[str, Any], merge_result: bool = False):
        """åœ¨ Neo4j å›¾æ•°æ®åº“ä¸­æœç´¢å®ä½“å…³ç³»
        
        Args:
            param: æœç´¢å‚æ•°
                - query: æŸ¥è¯¢æ–‡æœ¬
                - knowledge_id: çŸ¥è¯†åº“ID
                - user_id: ç”¨æˆ·ID
            merge_result: æ˜¯å¦åˆå¹¶ç»“æœï¼ˆTrueè¿”å›åˆ—è¡¨ï¼ŒFalseè¿”å›å­—å…¸ï¼‰
            
        Returns:
            å›¾æ•°æ®æœç´¢ç»“æœ
        """
        from Config.neo4j_config import is_neo4j_enabled
        if not is_neo4j_enabled():
            if not merge_result:
                return {"error_code": 7, "error_msg": "Neo4j is disabled."}
            else:
                return []
                
        if "query" not in param.keys():
            if not merge_result:
                return {"error_code": 3, "error_msg": "Error, lack of query."}
            else:
                return []
                
        query_text = param["query"]
        knowledge_id = param.get("knowledge_id")
        user_id = param.get("user_id")
        
        # å®ä½“å…³ç³»æŠ½å–
        key_word_json = entity_relation_split_run.entity_relation_split_run(query_text)
        
        if "decomposed_query" not in key_word_json.keys():
            if not merge_result:
                return {"error_code": 4, "error_msg": "Error, the decomposed_query is not exist."}
            else:
                return []
        if "entities" not in key_word_json["decomposed_query"].keys():
            if not merge_result:
                return {"error_code": 5, "error_msg": "Error, the entities is not exist."}
            else:
                return []
                
        flag = True
        if not self.check_knowledge_and_user(knowledge_id, user_id):
            flag = False

        entities = key_word_json.get("decomposed_query", {}).get("entities", [])
        keywords = key_word_json.get("decomposed_query", {}).get("keywords", [])
        entities.extend(keywords)
        if len(entities) == 0:
            if not merge_result:
                return {"error_code": 6, "error_msg": "Error, the entities is empty."}
            else:
                return []
        
        # å®ä½“åŒ¹é…å’Œç­›é€‰
        entity_list = []
        for _e in entities:
            param_query = {"knowledge_id": knowledge_id, "entity_name": _e}
            _e_em = cSingleEmb.embeddings.embed_query(_e)
            if flag is False:
                _entity = cSingleSqlite.query_graph_node_by_node_name_public(param_query)
            else:
                param_query = {"entity_name": _e, "knowledge_id": knowledge_id}
                _entity = cSingleSqlite.query_graph_node_by_node_name(param_query)
            if _entity is not None and len(_entity) > 0:
                en_lt = []
                for _en in _entity:
                    if _en["entity_name"] not in entity_list:
                        en_lt.append(_en["entity_name"])
            
                emb_list = cSingleEmb.embeddings.embed_documents(en_lt)
                key_list = []
                for i in range(len(en_lt)):
                    _emb = emb_list[i]
                    score = utils.cos_sim(_e_em, _emb)
                    key_list.append({"entity": en_lt[i], "score": score})
                key_list = sorted(key_list, key=lambda x: x["score"], reverse=True)
                if len(key_list) > 2:
                    key_list = key_list[0:2]
                entity_list.extend([k["entity"] for k in key_list])
        
        if len(entity_list) == 0:
            if not merge_result:
                return {"error_code": 6, "error_msg": "Error, the entities is empty."}
            else:
                return []
        
        # æ‰§è¡Œ Cypher æŸ¥è¯¢
        graph_data = []
        for entity in entity_list:
            if flag:
                cypher_query = """MATCH (start_node {entity_id: '"""+entity+"""'})-[relation]-(end_node) RETURN start_node, relation, end_node"""
            else:
                cypher_query = """MATCH (start_node {entity_id: '"""+entity+"""', permission_level: 'public'})-[relation]-(end_node {permission_level: 'public'}) RETURN start_node, relation, end_node"""
            cypher_result = {"cypher_query": cypher_query}
            if "cypher_query" in cypher_result.keys():
                cypher_query = cypher_result["cypher_query"]
                query_dict = {"cypher_query": cypher_query}
                results = self.graph_obj.execute_query(query_dict)
                
                tmp_list = []
                for _item in results:
                    _tm_d = {}
                    s_node = _item.get("start_node")
                    s_node_d = {}
                    s_node_d["entity_name"] = s_node.get("entity_id", "")
                    s_node_d["entity_type"] = s_node.get("entity_type", "")
                    s_node_d["description"] = s_node.get("description", "")
                    s_node_d["file"] = s_node.get("file_path", "")
                    s_node_d["created_at"] = s_node.get("created_at", "")
                    s_node_d["source_id"] = s_node.get("source_id", "")
                    
                    if "source_id" in s_node_d.keys():
                        chunk_param = {"chunk_id": s_node_d["source_id"],
                                 "knowledge_id": knowledge_id}
                        chunk_list = []
                        if "<SEP>" in s_node_d["source_id"]:
                            source_id_lt = s_node_d["source_id"].split("<SEP>")
                            for source_id in source_id_lt:
                                chunk_param = {"chunk_id": source_id,
                                     "knowledge_id": knowledge_id}
                                chunk_lt = cSingleSqlite.query_graph_chunk_by_chunk_id_and_knowledge_id(chunk_param)
                                for _ch in chunk_lt:
                                    chunk_list.append(_ch)
                        else:
                            chunk_list = cSingleSqlite.query_graph_chunk_by_chunk_id_and_knowledge_id(chunk_param) 
                        s_node_d["chunks"] = [chunk["chunk_text"] for chunk in chunk_list]
                        s_node_d["titles"] = [chunk["chunk_summary"] for chunk in chunk_list]
                       
                        s_node_d.pop("source_id")
                    
                    if "created_at" in s_node_d.keys():
                        s_node_d.pop("created_at")
                    _tm_d["start_node"] = s_node_d
                    
                    r_node = _item.get("relation")
                    r_node_d = {}
                    r_node_d["description"] = r_node.get("description", "")
                    r_node_d["keywords"] = r_node.get("keywords", "")
                    r_node_d["file_path"] = r_node.get("file_path", "")
                    r_node_d["source_id"] = r_node.get("source_id", "")
                    r_node_d["weight"] = r_node.get("weight", 1.0)
                    if "source_id" in r_node_d.keys():
                        r_node_d.pop("source_id")
                    _tm_d["relation"] = r_node_d
                    
                    e_node = _item.get("end_node")
                    e_node_d = {}
                    e_node_d["entity_name"] = e_node.get("entity_id", "")
                    e_node_d["entity_type"] = e_node.get("entity_type", "")
                    e_node_d["description"] = e_node.get("description", "")
                    e_node_d["file"] = e_node.get("file_path", "")
                    e_node_d["created_at"] = e_node.get("created_at", "")
                    e_node_d["source_id"] = e_node.get("source_id", "")
                    if "source_id" in e_node_d.keys():
                        
                        chunk_param = {"chunk_id": e_node_d["source_id"],
                                 "knowledge_id": knowledge_id}
                        chunk_list = []
                        if "<SEP>" in e_node_d["source_id"]:
                            source_id_lt = e_node_d["source_id"].split("<SEP>")
                            for source_id in source_id_lt:
                                chunk_param = {"chunk_id": source_id,
                                     "knowledge_id": knowledge_id}
                                chunk_lt = cSingleSqlite.query_graph_chunk_by_chunk_id_and_knowledge_id(chunk_param)
                                for _ch in chunk_lt:
                                    chunk_list.append(_ch)
                        else:
                            chunk_list = cSingleSqlite.query_graph_chunk_by_chunk_id_and_knowledge_id(chunk_param)
                        
                        e_node_d["chunks"] = [chunk["chunk_text"] for chunk in chunk_list]
                        e_node_d["titles"] = [chunk["chunk_summary"] for chunk in chunk_list]
                        
                        e_node_d.pop("source_id")
                        
                    if "created_at" in e_node_d.keys():
                        e_node_d.pop("created_at")
                    _tm_d["end_node"] = e_node_d
                    
                    tmp_list.append(_tm_d)
                if len(tmp_list) > 0:
                    graph_data.append(tmp_list)
                    
        if not merge_result:
            return {"error_code": 0, "error_msg": "Success", "data": graph_data}
        else:
            return graph_data
    
    def search_graph_data(self, param: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åœ¨å›¾æ•°æ®ä¸­æœç´¢ç›¸å…³å†…å®¹ï¼Œå¹¶è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        
        Args:
            param: æœç´¢å‚æ•°
                - query: æŸ¥è¯¢æ–‡æœ¬
                - knowledge_id: çŸ¥è¯†åº“ID
                - user_id: ç”¨æˆ·ID
                
        Returns:
            å›¾æ•°æ®æœç´¢ç»“æœåˆ—è¡¨
        """
        import re
        
        try:
            print("ğŸ•¸ï¸ æ‰§è¡Œå›¾æ•°æ®æœç´¢...")
            
            # ä½¿ç”¨ query_graph_neo4j è·å–å›¾æ•°æ®
            graph_data = self.query_graph_neo4j(param, merge_result=True)

            if not graph_data:
                print("âš ï¸ æ²¡æœ‰è·å–åˆ°å›¾æ•°æ®")
                return []

            results = []
            query_text = param.get("query", "")
            query_lower = query_text.lower()

            # éå†æ‰€æœ‰å›¾å…³ç³»
            for relation_group in graph_data:
                for relation in relation_group:
                    try:
                        start_node = relation.get("start_node", {})
                        end_node = relation.get("end_node", {})
                        relation_info = relation.get("relation", {})

                        # æå–ç›¸å…³æ–‡æœ¬è¿›è¡ŒåŒ¹é…
                        texts_to_search = []
                        media_info = {
                            "images": [],
                            "tables": []
                        }

                        # èŠ‚ç‚¹æè¿°
                        if start_node.get("description"):
                            texts_to_search.append(start_node["description"])
                        if end_node.get("description"):
                            texts_to_search.append(end_node["description"])

                        # å…³ç³»æè¿°
                        if relation_info.get("description"):
                            texts_to_search.append(relation_info["description"])

                        # å…³ç³»å…³é”®è¯
                        if relation_info.get("keywords"):
                            texts_to_search.append(relation_info["keywords"])

                        # å¤„ç†chunkså†…å®¹
                        for node in [start_node, end_node]:
                            chunks = node.get("chunks", [])
                            titles = node.get("titles", [])

                            for i, chunk in enumerate(chunks):
                                if isinstance(chunk, str) and chunk not in ["chunk1", "chunk2"]:
                                    texts_to_search.append(chunk)

                                    # æå–å›¾ç‰‡ä¿¡æ¯
                                    img_matches = re.findall(r'<img[^>]+src=["\']([^"\']+)["\']', chunk, re.IGNORECASE)
                                    media_info["images"].extend(img_matches)

                                    http_matches = re.findall(r'https?://[^\s]+\.(?:jpg|jpeg|png|gif|webp|bmp|svg)(?:\?[^\s]*)?', chunk, re.IGNORECASE)
                                    media_info["images"].extend(http_matches)

                                    # æå–è¡¨æ ¼ä¿¡æ¯
                                    if '<table' in chunk or '<tr' in chunk:
                                        clean_table = re.sub(r'<[^>]+>', ' | ', chunk)
                                        clean_table = re.sub(r'\s+', ' ', clean_table).strip()
                                        if len(clean_table) > 20:
                                            media_info["tables"].append({
                                                "content": clean_table[:1000],
                                                "title": titles[i] if i < len(titles) and titles[i] != f"title{i+1}" else f"è¡¨æ ¼{i+1}"
                                            })

                        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                        max_score = 0
                        for text in texts_to_search:
                            if not isinstance(text, str):
                                continue

                            text_lower = text.lower()
                            score = 0
                            query_words = query_lower.split()

                            for word in query_words:
                                if word in text_lower:
                                    score += 1

                            if score > 1:
                                score *= 1.5

                            if score > max_score:
                                max_score = score

                        # å¦‚æœæ‰¾åˆ°ç›¸å…³å†…å®¹
                        if max_score > 0:
                            result = {
                                "search_engine": "graph_data",
                                "title": f"å›¾å…³ç³»: {start_node.get('entity_id', 'æœªçŸ¥')} â†’ {end_node.get('entity_id', 'æœªçŸ¥')}",
                                "content": f"å…³ç³»æè¿°: {relation_info.get('description', 'æ— æè¿°')}\n\nèµ·å§‹èŠ‚ç‚¹: {start_node.get('entity_id', 'æœªçŸ¥')} ({start_node.get('entity_type', 'æœªçŸ¥ç±»å‹')})\n{start_node.get('description', '')[:200]}...\n\nç»“æŸèŠ‚ç‚¹: {end_node.get('entity_id', 'æœªçŸ¥')} ({end_node.get('entity_type', 'æœªçŸ¥ç±»å‹')})\n{end_node.get('description', '')[:200]}...",
                                "score": max_score,
                                "combined_score": max_score,
                                "graph_relation": {
                                    "start_node": {
                                        "entity_id": start_node.get("entity_id"),
                                        "entity_type": start_node.get("entity_type"),
                                        "description": start_node.get("description", ""),
                                        "chunks": start_node.get("chunks", []),
                                        "titles": start_node.get("titles", [])
                                    },
                                    "end_node": {
                                        "entity_id": end_node.get("entity_id"),
                                        "entity_type": end_node.get("entity_type"),
                                        "description": end_node.get("description", ""),
                                        "chunks": end_node.get("chunks", []),
                                        "titles": end_node.get("titles", [])
                                    },
                                    "relation": {
                                        "description": relation_info.get("description", ""),
                                        "keywords": relation_info.get("keywords", ""),
                                        "weight": relation_info.get("weight", 0)
                                    }
                                },
                                "metadata": {
                                    "start_entity": start_node.get("entity_id"),
                                    "end_entity": end_node.get("entity_id"),
                                    "relation_type": "graph_relation",
                                    "has_images": len(media_info["images"]) > 0,
                                    "has_tables": len(media_info["tables"]) > 0,
                                    "image_count": len(media_info["images"]),
                                    "table_count": len(media_info["tables"])
                                },
                                "media_content": media_info
                            }
                            results.append(result)

                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†å›¾å…³ç³»æ—¶å‡ºé”™: {e}")
                        continue

            # æŒ‰åˆ†æ•°æ’åº
            results.sort(key=lambda x: x.get("score", 0), reverse=True)

            # é™åˆ¶ç»“æœæ•°é‡
            results = results[:10]

            print(f"âœ… å›¾æ•°æ®æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            return results

        except Exception as e:
            print(f"âŒ å›¾æ•°æ®æœç´¢å¼‚å¸¸: {e}")
            return []
    
