# -*- coding:utf-8 -*-

'''
æ•°æ®åº“è¿æ¥æ§åˆ¶æ¨¡å—
'''

import os
import re
import logging
import threading
import uuid
import json
import traceback
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Set
import networkx as nx
# from typing import Dict, Any, List, Optional
try:
    import pymysql
    MYSQL_AVAILABLE = True
except ImportError:
    MYSQL_AVAILABLE = False

try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
    POSTGRESQL_AVAILABLE = True
except ImportError:
    POSTGRESQL_AVAILABLE = False

from Db.sqlite_db import cSingleSqlite
from Sql.schema_vector import SqlSchemaVectorAgent
# from Sql.vanna_manager import get_vanna_manager

from Control.control_elastic import CControl as ElasticSearchController
from Control.control_graph import CControl as GraphController

# from Agent import analysis_schema_run
from Agent.AgenticSqlAgent.AnalysisSql.database_analysis_agent import DatabaseAnalysisAgent

# from Agent.SqlIntelligentAgents.sql_intelligent_workflow import SqlIntelligentWorkflow
# from Agent.SqlIntelligentAgents.select_sql_agent import SelectSqlAgent
# åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # è®¾ç½®æ—¥å¿—çº§åˆ«
logger_lock = threading.Lock()

def thread_safe_log(level_func, message, *args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—è®°å½•å‡½æ•°"""
    with logger_lock:
        level_func(message, *args, **kwargs)

class CControl:
    
    def __init__(self):
        self.db_obj = cSingleSqlite
        self.elasticsearch_obj = ElasticSearchController()
        self.graph_obj = GraphController()
        self.vector_agent = SqlSchemaVectorAgent()
        # self.graph_id = "graphiti_sql_knowledge_graph"
    
    def generate_id(self, prefix=''):
        """ç”Ÿæˆå”¯ä¸€ID"""
        return f"{prefix}_{uuid.uuid4().hex[:16]}" if prefix else uuid.uuid4().hex[:16]
    
    def connect_database(self, ip, port, sql_type, sql_name, sql_user_name, sql_user_password):
        """è¿æ¥æ•°æ®åº“"""
        try:
            if sql_type == 'mysql':
                if not MYSQL_AVAILABLE:
                    raise Exception("PyMySQLæœªå®‰è£…ï¼Œæ— æ³•è¿æ¥MySQLæ•°æ®åº“")
                conn = pymysql.connect(
                    host=ip,
                    port=int(port),
                    user=sql_user_name,
                    password=sql_user_password,
                    database=sql_name,
                    charset='utf8mb4'
                )
                return conn, 'mysql'
            elif sql_type == 'postgresql':
                if not POSTGRESQL_AVAILABLE:
                    raise Exception("psycopg2æœªå®‰è£…ï¼Œæ— æ³•è¿æ¥PostgreSQLæ•°æ®åº“")
                conn = psycopg2.connect(
                    host=ip,
                    port=int(port),
                    user=sql_user_name,
                    password=sql_user_password,
                    database=sql_name
                )
                return conn, 'postgresql'
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {sql_type}")
        except Exception as e:
            logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def get_tables(self, conn, db_type):
        """è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨"""
        try:
            cursor = conn.cursor()
            if db_type == 'mysql':
                cursor.execute("SHOW TABLES")
                tables = [row[0] for row in cursor.fetchall()]
            elif db_type == 'postgresql':
                cursor.execute("""
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_schema = 'public'
                """)
                tables = [row[0] for row in cursor.fetchall()]
            else:
                tables = []
            cursor.close()
            return tables
        except Exception as e:
            logger.error(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    def get_table_description(self, conn, db_type, table_name):
        """è·å–è¡¨çš„æè¿°ä¿¡æ¯
        
        Args:
            conn: æ•°æ®åº“è¿æ¥
            db_type: æ•°æ®åº“ç±»å‹ ('mysql' æˆ– 'postgresql')
            table_name: è¡¨å
            
        Returns:
            str: è¡¨çš„æè¿°ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            cursor = conn.cursor()
            table_description = ""
            
            if db_type == 'mysql':
                # MySQL: æŸ¥è¯¢ information_schema.tables è·å– TABLE_COMMENT
                # å…ˆè·å–å½“å‰æ•°æ®åº“åç§°
                cursor.execute("SELECT DATABASE()")
                db_name_row = cursor.fetchone()
                db_name = db_name_row[0] if db_name_row and db_name_row[0] else None
                
                if db_name:
                    cursor.execute("""
                        SELECT TABLE_COMMENT 
                        FROM information_schema.TABLES 
                        WHERE TABLE_SCHEMA = %s 
                        AND TABLE_NAME = %s
                    """, (db_name, table_name))
                    row = cursor.fetchone()
                    if row and row[0]:
                        table_description = row[0]
            elif db_type == 'postgresql':
                # PostgreSQL: æŸ¥è¯¢ pg_catalog.pg_description è·å–è¡¨çš„æè¿°
                cursor.execute("""
                    SELECT obj_description(pc.oid, 'pg_class') AS table_comment
                    FROM pg_catalog.pg_class pc
                    LEFT JOIN pg_catalog.pg_namespace pn
                      ON pn.oid = pc.relnamespace
                    WHERE pc.relname = %s
                      AND pn.nspname = 'public'
                """, (table_name,))
                row = cursor.fetchone()
                if row and row[0]:
                    table_description = row[0]
            
            cursor.close()
            return table_description if table_description else ""
        except Exception as e:
            logger.warning(f"è·å–è¡¨æè¿°ä¿¡æ¯å¤±è´¥: {e}")
            return ""
    
    def get_table_columns(self, conn, db_type, table_name):
        """è·å–è¡¨çš„åˆ—ä¿¡æ¯
        
        Args:
            conn: æ•°æ®åº“è¿æ¥
            db_type: æ•°æ®åº“ç±»å‹ ('mysql' æˆ– 'postgresql')
            table_name: è¡¨å
            
        Returns:
            dict: åŒ…å« 'columns' å’Œ 'table_description' çš„å­—å…¸
                - columns: åˆ—ä¿¡æ¯åˆ—è¡¨
                - table_description: è¡¨çš„æè¿°ä¿¡æ¯
        """
        try:
            cursor = conn.cursor()
            columns = []
            
            # è·å–è¡¨æè¿°ä¿¡æ¯
            table_description = self.get_table_description(conn, db_type, table_name)
            
            if db_type == 'mysql':
                cursor.execute(f"SHOW FULL COLUMNS FROM `{table_name}`")
                rows = cursor.fetchall()
                for row in rows:
                    # SHOW FULL COLUMNS è¿”å›:
                    # Field, Type, Collation, Null, Key, Default, Extra, Privileges, Comment
                    col_info = {
                        'col_name': row[0],
                        'col_type': row[1],
                        'collation': row[2],
                        'is_null': row[3],
                        'key': row[4],
                        'default': row[5],
                        'extra': row[6],
                        'privileges': row[7],
                        'comment': row[8]
                    }
                    columns.append(col_info)
            elif db_type == 'postgresql':
                cursor.execute("""
                    SELECT 
                        c.column_name,
                        c.data_type,
                        c.is_nullable,
                        c.column_default,
                        pgd.description AS column_comment
                    FROM information_schema.columns c
                    LEFT JOIN pg_catalog.pg_class pc 
                      ON pc.relname = c.table_name
                    LEFT JOIN pg_catalog.pg_namespace pn
                      ON pn.nspname = c.table_schema 
                     AND pn.oid = pc.relnamespace
                    LEFT JOIN pg_catalog.pg_description pgd
                      ON pgd.objoid = pc.oid 
                     AND pgd.objsubid = c.ordinal_position
                    WHERE c.table_name = %s
                      AND c.table_schema = 'public'
                    ORDER BY c.ordinal_position
                """, (table_name,))
                rows = cursor.fetchall()
                for row in rows:
                    col_info = {
                        'col_name': row[0],
                        'col_type': row[1],
                        'is_nullable': row[2],
                        'column_default': row[3],
                        'comment': row[4]
                    }
                    columns.append(col_info)
            
            cursor.close()
            return {
                'columns': columns,
                'table_description': table_description
            }
        except Exception as e:
            logger.error(f"è·å–è¡¨åˆ—ä¿¡æ¯å¤±è´¥: {e}")
            raise

    def get_table_foreign_keys(self, conn, db_type, table_name):
        """è·å–è¡¨çš„å¤–é”®ä¿¡æ¯"""
        try:
            cursor = conn.cursor()
            foreign_keys = []

            if db_type == 'mysql':
                cursor.execute("""
                    SELECT 
                        kcu.TABLE_NAME,
                        kcu.COLUMN_NAME,
                        kcu.REFERENCED_TABLE_NAME,
                        kcu.REFERENCED_COLUMN_NAME
                    FROM information_schema.KEY_COLUMN_USAGE kcu
                    WHERE kcu.TABLE_SCHEMA = DATABASE()
                      AND kcu.TABLE_NAME = %s
                      AND kcu.REFERENCED_TABLE_NAME IS NOT NULL
                """, (table_name,))
                rows = cursor.fetchall()
                for row in rows:
                    foreign_keys.append({
                        'from_table': row[0],
                        'from_col': row[1],
                        'to_table': row[2],
                        'to_col': row[3]
                    })
            elif db_type == 'postgresql':
                cursor.execute("""
                    SELECT
                        tc.table_name AS from_table,
                        kcu.column_name AS from_column,
                        ccu.table_name AS to_table,
                        ccu.column_name AS to_column
                    FROM information_schema.table_constraints AS tc
                    JOIN information_schema.key_column_usage AS kcu
                      ON tc.constraint_name = kcu.constraint_name
                     AND tc.table_schema = kcu.table_schema
                    JOIN information_schema.constraint_column_usage AS ccu
                      ON ccu.constraint_name = tc.constraint_name
                     AND ccu.table_schema = tc.table_schema
                    WHERE tc.constraint_type = 'FOREIGN KEY'
                      AND tc.table_name = %s
                      AND tc.table_schema = 'public'
                """, (table_name,))
                rows = cursor.fetchall()
                for row in rows:
                    foreign_keys.append({
                        'from_table': row[0],
                        'from_col': row[1],
                        'to_table': row[2],
                        'to_col': row[3]
                    })

            cursor.close()
            return foreign_keys
        except Exception as e:
            logger.error(f"è·å–å¤–é”®ä¿¡æ¯å¤±è´¥: {e}")
            raise
    
    def get_relation(self, sql_id):
        all_foreign_keys_map = {}
        relations = cSingleSqlite.query_rel_sql_by_sql_id(sql_id)
        for rel in relations or []:
            from_table = rel.get("from_table", "")
            if from_table not in all_foreign_keys_map:
                all_foreign_keys_map[from_table] = []
        
            all_foreign_keys_map[from_table].append({
                "from_table": from_table,
                        "from_col": rel.get("from_col", ""),
                        "to_table": rel.get("to_table", ""),
                        "to_col": rel.get("to_col", "")
                    })
        return all_foreign_keys_map
    
    def analysis_schema(self, table):
        """
        åŸºäºè§„åˆ™çš„Schemaåˆ†æï¼ˆä¸ä½¿ç”¨æ™ºèƒ½ä½“ï¼‰
        
        Args:
            table: è¡¨ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼š
                - table_name: è¡¨å
                - table_description: è¡¨æè¿°
                - columns: åˆ—ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                    - col_name: åˆ—å
                    - col_type: åˆ—ç±»å‹
                    - col_info: åˆ—è¯¦ç»†ä¿¡æ¯ï¼ˆå­—å…¸ï¼ŒåŒ…å«comment, is_nullableç­‰ï¼‰
        
        Returns:
            Dict: åˆ†æç»“æœï¼Œæ ¼å¼è§æ³¨é‡Š
        """
        try:
            table_name = table.get("table_name", "")
            table_description = table.get("table_description", "")
            columns = table.get("columns", [])
            sql_id = table.get("sql_id", "")
            
            if not table_name:
                return {
                    "success": False,
                    "table_name": "",
                    "error": "è¡¨åä¸ºç©º"
                }
            
            # 1. åˆ¤æ–­æ˜¯å¦ä¸ºæµ‹è¯•è¡¨
            is_test_table = self._is_test_table(table_name, table_description)
            
            # 2. åˆ¤æ–­å‘½åè§„èŒƒç¨‹åº¦
            naming_standard = self._check_naming_standard(table_name, columns)
            
            # 3. ç¡®å®šä¸»ä½“ï¼ˆentityï¼‰
            entity_name, entity_description = self._extract_entity(table_name, table_description)
            
            # 4. è·å–å¤–é”®ä¿¡æ¯
            foreign_keys_list = []
            if sql_id:
                all_foreign_keys_map = self.get_relation(sql_id)
                foreign_keys_list = all_foreign_keys_map.get(table_name, [])
            
            # 5. å¤„ç†åˆ—ä¿¡æ¯
            attributes = []
            metrics = []
            unique_identifiers = []
            
            # è·å–ä¸»é”®å’Œå”¯ä¸€é”®ä¿¡æ¯ï¼ˆä»åˆ—ä¿¡æ¯ä¸­æ¨æ–­ï¼‰
            primary_keys = []
            unique_keys = []
            
            for col in columns:
                col_name = col.get("col_name", "")
                col_type = col.get("col_type", "").lower()
                
                # è·å–åˆ—æ³¨é‡Šï¼ˆä¼˜å…ˆä» col_commentï¼Œå…¶æ¬¡ä» col_infoï¼‰
                col_comment = col.get("col_comment", "")
                
                # å°è¯•ä» col_info è·å–æ›´å¤šä¿¡æ¯
                col_info = col.get("col_info", {})
                if isinstance(col_info, str):
                    try:
                        col_info = json.loads(col_info)
                    except:
                        col_info = {}
                elif col_info is None:
                    col_info = {}
                
                # å¦‚æœ col_comment ä¸ºç©ºï¼Œå°è¯•ä» col_info è·å–
                if not col_comment and isinstance(col_info, dict):
                    col_comment = col_info.get("comment", "")
                
                # åˆ¤æ–­ä¸»é”®å’Œå”¯ä¸€é”®
                # æ–¹æ³•1ï¼šä» col_info ä¸­è·å–ï¼ˆMySQLçš„keyå­—æ®µï¼šPRI=ä¸»é”®ï¼ŒUNI=å”¯ä¸€é”®ï¼‰
                is_primary = False
                is_unique = False
                
                if isinstance(col_info, dict):
                    key_type = col_info.get("key", "").upper()
                    if key_type == "PRI":
                        is_primary = True
                    elif key_type == "UNI":
                        is_unique = True
                    # ä¹Ÿæ£€æŸ¥å…¶ä»–å¯èƒ½çš„å­—æ®µ
                    if col_info.get("is_primary", False):
                        is_primary = True
                    if col_info.get("is_unique", False):
                        is_unique = True
                
                # æ–¹æ³•2ï¼šæ ¹æ®åˆ—åæ¨æ–­ï¼ˆå¸¸è§å‘½åè§„åˆ™ï¼‰
                col_name_lower = col_name.lower()
                table_name_lower = table_name.lower()
                
                # ä¸»é”®å¸¸è§å‘½åï¼šid, table_id
                if not is_primary and (col_name_lower == 'id' or col_name_lower == f"{table_name_lower}_id"):
                    is_primary = True
                # å”¯ä¸€é”®å¸¸è§å‘½åï¼šcode, no, number ç­‰ï¼ˆä¸”ä¸æ˜¯ä¸»é”®ï¼‰
                elif not is_unique and not is_primary and col_name_lower in ['code', 'no', 'number', 'sn', 'serial_no']:
                    is_unique = True
                
                # å¤„ç†åˆ—åå’Œæ³¨é‡Š
                col_display_name = self._parse_column_name(col_name)
                col_description = col_comment if col_comment else col_display_name
                
                # åˆ¤æ–­åˆ—ç±»å‹
                is_numeric = self._is_numeric_type(col_type)
                is_text = self._is_text_type(col_type)
                is_datetime = self._is_datetime_type(col_type)
                
                # ä¸»é”®å’Œå”¯ä¸€é”®ï¼ˆä¼˜å…ˆå¤„ç†ï¼Œå› ä¸ºå®ƒä»¬ä¸åº”è¯¥åŒæ—¶å‡ºç°åœ¨å±æ€§å’ŒæŒ‡æ ‡ä¸­ï¼‰
                if is_primary:
                    primary_keys.append(col_name)
                    unique_identifiers.append({
                        "identifier_name": col_display_name,
                        "identifier_type": "primary_key",
                        "col_name": col_name,
                        "description": col_description
                    })
                    # ä¸»é”®é€šå¸¸ä¹Ÿæ˜¯å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œè·³è¿‡åç»­çš„å±æ€§/æŒ‡æ ‡åˆ†ç±»
                    continue
                elif is_unique:
                    unique_keys.append(col_name)
                    unique_identifiers.append({
                        "identifier_name": col_display_name,
                        "identifier_type": "unique_key",
                        "col_name": col_name,
                        "description": col_description
                    })
                    # å”¯ä¸€é”®ä¹Ÿè·³è¿‡åç»­çš„å±æ€§/æŒ‡æ ‡åˆ†ç±»
                    continue
                
                # æ—¶é—´ç±»å‹åˆ—ï¼ˆä½œä¸ºå±æ€§ï¼‰
                if is_datetime:
                    attributes.append({
                        "attr_name": col_display_name,
                        "attr_type": "datetime",
                        "attr_description": col_description,
                        "col_name": col_name
                    })
                # æ•°å€¼ç±»å‹åˆ—ï¼ˆä½œä¸ºæŒ‡æ ‡ï¼‰
                elif is_numeric:
                    metrics.append({
                        "metric_name": col_display_name,
                        "metric_type": "numeric",  # å¯ä»¥æ˜¯ count, sum, avg, max, min ç­‰
                        "metric_description": col_description,
                        "col_name": col_name
                    })
                # æ–‡æœ¬ç±»å‹åˆ—ï¼ˆä½œä¸ºå±æ€§ï¼‰
                elif is_text:
                    attributes.append({
                        "attr_name": col_display_name,
                        "attr_type": "text",
                        "attr_description": col_description,
                        "col_name": col_name
                    })
                else:
                    # å…¶ä»–ç±»å‹é»˜è®¤ä½œä¸ºå±æ€§
                    attributes.append({
                        "attr_name": col_display_name,
                        "attr_type": "other",
                        "attr_description": col_description,
                        "col_name": col_name
                    })
            
            # 6. å¤„ç†å¤–é”®ä¿¡æ¯
            foreign_keys_result = []
            for fk in foreign_keys_list:
                from_col = fk.get("from_col", "")
                to_table = fk.get("to_table", "")
                to_col = fk.get("to_col", "")
                
                # æ¨æ–­å…³ç³»ç±»å‹ï¼ˆç®€åŒ–å¤„ç†ï¼Œé»˜è®¤ä¸º one_to_manyï¼‰
                relationship_type = "entity_to_entity"
                
                foreign_keys_result.append({
                    "fk_name": f"{table_name}_{from_col}_fk",
                    "from_col": from_col,
                    "to_table": to_table,
                    "to_col": to_col,
                    "relationship_type": relationship_type,
                    "description": f"å…³è”åˆ° {to_table} è¡¨çš„ {to_col} å­—æ®µ"
                })
            
            # 7. ç”Ÿæˆåˆ†æç†ç”±
            analysis_reason = self._generate_analysis_reason(
                table_name, entity_name, len(attributes), len(metrics), 
                len(unique_identifiers), len(foreign_keys_result)
            )
            
            # è¿”å›æ ¼å¼éœ€è¦ä¸ save_schema_analysis_graph_data æœŸæœ›çš„æ ¼å¼ä¸€è‡´
            return {
                "table_name": table_name,
                "table_id": table.get("table_id", ""),
                "analysis_result": {
                    "success": True,
                    "table_name": table_name,
                    "is_test_table": is_test_table,
                    "naming_standard": naming_standard,
                    "entity": {
                        "entity_name": entity_name,
                        "entity_description": entity_description
                    },
                    "attributes": attributes,
                    "metrics": metrics,
                    "unique_identifiers": unique_identifiers,
                    "foreign_keys": foreign_keys_result,
                    "analysis_reason": analysis_reason
                }
            }
            
        except Exception as e:
            traceback.print_exc()
            return {
                "table_name": table.get("table_name", ""),
                "table_id": table.get("table_id", ""),
                "analysis_result": {
                    "success": False,
                    "table_name": table.get("table_name", ""),
                    "error": str(e)
                }
            }
    
    def _is_test_table(self, table_name: str, table_description: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæµ‹è¯•è¡¨"""
        test_keywords = ['test', 'demo', 'tmp', 'temp', 'sample', 'æµ‹è¯•', 'ç¤ºä¾‹', 'ä¸´æ—¶', 'demo']
        table_name_lower = table_name.lower()
        desc_lower = (table_description or "").lower()
        
        # æ£€æŸ¥è¡¨å
        if any(keyword in table_name_lower for keyword in test_keywords):
            return True
        
        # æ£€æŸ¥è¡¨æè¿°
        if table_description and any(keyword in desc_lower for keyword in test_keywords):
            return True
        
        return False
    
    def _check_naming_standard(self, table_name: str, columns: list) -> str:
        """æ£€æŸ¥å‘½åè§„èŒƒç¨‹åº¦"""
        # æ£€æŸ¥è¡¨åï¼šsnake_case æˆ– camelCase éƒ½ç®—æ ‡å‡†
        table_pattern = re.compile(r'^[a-z][a-z0-9_]*$|^[a-z][a-zA-Z0-9]*$')
        if not table_pattern.match(table_name.lower()):
            return "non_standard"
        
        # æ£€æŸ¥åˆ—åï¼šè‡³å°‘80%çš„åˆ—ç¬¦åˆè§„èŒƒ
        standard_count = 0
        total_count = 0
        
        for col in columns:
            col_name = col.get("col_name", "")
            if not col_name:
                continue
            
            total_count += 1
            col_pattern = re.compile(r'^[a-z][a-z0-9_]*$|^[a-z][a-zA-Z0-9]*$')
            if col_pattern.match(col_name.lower()):
                standard_count += 1
        
        if total_count == 0:
            return "standard"
        
        # å¦‚æœ80%ä»¥ä¸Šç¬¦åˆè§„èŒƒï¼Œè®¤ä¸ºæ˜¯æ ‡å‡†å‘½å
        if standard_count / total_count >= 0.8:
            return "standard"
        else:
            return "non_standard"
    
    def _extract_entity(self, table_name: str, table_description: str) -> tuple:
        """æå–ä¸»ä½“ä¿¡æ¯"""
        # å¦‚æœæœ‰è¡¨æè¿°ï¼Œä½¿ç”¨è¡¨æè¿°ä½œä¸ºä¸»ä½“æè¿°
        entity_name = self._parse_table_name(table_name)
        if table_description:
            entity_description = table_description
        else:
            # ä½¿ç”¨è¡¨åä½œä¸ºä¸»ä½“åç§°
            entity_description = entity_name
        
        return entity_name, entity_description
    
    def _parse_table_name(self, table_name: str) -> str:
        """è§£æè¡¨åï¼ˆsnake_caseè½¬æˆå¯è¯»æ–‡æœ¬ï¼‰"""
        # ç§»é™¤è¡¨åå‰ç¼€ï¼ˆå¦‚ï¼št_, tbl_ç­‰ï¼‰
        name = table_name.lower()
        prefixes = ['t_', 'tbl_', 'table_']
        for prefix in prefixes:
            if name.startswith(prefix):
                name = name[len(prefix):]
                break
        
        # åˆ†å‰²ä¸‹åˆ’çº¿
        parts = name.split('_')
        # å°†æ¯ä¸ªéƒ¨åˆ†é¦–å­—æ¯å¤§å†™
        # parts = [part.capitalize() for part in parts if part]
        return " ".join(parts)
    
    def _parse_column_name(self, col_name: str) -> str:
        """è§£æåˆ—åï¼ˆsnake_caseè½¬æˆå¯è¯»æ–‡æœ¬ï¼‰"""
        # ç§»é™¤åˆ—ååç¼€ï¼ˆå¦‚ï¼š_id, _nameç­‰ï¼‰
        name = col_name.lower()
        
        # åˆ†å‰²ä¸‹åˆ’çº¿
        parts = name.split('_')
        # å°†æ¯ä¸ªéƒ¨åˆ†é¦–å­—æ¯å¤§å†™
        # parts = [part.capitalize() for part in parts if part]
        return " ".join(parts)
    
    def _is_numeric_type(self, col_type: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹"""
        numeric_types = ['int', 'integer', 'bigint', 'smallint', 'tinyint', 
                        'float', 'double', 'decimal', 'numeric', 'real', 
                        'number', 'money', 'smallmoney']
        return any(nt in col_type.lower() for nt in numeric_types)
    
    def _is_text_type(self, col_type: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬ç±»å‹"""
        text_types = ['varchar', 'char', 'text', 'string', 'nvarchar', 
                     'nchar', 'ntext', 'clob', 'blob']
        return any(tt in col_type.lower() for tt in text_types)
    
    def _is_datetime_type(self, col_type: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ—¶é—´ç±»å‹"""
        datetime_types = ['date', 'time', 'datetime', 'timestamp', 
                         'year', 'interval']
        return any(dt in col_type.lower() for dt in datetime_types)
    
    def _generate_analysis_reason(self, table_name: str, entity_name: str, 
                                  attr_count: int, metric_count: int,
                                  identifier_count: int, fk_count: int) -> str:
        """ç”Ÿæˆåˆ†æç†ç”±"""
        reasons = []
        reasons.append(f"è¡¨ {table_name} ä»£è¡¨ä¸šåŠ¡å®ä½“ï¼š{entity_name}")
        
        if attr_count > 0:
            reasons.append(f"åŒ…å« {attr_count} ä¸ªå±æ€§å­—æ®µ")
        if metric_count > 0:
            reasons.append(f"åŒ…å« {metric_count} ä¸ªæŒ‡æ ‡å­—æ®µ")
        if identifier_count > 0:
            reasons.append(f"åŒ…å« {identifier_count} ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦")
        if fk_count > 0:
            reasons.append(f"åŒ…å« {fk_count} ä¸ªå¤–é”®å…³è”")
        
        return "ï¼›".join(reasons)
    
    def insert_sql_info(self, param):
        """æ’å…¥æ•°æ®åº“ä¿¡æ¯ - è¿æ¥æ•°æ®åº“ï¼Œè·å–è¡¨å’Œåˆ—ä¿¡æ¯ï¼Œä¿å­˜åˆ°æ•°æ®åº“"""
        try:
            user_id = param.get("user_id")
            ip = param.get("ip")
            port = param.get("port")
            sql_type = param.get("sql_type")
            sql_name = param.get("sql_name")
            sql_user_name = param.get("sql_user_name")
            sql_user_password = param.get("sql_user_password")
            sql_description = param.get("sql_description", "")
            
            if not all([user_id, ip, port, sql_type, sql_name, sql_user_name, sql_user_password]):
                return {"success": False, "message": "ç¼ºå°‘å¿…è¦å‚æ•°"}
            
            # ç”Ÿæˆsql_id
            if("sql_id" in param.keys()):
                sql_id = param.get("sql_id")
            else:
                sql_id = self.generate_id("sql")
            
            # è¿æ¥æ•°æ®åº“æµ‹è¯•
            conn = None
            try:
                conn, db_type = self.connect_database(
                    ip, port, sql_type, sql_name, sql_user_name, sql_user_password
                )
                
                # è·å–æ‰€æœ‰è¡¨
                tables = self.get_tables(conn, db_type)
                logger.info(f"è·å–åˆ°{len(tables)}ä¸ªè¡¨")
                # ä¿å­˜base_sqlä¿¡æ¯
                base_sql_param = {
                    "sql_id": sql_id,
                    "user_id": user_id,
                    "ip": ip,
                    "port": port,
                    "sql_type": sql_type,
                    "sql_name": sql_name,
                    "sql_user_name": sql_user_name,
                    "sql_user_password": sql_user_password,
                    "sql_description": sql_description
                }
                
                if not self.db_obj.insert_base_sql(base_sql_param):
                    raise Exception("ä¿å­˜æ•°æ®åº“è¿æ¥ä¿¡æ¯å¤±è´¥")
                
                all_tables_analysis = []
                des_list = []
                for table_name in tables:
                    # è·å–è¡¨çš„åˆ—ä¿¡æ¯å’Œæè¿°ä¿¡æ¯
                    table_info = self.get_table_columns(conn, db_type, table_name)
                    columns = table_info.get("columns", [])
                    table_description = table_info.get("table_description", "")
                    
                    # ä¿å­˜è¡¨ä¿¡æ¯
                    table_id = self.generate_id("table")
                    table_sql_param = {
                        "table_id": table_id,
                        "sql_id": sql_id,
                        "table_name": table_name,
                        "table_description": table_description  # ä½¿ç”¨è·å–åˆ°çš„è¡¨æè¿°
                    }
                    self.db_obj.insert_table_sql(table_sql_param)
                    
                    columns_comments = []
                    # ä¿å­˜åˆ—ä¿¡æ¯
                    for col in columns:
                        col_id = self.generate_id("col")
                        
                        # åˆ¤æ–­åˆ—ç±»å‹
                        col_typy_ana = ""
                        col_type = col.get("col_type", "")
                        is_numeric = self._is_numeric_type(col_type)
                        if(is_numeric):
                            col_typy_ana = "numeric"
                        is_text = self._is_text_type(col_type)
                        if(is_text):
                            col_typy_ana = "attribute"
                        is_datetime = self._is_datetime_type(col_type)
                        if(is_datetime):
                            col_typy_ana = "datetime"
                        
                        col["ana_type"] = col_typy_ana
                        col_sql_param = {
                            "col_id": col_id,
                            "table_id": table_id,
                            "col_name": col["col_name"],
                            "col_type": col.get("col_type", ""),
                            "col_info": col  # ä¿å­˜å®Œæ•´çš„åˆ—ä¿¡æ¯JSON
                        }
                        
                        col_name = col["col_name"]
                        col_comment = col.get("comment", "")
                        if(col_comment):
                            columns_comments.append(col_comment)
                        else:
                            col_comment = self._parse_column_name(col_name)
                            columns_comments.append(col_comment)
                            
                        self.db_obj.insert_col_sql(col_sql_param)
                    
                    table_title = self._parse_table_name(table_name)
                    
                    if(table_description):
                        table_title = table_title + "\n" + table_description
                        
                    # ä¿å­˜å¤–é”®ä¿¡æ¯
                    foreign_keys = self.get_table_foreign_keys(conn, db_type, table_name)
                    for fk in foreign_keys:
                        rel_id = self.generate_id("rel")
                        rel_param = {
                            "rel_id": rel_id,
                            "sql_id": sql_id,
                            "from_table": fk.get("from_table"),
                            "from_col": fk.get("from_col"),
                            "to_table": fk.get("to_table"),
                            "to_col": fk.get("to_col")
                        }
                        self.db_obj.insert_rel_sql(rel_param)
                    
                    file_id = table_id
                    content_str = ";".join(columns_comments)
                    permission_level = "public"
                    # ä¿å­˜åˆ°Elasticsearchï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    from Config.elasticsearch_config import is_elasticsearch_enabled
                    if is_elasticsearch_enabled():
                        self.elasticsearch_obj.save_document_to_elastic(sql_id, file_id, 
                                                                        user_id, permission_level,
                                                                        table_title, content_str)
                    
                    des_list.append([table_id, table_title, content_str])

                    # ä»æ•°æ®åº“æŸ¥è¯¢å®Œæ•´çš„åˆ—ä¿¡æ¯ï¼ˆåŒ…å« col_infoï¼‰
                    full_columns = self.db_obj.query_col_sql_by_table_id(table_id)
                    
                    if(not table_description):
                        table_description = content_str
                    
                    # æ„å»ºå®Œæ•´çš„ table_info ç”¨äºåˆ†æ
                    analysis_table_info = {
                        "table_id": table_id,
                        "sql_id": sql_id,
                        "table_name": table_name,
                        "table_description": table_description,
                        "columns": full_columns  # ä½¿ç”¨å®Œæ•´çš„åˆ—ä¿¡æ¯ï¼ŒåŒ…å« col_info
                    }
                    
                    analysis_schema = self.analysis_schema(analysis_table_info)
                    all_tables_analysis.append(analysis_schema)
                    print("analysis_schema:", analysis_schema)
                    
                logger.info(f"ğŸ“Š å›è°ƒå‡½æ•°æ‰§è¡Œå®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(all_tables_analysis)} ä¸ªè¡¨çš„ç»“æœ")
                
                if(des_list):
                    try:
                        agent = DatabaseAnalysisAgent()
                        
                        logger.info(f"ğŸ“Š å¼€å§‹åˆ†ææ•°æ®åº“æè¿°æ–‡æœ¬ï¼Œå…± {len(des_list)} ä¸ªè¡¨")
                        
                        # ç›´æ¥è°ƒç”¨æ™ºèƒ½ä½“åˆ†ææ•´ä¸ªåˆ—è¡¨ï¼ˆdes_list æ ¼å¼: [["title", "content"], ...]ï¼‰
                        # des_list = des_list[:10]
                        result = agent.analyze_database_descriptions(des_list, sql_id)
                        
                        if result.get("success"):
                            logger.info(f"âœ… æ•°æ®åº“æè¿°æ–‡æœ¬åˆ†æå®Œæˆï¼Œå…±åˆ†æ {result.get('total_tables', 0)} ä¸ªè¡¨")
                            print("åˆ†ææ•°æ®åº“æè¿°æ–‡æœ¬ç»“æœ:", result)
                        else:
                            error_msg = result.get("error", "åˆ†æå¤±è´¥")
                            logger.warning(f"âš ï¸ æ•°æ®åº“æè¿°æ–‡æœ¬åˆ†æå¤±è´¥: {error_msg}")
                            return {"success": False, "message": f"åˆ†ææ•°æ®åº“æè¿°æ–‡æœ¬å¤±è´¥: {error_msg}"}
                    except Exception as e:
                        logger.error(f"åˆ†ææ•°æ®åº“æè¿°æ–‡æœ¬å¼‚å¸¸: {e}")
                        traceback.print_exc()
                        return {"success": False, "message": f"åˆ†ææ•°æ®åº“æè¿°æ–‡æœ¬å¤±è´¥: {str(e)}"}

                if all_tables_analysis:
                    try:
                        # å…ˆä¿å­˜åˆ° SQLiteï¼ˆæ‰¹é‡ä¿å­˜æ‰€æœ‰è¡¨çš„åˆ†æç»“æœï¼‰
                        try:
                            # ç»Ÿè®¡æµ‹è¯•è¡¨å’Œæ ‡å‡†å‘½åè¡¨çš„æ•°é‡
                            test_tables_count = sum(1 for ta in all_tables_analysis 
                                                  if ta.get("analysis_result", {}).get("is_test_table", False))
                            standard_naming_count = sum(1 for ta in all_tables_analysis 
                                                       if ta.get("analysis_result", {}).get("naming_standard") == "standard")
                            
                            # æ‰¹é‡ä¿å­˜æ‰€æœ‰è¡¨çš„åˆ†æç»“æœåˆ° SQLite
                            for table_analysis in all_tables_analysis:
                                table_id = table_analysis.get("table_id", "")
                                table_name = table_analysis.get("table_name", "")
                                analysis_result = table_analysis.get("analysis_result", {})
                                
                                if table_id and table_name and analysis_result:
                                    self.db_obj.insert_schema_analysis_result(
                                        sql_id=sql_id,
                                        table_id=table_id,
                                        table_name=table_name,
                                        analysis_result=analysis_result,
                                        total_tables=len(all_tables_analysis),
                                        test_tables_count=test_tables_count,
                                        standard_naming_count=standard_naming_count
                                    )
                            logger.info(f"âœ… ä¿å­˜ {len(all_tables_analysis)} ä¸ªè¡¨çš„åˆ†æç»“æœåˆ°SQLiteæˆåŠŸ")
                        except Exception as e:
                            logger.warning(f"æ‰¹é‡ä¿å­˜Schemaåˆ†æç»“æœåˆ°SQLiteå¤±è´¥: {e}")
                            traceback.print_exc()
                        
                        logger.info(f"ğŸ“Š å‡†å¤‡ä¿å­˜ {len(all_tables_analysis)} ä¸ªè¡¨çš„åˆ†æç»“æœåˆ°å›¾æ•°æ®åº“")
                        
                        # æ„å»ºå®Œæ•´çš„åˆ†æç»“æœç”¨äºå›¾æ•°æ®åº“ä¿å­˜
                        # æ³¨æ„ï¼šall_tables_analysis ä¸­æ¯ä¸ªå…ƒç´ çš„ç»“æ„ä¸ºï¼š
                        # {
                        #     "table_name": "...",
                        #     "table_id": "...",
                        #     "analysis_result": {
                        #         "entity": {...},
                        #         "attributes": [...],
                        #         "unique_identifiers": [...],
                        #         "foreign_keys": [...],
                        #         ...
                        #     }
                        # }
                        # è¿™ä¸ªç»“æ„å®Œå…¨åŒ¹é… save_schema_analysis_graph_data æ–¹æ³•æœŸæœ›çš„æ ¼å¼
                        filtered_schema_result = {
                            "success": True,
                            "sql_id": sql_id,
                            "tables_analysis": all_tables_analysis,  # å·²ç­›é€‰ï¼ŒåªåŒ…å«æ ¸å¿ƒä¸šåŠ¡è¡¨
                            "total_tables": len(all_tables_analysis),
                            "test_tables_count": 0,  # å·²è¿‡æ»¤ï¼Œä¸åŒ…å«æµ‹è¯•è¡¨
                            "standard_naming_count": len(all_tables_analysis)  # å·²è¿‡æ»¤ï¼ŒåªåŒ…å«è§„èŒƒå‘½åçš„è¡¨
                        }
                        
                        # self.graph_obj.save_schema_analysis_graph_data(
                        #     filtered_schema_result, 
                        #     sql_id, 
                        #     permission_level="public"
                        # )
                        logger.info(f"âœ… ä¿å­˜ {len(all_tables_analysis)} ä¸ªè¡¨çš„åˆ†æç»“æœåˆ°å›¾æ•°æ®åº“æˆåŠŸ")
                    except Exception as e:
                        logger.error(f"ä¿å­˜ Schema åˆ†æå›¾æ•°æ®å¼‚å¸¸: {e}")
                        traceback.print_exc()
                        # ç»§ç»­æ‰§è¡Œï¼Œä¸å½±å“åç»­æµç¨‹
                else:
                    logger.warning(f"âš ï¸ æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•è¡¨åˆ†æç»“æœï¼Œè·³è¿‡å›¾æ•°æ®åº“ä¿å­˜")
                
                conn.close()
                return {"success": True, "message": "æ•°æ®åº“ä¿¡æ¯æ·»åŠ æˆåŠŸ", "sql_id": sql_id}
                
            except Exception as e:
                if conn:
                    conn.close()
                logger.error(f"æ’å…¥æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
                return {"success": False, "message": f"æ·»åŠ æ•°æ®åº“å¤±è´¥: {str(e)}"}
                
        except Exception as e:
            logger.error(f"æ’å…¥æ•°æ®åº“ä¿¡æ¯å¼‚å¸¸: {e}")
            return {"success": False, "message": f"æ·»åŠ æ•°æ®åº“å¤±è´¥: {str(e)}"}
    
    def get_table_info(self, param):
        """è·å–è¡¨ä¿¡æ¯ - ç”Ÿæˆåˆ—æ•°æ®"""
        try:
            sql_id = param.get("sql_id")
            ip = param.get("ip")
            port = param.get("port")
            sql_type = param.get("sql_type")
            sql_name = param.get("sql_name")
            user_id = param.get("user_id")
            
            if not sql_id:
                # å¦‚æœæ²¡æœ‰sql_idï¼Œå°è¯•æ ¹æ®å…¶ä»–å‚æ•°æŸ¥æ‰¾
                base_sql_list = self.db_obj.query_base_sql_by_user_id(user_id)
                matched = None
                for db in base_sql_list:
                    if db["ip"] == ip and db["port"] == port and db["sql_type"] == sql_type and db["sql_name"] == sql_name:
                        matched = db
                        break
                if not matched:
                    return {"success": False, "message": "æœªæ‰¾åˆ°å¯¹åº”çš„æ•°æ®åº“ä¿¡æ¯"}
                sql_id = matched["sql_id"]
            
            # æŸ¥è¯¢è¡¨ä¿¡æ¯
            tables = self.db_obj.query_table_sql_by_sql_id(sql_id)
            
            # ç»„è£…è¡¨åˆ—è¡¨å’Œåˆ—ä¿¡æ¯
            table_list = []
            for table in tables:
                table_id = table["table_id"]
                # æŸ¥è¯¢åˆ—ä¿¡æ¯
                columns = self.db_obj.query_col_sql_by_table_id(table_id)
                
                # è½¬æ¢ä¸ºColumnInfoæ ¼å¼
                column_info_list = []
                for col in columns:
                    col_info = {
                        "col_id": col.get("col_id"),
                        "table_id": col.get("table_id"),
                        "col_name": col.get("col_name"),
                        "col_type": col.get("col_type"),
                        "col_info": col.get("col_info")
                    }
                    column_info_list.append(col_info)
                
                table_info = {
                    "table_id": table["table_id"],
                    "sql_id": table["sql_id"],
                    "table_name": table["table_name"],
                    "table_description": table.get("table_description", ""),
                    "columns": column_info_list
                }
                table_list.append(table_info)
            
            # æŸ¥è¯¢å…³è”å…³ç³»
            relations = self.db_obj.query_rel_sql_by_sql_id(sql_id)
            sql_des_list = self.db_obj.query_sql_des_by_sql_id(sql_id)
            sql_list = []
            for sql_des in sql_des_list:
                tmp_d = {}
                tmp_d["des"] = sql_des.get("sql_des", "")
                tmp_d["sql"] = sql_des.get("sql", "")
                sql_list.append(tmp_d)
            
            return {
                "success": True,
                "message": "è·å–è¡¨ä¿¡æ¯æˆåŠŸ",
                "table_list": table_list,
                "relations": relations,
                "sql_list": sql_list
            }
            
        except Exception as e:
            logger.error(f"è·å–è¡¨ä¿¡æ¯å¤±è´¥: {e}")
            return {"success": False, "message": f"è·å–è¡¨ä¿¡æ¯å¤±è´¥: {str(e)}"}
        
    def delete_sql_rel(self, param):
        """åˆ é™¤æ•°æ®åº“å…³è”å…³ç³»"""
        try:
            rel_id = param.get("rel_id")
            
            if not rel_id:
                return {"success": False, "message": "ç¼ºå°‘å¿…è¦å‚æ•°"}
            
            # åˆ é™¤å…³è”å…³ç³»
            if self.db_obj.delete_rel_sql_by_rel_id(rel_id):
                return {"success": True, "message": "åˆ é™¤å…³è”å…³ç³»æˆåŠŸ"}
            else:
                return {"success": False, "message": "å…³è”å…³ç³»ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥"}
            
        except Exception as e:
            logger.error(f"åˆ é™¤å…³è”å…³ç³»å¤±è´¥: {e}")
            return {"success": False, "message": f"åˆ é™¤å…³è”å…³ç³»å¤±è´¥: {str(e)}"}
            
    def insert_sql_rel(self, param):
        """æ’å…¥æ•°æ®åº“å…³è”å…³ç³»"""
        try:
            sql_id = param.get("sql_id")
            relations = param.get("relations", [])
            
            if not sql_id:
                return {"success": False, "message": "ç¼ºå°‘å¿…è¦å‚æ•°"}
            
            # æ’å…¥æ–°çš„å…³è”å…³ç³»
            for relation in relations:
                rel_id = self.generate_id("rel")
                rel_param = {
                    "rel_id": rel_id,
                    "sql_id": sql_id,
                    "from_table": relation.get("from_table"),
                    "from_col": relation.get("from_col"),
                    "to_table": relation.get("to_table"),
                    "to_col": relation.get("to_col")
                }
                self.db_obj.insert_rel_sql(rel_param)
            
            return {"success": True, "message": "æ’å…¥å…³è”å…³ç³»æˆåŠŸ"}
            
        except Exception as e:
            logger.error(f"æ’å…¥å…³è”å…³ç³»å¤±è´¥: {e}")
            return {"success": False, "message": f"æ’å…¥å…³è”å…³ç³»å¤±è´¥: {str(e)}"}
    
    def delete_sql_info(self, param):
        """åˆ é™¤æ•°æ®åº“æ•°æ®"""
        sql_id = param.get("sql_id")
        
        #self.graph_obj.delete_sql_graph_data(sql_id)
        #self.graph_obj.delete_all_graph()
        #self.vector_agent.delete_vector_store_by_sql_id(sql_id)
        
        self.elasticsearch_obj.delete_knowledge_elasticsearch_data(knowledge_id=sql_id)
        
        try:
            success = cSingleSqlite.delete_base_sql(sql_id)
            if success:
                return {"success": True, "message": "åˆ é™¤æ•°æ®åº“ä¿¡æ¯æˆåŠŸ"}
            else:
                return {"success": False, "message": "åˆ é™¤æ•°æ®åº“ä¿¡æ¯å¤±è´¥"}
        except Exception as e:
            logger.error(f"åˆ é™¤æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
            return {"success": False, "message": f"åˆ é™¤æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {str(e)}"}
        return {"success": False, "message": "åˆ é™¤æ•°æ®åº“ä¿¡æ¯å¤±è´¥"}
    
    def update_sql_info(self, param):
        """æ›´æ–°æ•°æ®åº“ä¿¡æ¯ - ä¿å­˜åˆ—å…³è”æ•°æ®"""
        try:
            success = self.delete_sql_info(param)
            if(success):
                self.insert_sql_info(param)
                return {"success": True, "message": "æ›´æ–°æˆåŠŸ"}
            else:
                return {"success": False, "message": "æ›´æ–°å¤±è´¥"}
        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
            return {"success": False, "message": f"æ›´æ–°æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {str(e)}"}

    # def build_schema_vector_store(self, param):
    #     """æ„å»ºæ•°æ®åº“æ¨¡å¼å‘é‡åº“"""
    #     try:
    #         user_id = param.get("user_id")
    #         if not user_id:
    #             return {"success": False, "message": "ç¼ºå°‘ç”¨æˆ·ID"}
    #
    #         # è°ƒç”¨å‘é‡ä»£ç†æ„å»ºå‘é‡åº“
    #         agent = SqlSchemaVectorAgent()
    #         result = agent.build_or_update_store(user_id)
    #
    #         return result
    #     except Exception as e:
    #         logger.error(f"æ„å»ºå‘é‡åº“å¤±è´¥: {e}")
    #         return {"success": False, "message": f"æ„å»ºå‘é‡åº“å¤±è´¥: {str(e)}"}


class SchemaGraphBuilder:
    """æ•°æ®åº“æ¨¡å¼å…³è”å›¾æ„å»ºå™¨"""

    def __init__(self):
        try:
            self.nx = nx
            # NETWORKX_AVAILABLE = True
        except ImportError:
            raise ImportError("NetworkXæœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install networkx")

    def build_graph(self, sql_id: str, filtered_tables: List[str] = None) -> Any:
        """æ„å»ºæ•°æ®åº“æ¨¡å¼çš„å…³è”å›¾"""
        try:
            # åˆ›å»ºå¤šé‡æœ‰å‘å›¾
            G = self.nx.MultiDiGraph()

            # è·å–è¡¨ä¿¡æ¯
            tables = self.db_obj.query_table_sql_by_sql_id(sql_id)

            # å¦‚æœæŒ‡å®šäº†è¿‡æ»¤è¡¨ï¼Œåˆ™åªå¤„ç†è¿™äº›è¡¨
            if filtered_tables:
                table_name_set = set(filtered_tables)
                tables = [t for t in tables if t["table_name"] in table_name_set]

            # æ·»åŠ è¡¨èŠ‚ç‚¹
            for table in tables:
                table_name = table["table_name"]
                G.add_node(table_name,
                          node_type="table",
                          table_id=table["table_id"],
                          description=table.get("table_description", ""),
                          sql_id=sql_id)

                # è·å–åˆ—ä¿¡æ¯å¹¶æ·»åŠ åˆ°èŠ‚ç‚¹å±æ€§
                columns = self.db_obj.query_col_sql_by_table_id(table["table_id"])
                col_info = {}
                for col in columns:
                    col_info[col["col_name"]] = {
                        "col_id": col["col_id"],
                        "col_type": col.get("col_type", ""),
                        "col_info": col.get("col_info", {})
                    }
                G.nodes[table_name]["columns"] = col_info

            # è·å–å…³è”å…³ç³»å¹¶æ·»åŠ è¾¹
            relations = self.db_obj.query_rel_sql_by_sql_id(sql_id)

            for rel in relations:
                from_table = rel["from_table"]
                to_table = rel["to_table"]
                from_col = rel["from_col"]
                to_col = rel["to_col"]

                # åªæ·»åŠ åœ¨è¿‡æ»¤è¡¨èŒƒå›´å†…çš„å…³è”
                if filtered_tables:
                    if from_table not in table_name_set or to_table not in table_name_set:
                        continue

                # æ·»åŠ æœ‰å‘è¾¹ï¼ˆå¤–é”®å…³ç³»ï¼‰
                G.add_edge(from_table, to_table,
                          from_col=from_col,
                          to_col=to_col,
                          relation_type="foreign_key",
                          weight=1.0)  # åŸºç¡€æƒé‡

                # åŒæ—¶æ·»åŠ åå‘è¾¹ï¼ˆç”¨äºæŸ¥è¯¢è·¯å¾„ï¼‰
                G.add_edge(to_table, from_table,
                          from_col=to_col,
                          to_col=from_col,
                          relation_type="reverse_foreign_key",
                          weight=2.0)  # åå‘æŸ¥è¯¢æƒé‡ç¨é«˜

            logger.info(f"æ„å»ºå…³è”å›¾å®Œæˆ: {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹")
            return G

        except Exception as e:
            logger.error(f"æ„å»ºå…³è”å›¾å¤±è´¥: {e}")
            raise

    def find_optimal_paths(self, graph: Any, query_entities: List[str],
                          max_depth: int = 3) -> Dict[str, Any]:
        """å¯»æ‰¾æœ€ä¼˜çš„å…³è”è·¯å¾„"""
        try:
            if not query_entities:
                return {"paths": [], "tables": set(), "columns": set()}

            # æ‰¾åˆ°å›¾ä¸­å­˜åœ¨çš„æŸ¥è¯¢å®ä½“ï¼ˆè¡¨åï¼‰
            available_entities = []
            for entity in query_entities:
                if entity in graph.nodes:
                    available_entities.append(entity)
                else:
                    # å°è¯•æ¨¡ç³ŠåŒ¹é…
                    matches = [n for n in graph.nodes if entity.lower() in n.lower()]
                    available_entities.extend(matches[:1])  # æœ€å¤šåŒ¹é…ä¸€ä¸ª

            if not available_entities:
                return {"paths": [], "tables": set(), "columns": set()}

            all_paths = []
            visited_tables = set()
            visited_columns = set()

            # ä»æ¯ä¸ªæŸ¥è¯¢å®ä½“å‡ºå‘ï¼Œå¯»æ‰¾å…³è”è·¯å¾„
            for start_entity in available_entities:
                # BFSæœç´¢å…³è”è·¯å¾„
                paths_from_start = self._bfs_search(graph, start_entity, max_depth)

                for path_info in paths_from_start:
                    all_paths.append(path_info)

                    # æ”¶é›†æ¶‰åŠçš„è¡¨å’Œåˆ—
                    for node in path_info["path"]:
                        visited_tables.add(node)

                        # è·å–è¯¥è¡¨çš„æ‰€æœ‰åˆ—
                        if node in graph.nodes:
                            columns = graph.nodes[node].get("columns", {})
                            visited_columns.update(columns.keys())

            # æŒ‰è·¯å¾„é•¿åº¦å’Œæƒé‡æ’åº
            all_paths.sort(key=lambda x: (len(x["path"]), x["total_weight"]))

            # å»é‡å¹¶ä¿ç•™æœ€ä¼˜è·¯å¾„
            unique_paths = self._deduplicate_paths(all_paths)

            return {
                "paths": unique_paths[:10],  # æœ€å¤šè¿”å›10æ¡æœ€ä¼˜è·¯å¾„
                "tables": list(visited_tables),
                "columns": list(visited_columns)
            }

        except Exception as e:
            logger.error(f"å¯»æ‰¾æœ€ä¼˜è·¯å¾„å¤±è´¥: {e}")
            raise

    def _bfs_search(self, graph: Any, start_node: str, max_depth: int) -> List[Dict[str, Any]]:
        """BFSæœç´¢ä»èµ·å§‹èŠ‚ç‚¹å‡ºå‘çš„è·¯å¾„"""
        from collections import deque

        paths = []
        visited = set()
        queue = deque([(start_node, [start_node], 0.0, [])])  # (å½“å‰èŠ‚ç‚¹, è·¯å¾„, æ€»æƒé‡, è¾¹ä¿¡æ¯)

        while queue:
            current_node, path, total_weight, edge_info = queue.popleft()

            if len(path) > max_depth + 1:  # è·¯å¾„é•¿åº¦è¶…è¿‡æœ€å¤§æ·±åº¦
                continue

            # è®°å½•è·¯å¾„
            if len(path) > 1:  # è‡³å°‘åŒ…å«ä¸¤ä¸ªèŠ‚ç‚¹
                paths.append({
                    "path": path.copy(),
                    "total_weight": total_weight,
                    "edges": edge_info.copy(),
                    "start_table": start_node,
                    "end_table": current_node
                })

            # é¿å…é‡å¤è®¿é—®
            if current_node in visited and len(path) > 2:
                continue
            visited.add(current_node)

            # æ¢ç´¢é‚»å±…èŠ‚ç‚¹
            for neighbor in graph.neighbors(current_node):
                if neighbor not in path:  # é¿å…ç¯
                    # è·å–è¾¹ä¿¡æ¯
                    edges = graph.get_edge_data(current_node, neighbor)
                    if edges:
                        for edge_key, edge_data in edges.items():
                            new_path = path + [neighbor]
                            new_weight = total_weight + edge_data.get("weight", 1.0)
                            new_edge_info = edge_info + [{
                                "from_table": current_node,
                                "to_table": neighbor,
                                "from_col": edge_data.get("from_col", ""),
                                "to_col": edge_data.get("to_col", ""),
                                "relation_type": edge_data.get("relation_type", "")
                            }]

                            queue.append((neighbor, new_path, new_weight, new_edge_info))

        return paths

    def _deduplicate_paths(self, paths: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡è·¯å¾„ï¼Œä¿ç•™æƒé‡æœ€å°çš„"""
        path_dict = {}

        for path_info in paths:
            path_tuple = tuple(path_info["path"])
            weight = path_info["total_weight"]

            if path_tuple not in path_dict or weight < path_dict[path_tuple]["total_weight"]:
                path_dict[path_tuple] = path_info

        return list(path_dict.values())

    def get_related_tables_and_columns(self, sql_id: str, related_tables: List[str]) -> Dict[str, Any]:
        """è·å–å…³è”çš„è¡¨å’Œåˆ—ä¿¡æ¯"""
        try:
            result = {
                "tables": [],
                "columns": [],
                "relations": []
            }

            # è·å–è¡¨ä¿¡æ¯
            for table_name in related_tables:
                table_info = self.db_obj.query_table_sql_by_sql_id_and_name(sql_id, table_name)
                if table_info:
                    result["tables"].append(table_info)

                    # è·å–åˆ—ä¿¡æ¯
                    columns = self.db_obj.query_col_sql_by_table_id(table_info["table_id"])
                    result["columns"].extend(columns)

            # è·å–å…³è”å…³ç³»
            relations = self.db_obj.query_rel_sql_by_sql_id(sql_id)

            # è¿‡æ»¤åªåŒ…å«ç›¸å…³è¡¨çš„å…³è”å…³ç³»
            related_table_set = set(related_tables)
            filtered_relations = [
                rel for rel in relations
                if rel["from_table"] in related_table_set and rel["to_table"] in related_table_set
            ]
            result["relations"] = filtered_relations

            return result

        except Exception as e:
            logger.error(f"è·å–å…³è”è¡¨å’Œåˆ—ä¿¡æ¯å¤±è´¥: {e}")
            raise


# æ·»åŠ åˆ°CControlç±»ä¸­ä½œä¸ºæ–¹æ³•
def build_schema_graph(self, param):
    """æ„å»ºæ•°æ®åº“æ¨¡å¼å…³è”å›¾"""
    try:
        sql_id = param.get("sql_id")
        filtered_tables = param.get("filtered_tables", [])

        if not sql_id:
            return {"success": False, "message": "ç¼ºå°‘sql_id"}

        graph_builder = SchemaGraphBuilder()
        graph = graph_builder.build_graph(sql_id, filtered_tables if filtered_tables else None)

        return {
            "success": True,
            "message": "å…³è”å›¾æ„å»ºæˆåŠŸ",
            "graph_info": {
                "nodes": graph.number_of_nodes(),
                "edges": graph.number_of_edges(),
                "tables": list(graph.nodes())
            }
        }

    except Exception as e:
        logger.error(f"æ„å»ºå…³è”å›¾å¤±è´¥: {e}")
        return {"success": False, "message": f"æ„å»ºå…³è”å›¾å¤±è´¥: {str(e)}"}

def find_schema_paths(self, param):
    """å¯»æ‰¾æ•°æ®åº“æ¨¡å¼çš„æœ€ä¼˜å…³è”è·¯å¾„"""
    try:
        sql_id = param.get("sql_id")
        query_entities = param.get("query_entities", [])
        max_depth = param.get("max_depth", 3)

        if not sql_id:
            return {"success": False, "message": "ç¼ºå°‘sql_id"}

        # å…ˆæ„å»ºå®Œæ•´å›¾
        graph_builder = SchemaGraphBuilder()
        graph = graph_builder.build_graph(sql_id)

        # å¯»æ‰¾æœ€ä¼˜è·¯å¾„
        path_result = graph_builder.find_optimal_paths(graph, query_entities, max_depth)

        # è·å–è¯¦ç»†çš„è¡¨å’Œåˆ—ä¿¡æ¯
        if path_result["tables"]:
            detail_info = graph_builder.get_related_tables_and_columns(sql_id, path_result["tables"])
            path_result["detail_info"] = detail_info

        return {
            "success": True,
            "message": "è·¯å¾„æœç´¢æˆåŠŸ",
            "result": path_result
        }

    except Exception as e:
        logger.error(f"è·¯å¾„æœç´¢å¤±è´¥: {e}")
        return {"success": False, "message": f"è·¯å¾„æœç´¢å¤±è´¥: {str(e)}"}

# å°†æ–¹æ³•æ·»åŠ åˆ°CControlç±»
CControl.build_schema_graph = build_schema_graph
CControl.find_schema_paths = find_schema_paths

def generate_table_ddl(self, conn, db_type: str, table_name: str) -> str:
    """
    ç”Ÿæˆè¡¨çš„DDLè¯­å¥

    Args:
        conn: æ•°æ®åº“è¿æ¥
        db_type: æ•°æ®åº“ç±»å‹
        table_name: è¡¨å

    Returns:
        DDLè¯­å¥å­—ç¬¦ä¸²
    """
    try:
        if db_type.lower() == "mysql":
            # MySQL DDLç”Ÿæˆ
            cursor = conn.cursor()
            cursor.execute(f"SHOW CREATE TABLE `{table_name}`")
            result = cursor.fetchone()
            if result:
                ddl = result[1]  # SHOW CREATE TABLE è¿”å› (è¡¨å, DDL)
                return ddl
        elif db_type.lower() == "postgresql":
            # PostgreSQL DDLç”Ÿæˆ
            cursor = conn.cursor(cursor_factory=RealDictCursor)
            # è·å–è¡¨ç»“æ„ä¿¡æ¯
            cursor.execute("""
                SELECT
                    column_name,
                    data_type,
                    is_nullable,
                    column_default,
                    character_maximum_length,
                    numeric_precision,
                    numeric_scale
                FROM information_schema.columns
                WHERE table_name = %s
                ORDER BY ordinal_position
            """, (table_name,))

            columns = cursor.fetchall()

            if not columns:
                return ""

            # æ„å»ºCREATE TABLEè¯­å¥
            ddl_parts = [f"CREATE TABLE {table_name} ("]

            col_defs = []
            for col in columns:
                col_def = f"{col['column_name']} {col['data_type']}"

                # å¤„ç†é•¿åº¦
                if col['character_maximum_length']:
                    col_def += f"({col['character_maximum_length']})"
                elif col['numeric_precision'] and col['data_type'].upper() in ('DECIMAL', 'NUMERIC'):
                    if col['numeric_scale']:
                        col_def += f"({col['numeric_precision']},{col['numeric_scale']})"
                    else:
                        col_def += f"({col['numeric_precision']})"

                # å¤„ç†NOT NULL
                if col['is_nullable'] == 'NO':
                    col_def += " NOT NULL"

                # å¤„ç†é»˜è®¤å€¼
                if col['column_default']:
                    col_def += f" DEFAULT {col['column_default']}"

                col_defs.append(col_def)

            ddl_parts.append(",\n    ".join(col_defs))
            ddl_parts.append(");")

            return "".join(ddl_parts)

        logger.warning(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}")
        return ""

    except Exception as e:
        logger.error(f"ç”Ÿæˆè¡¨ {table_name} DDLå¤±è´¥: {e}")
        return ""

def generate_table_ddl_from_sqlite(self, sql_id: str, table_name: str, db_type: str) -> str:
    """
    ä»SQLiteä¸­è¯»å–è¡¨å’Œåˆ—ä¿¡æ¯ç”ŸæˆDDLè¯­å¥

    Args:
        sql_id: æ•°æ®åº“ID
        table_name: è¡¨å
        db_type: æ•°æ®åº“ç±»å‹ ('mysql' æˆ– 'postgresql')

    Returns:
        DDLè¯­å¥å­—ç¬¦ä¸²
    """
    try:
        # è·å–è¡¨ä¿¡æ¯
        table_info = self.db_obj.query_table_sql_by_sql_id_and_name(sql_id, table_name)
        if not table_info:
            logger.warning(f"æœªæ‰¾åˆ°è¡¨ {table_name} çš„ä¿¡æ¯")
            return ""

        table_id = table_info["table_id"]
        table_desc = table_info.get("table_description", "")

        # è·å–åˆ—ä¿¡æ¯
        columns = self.db_obj.query_col_sql_by_table_id(table_id)
        if not columns:
            logger.warning(f"è¡¨ {table_name} æ²¡æœ‰åˆ—ä¿¡æ¯")
            return ""

        # è½¬æ¢åˆ—ä¿¡æ¯æ ¼å¼ä»¥åŒ¹é…control_chat.pyçš„æ ¼å¼
        formatted_columns = []
        for col in columns:
            col_info = col.get("col_info", {})
            if isinstance(col_info, str):
                try:
                    col_info = json.loads(col_info)
                except:
                    col_info = {}

            formatted_col = {
                "col_name": col.get("col_name", ""),
                "col_type": col_info.get("col_type", ""),
                "is_nullable": col_info.get("is_nullable", "YES") if db_type.lower() == "postgresql" else ("NO" if col_info.get("is_null", "").upper() == "NO" else "YES"),
                "column_default": col_info.get("column_default") if db_type.lower() == "postgresql" else col_info.get("default"),
                "comment": col_info.get("comment")
            }
            formatted_columns.append(formatted_col)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„è¡¨æ•°æ®ç»“æ„
        table_data = {
            "table_name": table_name,
            "table_description": table_desc,
            "columns": formatted_columns
        }

        # ä½¿ç”¨ç°æœ‰çš„æ ¼å¼åŒ–æ–¹æ³•
        if db_type.lower() == "mysql":
            return self._format_table_structure_mysql([table_data])
        elif db_type.lower() == "postgresql":
            return self._format_table_structure_postgresql([table_data])
        else:
            logger.warning(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}")
            return ""

    except Exception as e:
        logger.error(f"ä»SQLiteç”Ÿæˆè¡¨ {table_name} DDLå¤±è´¥: {e}")
        return ""

def _format_table_structure_mysql(self, tables):
    """å°†è¡¨æ ¼æ•°æ®è½¬æˆmysqlçš„ddlæ ¼å¼"""
    try:
        if not tables:
            return "æš‚æ— è¡¨ç»“æ„ä¿¡æ¯"

        ddl_statements = []

        for table in tables:
            table_name = table.get("table_name", "")
            table_desc = table.get("table_description", "")
            columns = table.get("columns", [])

            if not table_name:
                continue

            # æ„å»ºCREATE TABLEè¯­å¥
            ddl_lines = []
            ddl_lines.append(f"CREATE TABLE `{table_name}` (")

            # æ·»åŠ åˆ—å®šä¹‰
            column_defs = []
            for col in columns:
                col_name = col.get("col_name", "")
                col_type = col.get("col_type", "")
                is_nullable = col.get("is_nullable", "YES")
                column_default = col.get("column_default")
                comment = col.get("comment")

                if not col_name or not col_type:
                    continue

                # æ˜ å°„PostgreSQLç±»å‹åˆ°MySQLç±»å‹
                mysql_type = self._map_postgres_to_mysql_type(col_type)

                # æ„å»ºåˆ—å®šä¹‰
                col_def = f"`{col_name}` {mysql_type}"

                # æ·»åŠ NULL/NOT NULLçº¦æŸ
                if is_nullable.upper() == "NO":
                    col_def += " NOT NULL"
                else:
                    col_def += " NULL"

                # æ·»åŠ é»˜è®¤å€¼
                if column_default is not None:
                    if isinstance(column_default, str):
                        col_def += f" DEFAULT '{column_default}'"
                    else:
                        col_def += f" DEFAULT {column_default}"

                column_defs.append(col_def)

            # ç»„åˆåˆ—å®šä¹‰
            ddl_lines.append(",\n".join(f"  {col_def}" for col_def in column_defs))
            ddl_lines.append(") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;")

            # æ·»åŠ è¡¨æ³¨é‡Š
            if table_desc:
                ddl_lines.append(f"")
                ddl_lines.append(f"ALTER TABLE `{table_name}` COMMENT = '{table_desc}';")

            # æ·»åŠ åˆ—æ³¨é‡Š
            for col in columns:
                col_name = col.get("col_name", "")
                comment = col.get("comment")
                if comment:
                    ddl_lines.append(f"ALTER TABLE `{table_name}` MODIFY COLUMN `{col_name}` {self._map_postgres_to_mysql_type(col.get('col_type', ''))} COMMENT '{comment}';")

            ddl_statements.append("\n".join(ddl_lines))

        return "\n\n".join(ddl_statements)

    except Exception as e:
        logger.error(f"MySQL DDLæ ¼å¼åŒ–å¤±è´¥: {e}")
        return "MySQL DDLæ ¼å¼åŒ–å¤±è´¥"

def _format_table_structure_postgresql(self, tables):
    """å°†è¡¨æ ¼æ•°æ®è½¬æˆpostgresqlçš„ddlæ ¼å¼"""
    try:
        if not tables:
            return "æš‚æ— è¡¨ç»“æ„ä¿¡æ¯"

        ddl_statements = []

        for table in tables:
            table_name = table.get("table_name", "")
            table_desc = table.get("table_description", "")
            columns = table.get("columns", [])

            if not table_name:
                continue

            # æ„å»ºCREATE TABLEè¯­å¥
            ddl_lines = []
            ddl_lines.append(f'CREATE TABLE "{table_name}" (')

            # æ·»åŠ åˆ—å®šä¹‰
            column_defs = []
            for col in columns:
                col_name = col.get("col_name", "")
                col_type = col.get("col_type", "")
                is_nullable = col.get("is_nullable", "YES")
                column_default = col.get("column_default")
                comment = col.get("comment")

                if not col_name or not col_type:
                    continue

                # æ„å»ºåˆ—å®šä¹‰
                col_def = f'"{col_name}" {col_type}'

                # æ·»åŠ NULL/NOT NULLçº¦æŸ
                if is_nullable.upper() == "NO":
                    col_def += " NOT NULL"
                else:
                    col_def += " NULL"

                # æ·»åŠ é»˜è®¤å€¼
                if column_default is not None:
                    if isinstance(column_default, str):
                        col_def += f" DEFAULT '{column_default}'"
                    else:
                        col_def += f" DEFAULT {column_default}"

                column_defs.append(col_def)

            # ç»„åˆåˆ—å®šä¹‰
            ddl_lines.append(",\n".join(f"  {col_def}" for col_def in column_defs))
            ddl_lines.append(");")

            # æ·»åŠ è¡¨æ³¨é‡Š
            if table_desc:
                ddl_lines.append(f"")
                ddl_lines.append(f"COMMENT ON TABLE \"{table_name}\" IS '{table_desc}';")

            # æ·»åŠ åˆ—æ³¨é‡Š
            for col in columns:
                col_name = col.get("col_name", "")
                comment = col.get("comment")
                if comment:
                    ddl_lines.append(f"COMMENT ON COLUMN \"{table_name}\".\"{col_name}\" IS '{comment}';")

            ddl_statements.append("\n".join(ddl_lines))

        return "\n\n".join(ddl_statements)

    except Exception as e:
        logger.error(f"PostgreSQL DDLæ ¼å¼åŒ–å¤±è´¥: {e}")
        return "PostgreSQL DDLæ ¼å¼åŒ–å¤±è´¥"

def _map_postgres_to_mysql_type(self, postgres_type):
    """å°†PostgreSQLæ•°æ®ç±»å‹æ˜ å°„åˆ°MySQLæ•°æ®ç±»å‹"""
    if not postgres_type:
        return "VARCHAR(255)"

    type_mapping = {
        "character varying": "VARCHAR(255)",
        "varchar": "VARCHAR(255)",
        "text": "TEXT",
        "integer": "INT",
        "int": "INT",
        "bigint": "BIGINT",
        "smallint": "SMALLINT",
        "numeric": "DECIMAL(10,2)",
        "decimal": "DECIMAL(10,2)",
        "real": "FLOAT",
        "double precision": "DOUBLE",
        "boolean": "TINYINT(1)",
        "timestamp": "TIMESTAMP",
        "timestamp without time zone": "TIMESTAMP",
        "timestamp with time zone": "TIMESTAMP",
        "date": "DATE",
        "time": "TIME",
        "json": "JSON",
        "jsonb": "JSON",
    }

    # å¤„ç†å¸¦é•¿åº¦é™åˆ¶çš„ç±»å‹
    if postgres_type.startswith("character varying(") or postgres_type.startswith("varchar("):
        return postgres_type.replace("character varying", "VARCHAR").replace("varchar", "VARCHAR")
    elif postgres_type.startswith("numeric(") or postgres_type.startswith("decimal("):
        return postgres_type.replace("numeric", "DECIMAL").replace("decimal", "DECIMAL")

    return type_mapping.get(postgres_type.lower(), postgres_type.upper())

# æ·»åŠ åˆ°CControlç±»
CControl.generate_table_ddl = generate_table_ddl
CControl.generate_table_ddl_from_sqlite = generate_table_ddl_from_sqlite
CControl._format_table_structure_mysql = _format_table_structure_mysql
CControl._format_table_structure_postgresql = _format_table_structure_postgresql
CControl._map_postgres_to_mysql_type = _map_postgres_to_mysql_type


def _filter_schema_analysis_result(schema_analysis_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç­›é€‰Schemaåˆ†æç»“æœï¼šæ’é™¤æµ‹è¯•è¡¨å’Œå‘½åä¸è§„èŒƒçš„è¡¨
    
    Args:
        schema_analysis_result: åŸå§‹Schemaåˆ†æç»“æœ
        
    Returns:
        Dict[str, Any]: ç­›é€‰åçš„Schemaåˆ†æç»“æœ
    """
    try:
        if not schema_analysis_result.get("success"):
            return schema_analysis_result
        
        tables_analysis = schema_analysis_result.get("tables_analysis", [])
        filtered_tables = []
        filtered_test_count = 0
        filtered_non_standard_count = 0
        
        for table_analysis in tables_analysis:
            analysis_result = table_analysis.get("analysis_result", {})
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•è¡¨
            is_test_table = analysis_result.get("is_test_table", False)
            if is_test_table:
                filtered_test_count += 1
                continue
            
            # æ£€æŸ¥å‘½åè§„èŒƒ
            naming_standard = analysis_result.get("naming_standard", "")
            if naming_standard == "non_standard":
                filtered_non_standard_count += 1
                continue
            
            # ä¿ç•™è¯¥è¡¨
            filtered_tables.append(table_analysis)
        
        # æ„å»ºç­›é€‰åçš„ç»“æœ
        filtered_result = {
            "success": schema_analysis_result.get("success", True),
            "sql_id": schema_analysis_result.get("sql_id", ""),
            "tables_analysis": filtered_tables,
            "total_tables": len(filtered_tables),
            "test_tables_count": schema_analysis_result.get("test_tables_count", 0),
            "standard_naming_count": len(filtered_tables),  # æ›´æ–°ä¸ºæ ‡å‡†å‘½åè¡¨æ•°é‡
            "filtered_test_tables": filtered_test_count,
            "filtered_non_standard_tables": filtered_non_standard_count
        }
        
        logger.info(f"Schemaåˆ†æç»“æœç­›é€‰: åŸå§‹ {len(tables_analysis)} ä¸ªè¡¨, "
                   f"ç­›é€‰å {len(filtered_tables)} ä¸ªè¡¨ "
                   f"(æ’é™¤ {filtered_test_count} ä¸ªæµ‹è¯•è¡¨, {filtered_non_standard_count} ä¸ªå‘½åä¸è§„èŒƒçš„è¡¨)")
        
        return filtered_result
        
    except Exception as e:
        logger.error(f"ç­›é€‰Schemaåˆ†æç»“æœå¤±è´¥: {e}")
        return schema_analysis_result  # å¦‚æœç­›é€‰å¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ

