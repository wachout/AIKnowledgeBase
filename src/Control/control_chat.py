# -*- coding:utf-8 -*-

import os
import re
import json
import csv
import time
import logging
import copy
import uuid
import threading
import queue
import traceback
from typing import Dict, Any
from datetime import datetime

from bs4 import BeautifulSoup

from langchain_community.document_loaders import DirectoryLoader
from langchain_community.document_loaders import UnstructuredFileLoader

# from langchain_text_splitters import RecursiveCharacterTextSplitter

# from langchain_core.documents import Document

# from pbox import CodeSandBox  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œé¿å…socketé”™è¯¯

from Control.control_milvus import CControl as ControlMilvus
from Control.control_file import CControl as CFileControl
from Control.control_graph import CControl as ControlGraph
from Control.control_search import CControl as ControlSearch
# from Control.control_graphiti import CControl as ControlGraphiti
# from Graphrag.light_rag import run as graph_run

# from Agent import article_theme_run
from Agent import entity_relation_split_run
# from Agent import text_to_cypher_run
from Agent import intent_recog_run
from Agent import file_analysis_run
# from Agent import analysis_code_run
# from Agent import report_form_code_run
from Agent import echarts_run
from Agent import agentic_rag_run
from Agent import agentic_sql_run
from Agent import agentic_query_run
from Agent import table_file_run
# from Agent.table_file_run import run_table_analysis_stream
# from Agent import intent_recog_sql_run
from Math.statistics import calculate_statistics
# from Agent.emb_query_run import run_agent
# table_file_run.run_table_analysis_stream
# å¯¼å…¥çŸ¥è¯†åº“æ•°æ®åº“å®ä¾‹
from Db.sqlite_db import cSingleSqlite

from Config.embedding_config import get_embeddings
from Config.neo4j_config import is_neo4j_enabled
from Config.pdf_config import is_pdf_advanced_enabled
from Config.llm_config import get_chat_tongyi

# å¯¼å…¥åœ†æ¡Œè®¨è®ºç³»ç»Ÿ
from Roles import RoundtableDiscussion

from Utils import utils

from Control.control_sessions import CControl as ControlSessions
from Control.control_sql import CControl as ControlSql
from Control.control_discussion import DiscussionControl
# from Sql.vanna_manager import get_vanna_manager
from Sql.Graph.graph import Graph

# from pbox import CodeSandBox
# sandbox = CodeSandBox()

# åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # è®¾ç½®æ—¥å¿—çº§åˆ«
logger_lock = threading.Lock()

def thread_safe_log(level_func, message, *args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—è®°å½•å‡½æ•°"""
    with logger_lock:
        level_func(message, *args, **kwargs)


DOWNFILE = "conf/file"
URL_IMAGES = "http://192.168.35.125:8500/"
WORKING_DIR = "lightrag_data"

IAMGES_PATH = "/home/ubumtu/Downloads/Server/tmp_url"


class CControl():
    
    def __init__(self):
        self.file_obj = CFileControl()
        self.milvus_obj = ControlMilvus()
        self.graph_obj = ControlGraph()
        self.search_obj = ControlSearch()
        self.sess_obj = ControlSessions()
        self.sql_obj = ControlSql()
        self.discussion_obj = DiscussionControl()
    
    def content_list_to_json(self, content_list, file_id):
        _json = json.loads(content_list)
        _content = ""
        for item in _json:
            _type = item["type"]
            if(_type == "text"):
                _text = item["text"]
                _content = _content + _text+ "\n"
            elif(_type == "image"):
                if("img_path" in item.keys()):
                    url_image = URL_IMAGES + item["img_path"]
                else:
                    url_image = ""
                if("img_caption" in item.keys()):
                    img_caption = item["img_caption"]
                else:
                    img_caption = []
                if("img_footnote" in item.keys()):
                    img_footnote = item["img_footnote"]
                else:
                    img_footnote = []
                _tmpt_txt = ""
                if(img_caption):
                    _caption = "caption:\n"
                    for _cap in img_caption:
                        _caption = _caption + _cap + "\n"
                    _tmpt_txt = _tmpt_txt + _caption
                if(url_image):
                    _tmpt_txt = _tmpt_txt + "å›¾ç‰‡åœ°å€ï¼š" + url_image + "\n"
                if(img_footnote):
                    _footnote = "images footnote:\n"
                    for _foot in img_footnote:
                        _footnote = _footnote + _foot + "\n"
                    _tmpt_txt = _tmpt_txt + _footnote
                _content = _content + _tmpt_txt
                if(url_image):
                    caption = "\n".join(img_caption)
                    footnote = "\n".join(img_footnote)
                    upload_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                    if(not file_id):
                        param = {"file_id":file_id, 
                                 "img_path":url_image,
                                 "caption":caption,
                                 "footnote":footnote,
                                 "upload_time":upload_time}
                        cSingleSqlite.insert_image_file(param)
                
            elif(_type == "table"):
                _tmpt_txt = ""
                if("text" in item.keys()):
                    _text = item["text"]
                    _tmpt_txt = _tmpt_txt + _text
                if("img_path" in item.keys()):
                    url_image = URL_IMAGES + item["img_path"]
                if("table_caption" in item.keys()):
                    table_caption = item["table_caption"]
                else:
                    table_caption = []
                if("table_footnote" in item.keys()):
                    table_footnote = item["table_footnote"]
                else:
                    table_footnote = []
                if(table_caption):
                    _caption = "caption:\n"
                    for _cap in table_caption:
                        _caption = _caption + _cap + "\n"
                    _tmpt_txt = _tmpt_txt + _caption
                modified_text = ""
                if("table_body" in item.keys()):
                    table_body = item["table_body"]
                    json_table = self.convert_xml_to_json(table_body)
                    modified_text = json_table["modified_text"]
                    _tmpt_txt = _tmpt_txt + "è¡¨æ ¼çš„Jsonæ ¼å¼ï¼š\n" + modified_text
                    _tmpt_txt = _tmpt_txt + "è¡¨æ ¼ï¼š\n" + table_body + "\n"
                    _tmpt_txt = _tmpt_txt + "è¡¨æ ¼çš„å›¾ç‰‡æ˜¾ç¤ºï¼š\n" + url_image + "\n"
                if(table_footnote):
                    _footnote = "table footnote:\n"
                    for _foot in table_footnote:
                        _footnote = _footnote + _foot + "\n"
                    _tmpt_txt = _tmpt_txt + _footnote
                _content = _content + _tmpt_txt
                
                if(modified_text):
                    caption = "\n".join(table_caption)
                    footnote = "\n".join(table_footnote)
                    upload_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                    if(not file_id):
                        param = {"file_id":file_id, 
                                 "table_data":modified_text,
                                 "caption":caption,
                                 "footnote":footnote,
                                 "upload_time":upload_time}
                        cSingleSqlite.insert_table_data(param)
                    
        return _content
    
    def load_files(self, _file):
        if(os.path.isfile(_file)):
            loader = UnstructuredFileLoader(_file)
            _loader = loader.load()
        else:
            _path = os.path.dirname(_file)
            loader = DirectoryLoader(_file)
            _loader = loader.load()
        return _loader
                    
    def convert_html_table_to_json(self, html):
        """
        Parses an HTML string, extracts the first table, and converts it to a JSON array of objects.
        The first row of the table is assumed to be the header.
        """
        soup = BeautifulSoup(html, 'html.parser')
        table = soup.find('table')
        if not table:
            # Return an empty list instead of a JSON string
            return []
    
        # Find all rows, then get the header from the first row
        rows = table.find_all('tr')
        if not rows:
            return []
        
        headers = [header.text.strip() for header in rows[0].find_all('td')]
        
        data = []
        # Process the rest of the rows
        for row in rows[1:]:
            cells = row.find_all('td')
            if len(cells) == len(headers):  # Ensure row is not malformed
                row_data = {headers[i]: cells[i].text.strip() for i in range(len(headers))}
                data.append(row_data)
        return data
    
    def convert_xml_to_json(self, content):
        all_tables_data = []
        
        def replacer(match):
            """
            This function is called for each <html>...</html> match.
            It converts the table to JSON, adds it to our master list,
            and returns the JSON string for replacement.
            """
            html_block = match.group(0)
            table_data = self.convert_html_table_to_json(html_block)
            if table_data:
                all_tables_data.extend(table_data)
                # Return a compact JSON string for in-place replacement
                return json.dumps(table_data, ensure_ascii=False)
            # If no table is found, return the original block
            return html_block
        
        modified_text = re.sub('<html>.*?</html>', replacer, content, flags=re.DOTALL)
        
        final_output = {
            "combined_json_tables": all_tables_data,
            "modified_text": modified_text
        }
        return final_output
    
    def query_milvus(self, param):
        """åœ¨ Milvus ä¸­æœç´¢å‘é‡æ•°æ®ï¼ˆä»£ç†æ–¹æ³•ï¼Œå®é™…å®ç°åœ¨ control_search.pyï¼‰"""
        return self.search_obj.query_milvus(param)
        
    def check_knowledge_and_user(self, knowledge_id, user_id):
        param = {"knowledge_id":knowledge_id, "user_id":user_id}
        result = cSingleSqlite.search_knowledge_base_by_id_and_user_id(param)
        return result
    
    def query_graph_neo4j(self, param, merge_result = False):
        """åœ¨ Neo4j å›¾æ•°æ®åº“ä¸­æœç´¢å®ä½“å…³ç³»ï¼ˆä»£ç†æ–¹æ³•ï¼Œå®é™…å®ç°åœ¨ control_search.pyï¼‰"""
        return self.search_obj.query_graph_neo4j(param, merge_result)
    
    def execute_all_query(self, param):
        query = param.get("query", "")
        if(not query or query.strip() == ""):
            return {"error_code":3, "error_msg":"Error, lack of query."}
        start_time = time.time()
        graph_data = self.query_graph_neo4j(param, merge_result=True)
        end_time = time.time()
        print(f"Graph query time: {end_time - start_time} seconds")
        
        milvus_data = self.query_milvus(param)
        end_time_2 = time.time()
        print(f"Milvus query time: {end_time_2 - end_time} seconds")
        
        result = self.search_obj.search_graph_emb(query, graph_data, milvus_data)
        
        # param = {"query":query, "graph_data":graph_data, "emb_data":milvus_data}
        # result = self.chat_obj.answer_question(param)
        
        res = {"error_code":0, "error_msg":"Success", "data": result}
        return res
    
    def chat_with_rag(self, param):
        """RAG Agenticæ™ºèƒ½ä½“æµå¼èŠå¤©"""
        knowledge_id = param.get("knowledge_id")
        user_id = param.get("user_id")
        flag = True
        # æ£€æŸ¥çŸ¥è¯†åº“å’Œç”¨æˆ·æƒé™ï¼Œå¦‚æœæ— æƒé™ï¼Œåªèƒ½æŸ¥è¯¢å…¬å…±çŸ¥è¯†éƒ¨åˆ†
        if(not self.check_knowledge_and_user(knowledge_id, user_id)):
            flag = False
        else:
            flag = True

        try:
            knowledge_id = param.get("knowledge_id")
            query = param.get("query", "")
            user_id = param.get("user_id")

            if not knowledge_id:
                yield {"error_code": 1, "error_msg": "ç¼ºå°‘knowledge_idå‚æ•°"}
                return

            if not query or query.strip() == "":
                yield {"error_code": 2, "error_msg": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º"}
                return

            logger.info(f"å¼€å§‹RAG Agenticå¤„ç† - knowledge_id: {knowledge_id}, query: {query}")

            # è½¬æ¢èŠå¤©å†å²æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
            chat_history = []  # æš‚æ—¶ä¸ºç©ºï¼Œåç»­å¯ä»¥ä» param ä¸­æå–
            
            # è·å–æµå¼ç»“æœï¼Œå›¾æ•°æ®é€šè¿‡ä¸‰å¼•æ“æœç´¢å†…éƒ¨è·å–
            chunk_count = 0
            for chunk in agentic_rag_run.run_rag_agentic_stream(query, knowledge_id, user_id,
                                                                chat_history,
                                                                flag):
                chunk_count += 1
                yield chunk
            return
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(f"RAG Agenticå¤„ç†å¤±è´¥: {e}")
            logger.error(f"Traceback: {error_traceback}")

            yield {
                "error_code": 5,
                "error_msg": f"RAG Agenticå¤„ç†å¤±è´¥: {str(e)}",
                "traceback": error_traceback
            }

    def execute_all_chat(self, param):
        
        logger.info(f"execute_all_chat called with param: {param}")
        
        query = param.get("query", "")
        logger.info(f"Extracted query: {query}")
        
        if(not query or query.strip() == ""):
            logger.warning("No query text provided")
            yield {"error_code":3, "error_msg":"Error, lack of query."}
            return
        try:
            logger.info("Calling query_graph_neo4j")
            graph_data = self.query_graph_neo4j(param, merge_result=True)
            if(graph_data):
                graph_data = copy.deepcopy(graph_data)
            logger.info(f"graph_data result: {graph_data}")
            graph_data = []
            logger.info("Calling query_milvus")
            milvus_data = self.query_milvus(param)
            logger.info(f"milvus_data result: {milvus_data}")
            
            # ç›´æ¥è¿”å›æµå¼ç»“æœ
            logger.info("Calling search_obj.stream_openai_chat")
            stream_result = self.search_obj.stream_openai_chat(query, graph_data, milvus_data)
            logger.info(f"stream_openai_chat returned: {type(stream_result)}")
            
            # ç¡®ä¿æ­£ç¡®yieldæ•°æ®
            has_data = False
            for item in stream_result:
                has_data = True
                logger.info(f"Yielding item from stream_openai_chat: {item}")
                yield item
            
            if not has_data:
                logger.warning("No data yielded from stream_openai_chat")
                yield {"message": "No data returned from chat processing"}
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(f"Error in execute_all_chat: {str(e)}")
            logger.error(f"Traceback: {error_traceback}")
            yield {"error_code":5, "error_msg":f"Error processing stream query: {str(e)}", "traceback": error_traceback}

    def chat_with_discussion(self, user_id, session_id, query, file_path, chat_history=None, discussion_id=None):
        """
        åœ†æ¡Œè®¨è®ºå¤´è„‘é£æš´ä¼šè®®ç³»ç»Ÿ
        æ”¯æŒå¤šæ™ºèƒ½ä½“åä½œçš„æ·±åº¦è®¨è®ºå’Œå†³ç­–
        æ”¯æŒçŠ¶æ€æŒä¹…åŒ–ï¼Œå¯ä»æ–­ç‚¹æ¢å¤
        åŸºäºå†å²èŠå¤©è®°å½•æ™ºèƒ½è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼Œè‡ªåŠ¨æ¢å¤å·²æœ‰ä¼šè®®
        
        ä¼˜åŒ–ç‰ˆæœ¬ï¼šåªè¿”å›ä»»åŠ¡åˆ›å»ºæˆåŠŸä¿¡æ¯ï¼Œä¸æµå¼è¿”å›ä¸­é—´ä¿¡æ¯ã€‚

        å¯é€‰ä¼˜åŒ–ï¼šç¬¬ä¸€/äºŒ/ä¸‰å±‚ä¸ºä¸åŒæ™ºèƒ½ä½“å‘è¨€ï¼Œå¯é€šè¿‡ AgentScope ç»Ÿä¸€æ¶ˆæ¯ä¸æ‰§è¡Œã€‚
        å®‰è£… agentscope åè®¾ç½®ç¯å¢ƒå˜é‡ USE_AGENTSCOPE=1 å³å¯å¯ç”¨ï¼ˆè§ Roles/roundtable/agentscope_bridge.pyï¼‰ã€‚

        Args:
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID
            query: ç”¨æˆ·æŸ¥è¯¢
            file_path: æ–‡ä»¶è·¯å¾„
            chat_history: å†å²èŠå¤©è®°å½•ï¼Œç”¨äºæ„å›¾è¯†åˆ«
            discussion_id: å¯é€‰ï¼ŒæŒ‡å®šä»»åŠ¡IDã€‚è‹¥ä¼ å…¥ï¼ˆå¦‚å‰ç«¯ã€Œé‡å¯æŒ‡å®šä»»åŠ¡ã€ï¼‰ï¼Œåˆ™æ²¿ç”¨è¯¥IDä¸åŸæ–‡ä»¶å¤¹ï¼Œä¸åˆ›å»ºæ–°ä»»åŠ¡
        """
        # ç”Ÿæˆå”¯ä¸€IDï¼ˆç”¨äºåˆ›å»ºchunkï¼‰
        _id = f"roundtable-{int(time.time())}"
        
        # å¦‚æœæ²¡æœ‰æä¾›å†å²èŠå¤©è®°å½•ï¼Œåˆ™è‡ªåŠ¨è·å–
        if chat_history is None and session_id:
            chat_history = self.sess_obj.get_session_messages_by_id(session_id)

        # ä½¿ç”¨æ™ºèƒ½ä½“è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼ˆå››ç§æ„å›¾ï¼‰
        intent_result = self._identify_roundtable_intent(session_id, query)
        user_intent = intent_result.get('intent', 'start_new')
        target_discussion_id = intent_result.get('discussion_id')  # å¯èƒ½æ˜¯ç”¨æˆ·è¾“å…¥ä¸­æå–çš„ID
        
        # è‹¥è°ƒç”¨æ–¹æ˜¾å¼ä¼ å…¥ discussion_idï¼ˆå¦‚ API é‡å¯æŒ‡å®šä»»åŠ¡ï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨ï¼Œç¡®ä¿ç”¨åŸä»»åŠ¡IDä¸æ–‡ä»¶å¤¹
        if discussion_id and str(discussion_id).strip():
            target_discussion_id = str(discussion_id).strip()
            logger.info(f"ğŸ¯ ä½¿ç”¨è°ƒç”¨æ–¹æŒ‡å®šçš„ä»»åŠ¡IDï¼ˆé‡å¯æŒ‡å®šä»»åŠ¡ï¼‰: {target_discussion_id}")
        
        logger.info(f"ğŸ¯ æ„å›¾è¯†åˆ«ç»“æœ: intent={user_intent}, discussion_id={target_discussion_id}")
        
        # æ„å›¾1ï¼šå…¶ä»–èŠå¤©ï¼ˆéåœ†æ¡Œä¼šè®®ç›¸å…³ï¼‰
        if user_intent == 'other_chat':
            # è¿”å›æç¤ºä¿¡æ¯ï¼Œå¼•å¯¼ç”¨æˆ·ä½¿ç”¨åœ†æ¡Œä¼šè®®åŠŸèƒ½
            yield self._create_chunk(_id,
                content="""ğŸ’¡ **åœ†æ¡Œä¼šè®®ç³»ç»Ÿæç¤º**

æ‚¨çš„è¾“å…¥çœ‹èµ·æ¥ä¸æ˜¯åœ†æ¡Œä¼šè®®ç›¸å…³çš„è¯·æ±‚ã€‚

**æ”¯æŒçš„æ“ä½œï¼š**
- ğŸš€ **åˆ›å»ºæ–°ä»»åŠ¡**: "å¸®æˆ‘åˆ†æXXé—®é¢˜" / "è®¨è®ºXXä¸»é¢˜"
- ğŸ“Š **æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€**: "ä»»åŠ¡è¿›åº¦æ€ä¹ˆæ ·" / "æŸ¥çœ‹æœ€æ–°ä»»åŠ¡"
- ğŸ” **æŸ¥è¯¢æŒ‡å®šä»»åŠ¡**: "æŸ¥è¯¢ discussion_xxx çš„æƒ…å†µ"

å¦‚æœæ‚¨æƒ³è¿›è¡Œæ™®é€šå¯¹è¯ï¼Œè¯·ä½¿ç”¨å…¶ä»–èŠå¤©æ¨¡å¼ã€‚
""",
                chunk_type="text", finish_reason=""
            )

            time.sleep(1)
            yield self._create_chunk(_id,
                content="",
                chunk_type="text", finish_reason="stop"
            )

            return
        
        # æ„å›¾2ï¼šæŸ¥è¯¢æœ€æ–°ä»»åŠ¡æƒ…å†µ
        if user_intent == 'query_latest':
            # æµå¼è¿”å›ä»»åŠ¡ä¿¡æ¯
            yield from self._get_discussion_task_info(session_id, None)
            return
        
        # æ„å›¾3ï¼šæŸ¥è¯¢æŒ‡å®šä»»åŠ¡æƒ…å†µï¼ˆä»…æŸ¥çœ‹çŠ¶æ€ï¼Œä¸è¿è¡Œï¼‰
        if user_intent == 'query_specific' and target_discussion_id:
            # æµå¼è¿”å›ä»»åŠ¡ä¿¡æ¯
            yield from self._get_discussion_task_info(session_id, target_discussion_id)
            return

        # æ„å›¾5ï¼šä¿®æ”¹æŒ‡å®šä»»åŠ¡ä¸­æŸæ™ºèƒ½ä½“å‘è¨€ï¼ˆéœ€ä»»åŠ¡IDï¼‰
        if user_intent == 'modify_speech' and target_discussion_id:
            speaker = intent_result.get('speaker', '') or ''
            layer = intent_result.get('layer', 0)
            modification_content = intent_result.get('modification_content', '') or ''
            yield self._create_chunk(_id, content=f"âœ… æ”¶åˆ°ä¿®æ”¹å‘è¨€è¯·æ±‚\n\n**ä»»åŠ¡ID**: `{target_discussion_id}`\n**è§’è‰²**: {speaker or 'ï¼ˆå°†è‡ªåŠ¨åŒ¹é…ï¼‰'}\n**å±‚çº§**: {'ç¬¬ä¸€å±‚' if layer == 1 else 'ç¬¬äºŒå±‚' if layer == 2 else 'è‡ªåŠ¨'}\næ­£åœ¨åå°æ‰§è¡Œä¿®æ”¹å¹¶è§¦å‘ä¸‹æ¸¸é‡å‘è¨€â€¦\n\n", chunk_type="text", finish_reason="")
            yield self._create_chunk(_id, content="", chunk_type="text", finish_reason="stop")
            try:
                self.discussion_obj.modify_agent_speech(
                    discussion_id=target_discussion_id,
                    speaker_name=speaker.strip() or None,
                    layer=layer if layer in (1, 2) else None,
                    user_content=modification_content.strip() or query,
                )
            except Exception as e:
                logger.error(f"ä¿®æ”¹å‘è¨€æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return
        
        # æ„å›¾4ï¼šåˆ›å»ºæ–°ä»»åŠ¡ æˆ– æ¢å¤/é‡å¯å·²æœ‰ä»»åŠ¡ï¼ˆresume æˆ– start_new ä¸”å¸¦ä»»åŠ¡IDæ—¶ç”¨åŸIDï¼‰

        # é‡å¯/æ¢å¤æ—¶æ²¿ç”¨åŸä»»åŠ¡IDä¸æ–‡ä»¶å¤¹ï¼Œä¸åˆ›å»ºæ–°ID
        discussion_id = target_discussion_id if target_discussion_id else f"discussion_{uuid.uuid4().hex[:8]}"
        
        # è¿”å›ä»»åŠ¡åˆ›å»ºæˆåŠŸä¿¡æ¯
        task_type = "æ¢å¤" if target_discussion_id else "åˆ›å»º"
        yield self._create_chunk(_id,
            content=f"âœ… åœ†æ¡Œä¼šè®®ä»»åŠ¡{task_type}æˆåŠŸ\n\n"
                    f"**ä¼šè®®ID**: `{discussion_id}`\n"
                    f"ä¼šè®®å·²åœ¨åå°å¼‚æ­¥æ‰§è¡Œã€‚\n\n",
            chunk_type="text", finish_reason=""
        )
        
        yield self._create_chunk(_id,
            content="",
            chunk_type="text", finish_reason="stop"
        )

        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œåœ†æ¡Œä¼šè®®: {discussion_id} (task_type={task_type})")
        
        # å¼‚æ­¥æ‰§è¡Œåœ†æ¡Œä¼šè®®ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        def run_discussion_async():
            """åå°çº¿ç¨‹æ‰§è¡Œåœ†æ¡Œä¼šè®®"""
            try:
                self.discussion_obj.chat_with_discussion(
                    user_id, session_id, query, file_path, discussion_id)
            except Exception as e:
                logger.error(f"âŒ åœ†æ¡Œä¼šè®®æ‰§è¡Œå¤±è´¥: {discussion_id}, é”™è¯¯: {e}")
        
        # å¯åŠ¨åå°çº¿ç¨‹
        discussion_thread = threading.Thread(
            target=run_discussion_async,
            name=f"discussion_{discussion_id}",
            daemon=True  # å®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨ç»“æŸ
        )
        discussion_thread.start()
        logger.info(f"ğŸ“¤ åœ†æ¡Œä¼šè®®å·²åœ¨åå°çº¿ç¨‹å¯åŠ¨: {discussion_thread.name}")
            

    def _identify_roundtable_intent(self, session_id: str, query: str) -> dict:
        """
        åœ†æ¡Œä¼šè®®æ„å›¾è¯†åˆ«æ™ºèƒ½ä½“
        
        é€šè¿‡ session_id æŸ¥è¯¢å·²æœ‰ä»»åŠ¡åˆ—è¡¨ï¼Œç»“åˆLLMè¯†åˆ«ç”¨æˆ·æ„å›¾
        
        å››ç§æ„å›¾ï¼š
        1. start_new: åˆ›å»ºæ–°ä»»åŠ¡
        2. query_latest: å’¨è¯¢æœ€æ–°ä»»åŠ¡æƒ…å†µ
        3. query_specific: å’¨è¯¢æŒ‡å®šä»»åŠ¡ï¼ˆå¸¦ä»»åŠ¡IDï¼‰
        4. other_chat: å…¶ä»–æ™®é€šèŠå¤©
        
        Returns:
            dict: {'intent': 'start_new|query_latest|query_specific|other_chat', 'discussion_id': 'å¯èƒ½çš„ID'}
        """
        try:
            import re
            
            # æ­¥éª¤1: æå–ç”¨æˆ·è¾“å…¥ä¸­å¯èƒ½çš„ä»»åŠ¡ID
            id_pattern = r'(discussion_[a-f0-9]+)'
            id_match = re.search(id_pattern, query, re.IGNORECASE)
            extracted_id = id_match.group(1) if id_match else None
            
            # æ­¥éª¤2: é€šè¿‡ session_id æŸ¥è¯¢è¯¥ä¼šè¯ä¸‹çš„æ‰€æœ‰ä»»åŠ¡
            task_summary = cSingleSqlite.count_discussion_tasks_by_session_id(session_id)
            tasks_list = task_summary.get('tasks', [])
            total_count = task_summary.get('total_count', 0)
            
            # æ„å»ºä»»åŠ¡åˆ—è¡¨ä¸Šä¸‹æ–‡ï¼ˆä½œä¸ºLLMå‚è€ƒï¼‰
            task_context = ""
            if tasks_list:
                task_lines = []
                for i, task in enumerate(tasks_list[:5]):  # æœ€å¤šæ˜¾ç¤´5ä¸ª
                    task_id = task.get('discussion_id', '')
                    status = task.get('task_status', 'æœªçŸ¥')
                    updated = task.get('updated_at', '')[:16] if task.get('updated_at') else ''
                    task_lines.append(f"  - {task_id} (çŠ¶æ€: {status}, æ›´æ–°: {updated})")
                task_context = f"""\n**å½“å‰ä¼šè¯å·²æœ‰{total_count}ä¸ªä»»åŠ¡ï¼š**
{chr(10).join(task_lines)}
"""
            else:
                task_context = "\n**å½“å‰ä¼šè¯æš‚æ— ä»»åŠ¡è®°å½•**\n"
            
            # å¦‚æœæå–åˆ°ä»»åŠ¡IDï¼Œè¯»å–å…¶ä¸»é¢˜ä¿¡æ¯
            specific_task_info = ""
            if extracted_id:
                topic_info = self._read_task_topic_from_file(extracted_id)
                if topic_info:
                    specific_task_info = f"\n**ç”¨æˆ·æåˆ°çš„ä»»åŠ¡ID {extracted_id} çš„ä¸»é¢˜ï¼š**\n{topic_info}\n"
            
            # æ­¥éª¤3: ä½¿ç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«
            llm = get_chat_tongyi(temperature=0.1, enable_thinking=False)
            
            intent_prompt = f"""ä½ æ˜¯åœ†æ¡Œä¼šè®®ç³»ç»Ÿçš„æ„å›¾è¯†åˆ«æ™ºèƒ½ä½“ã€‚è¯·åˆ†æç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­ç”¨æˆ·æ„å›¾ã€‚

**ç”¨æˆ·è¾“å…¥ï¼š**
{query}
{task_context}{specific_task_info}
**æ„å›¾åˆ†ç±»ï¼ˆä¸¥æ ¼ä»ä»¥ä¸‹äº”ç§ä¸­é€‰æ‹©ä¸€ç§ï¼‰ï¼š**

1. **start_new** - åˆ›å»ºæ–°ä»»åŠ¡
   - ç”¨æˆ·æƒ³è¦å‘èµ·æ–°çš„è®¨è®º/åˆ†æ/å¤´è„‘é£æš´
   - ä¾‹å¦‚ï¼š"å¸®æˆ‘åˆ†æXX"ã€"è®¨è®ºXXé—®é¢˜"ã€"ç ”ç©¶ä¸€ä¸‹XX"ã€"åˆ¶å®šXXæ–¹æ¡ˆ"

2. **query_latest** - å’¨è¯¢æœ€æ–°ä»»åŠ¡
   - ç”¨æˆ·æƒ³äº†è§£æœ€è¿‘ä»»åŠ¡çš„è¿›åº¦æˆ–ç»“æœï¼Œä½†æ²¡æœ‰æŒ‡å®šå…·ä½“ä»»åŠ¡ID
   - ä¾‹å¦‚ï¼š"ä»»åŠ¡è¿›åº¦æ€ä¹ˆæ ·"ã€"æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€"ã€"å®Œæˆäº†å—"ã€"ç»“æœæ˜¯ä»€ä¹ˆ"

3. **query_specific** - ä»…æŸ¥è¯¢æŒ‡å®šä»»åŠ¡ï¼ˆåªçœ‹çŠ¶æ€ï¼Œä¸è¿è¡Œï¼‰
   - ç”¨æˆ·æŒ‡å®šäº†ä»»åŠ¡IDä¸”ä»…æƒ³æŸ¥çœ‹è¯¥ä»»åŠ¡æƒ…å†µ
   - ä¾‹å¦‚ï¼š"æŸ¥è¯¢ discussion_abc123 çš„æƒ…å†µ"ã€"discussion_xxx å®Œæˆäº†å—"

4. **resume** - æ¢å¤/é‡å¯æŒ‡å®šä»»åŠ¡ï¼ˆç»§ç»­è¿è¡Œè¯¥ä»»åŠ¡ï¼Œç”¨åŸä»»åŠ¡IDä¸æ–‡ä»¶å¤¹ï¼‰
   - ç”¨æˆ·è¦ç»§ç»­è¿è¡Œæˆ–é‡å¯å·²æœ‰ä»»åŠ¡ï¼Œä¸”è¾“å…¥ä¸­åŒ…å«ä»»åŠ¡ID
   - ä¾‹å¦‚ï¼š"ç»§ç»­ discussion_xxx"ã€"é‡å¯ discussion_xxx"ã€"ç»§ç»­è¿è¡Œ discussion_xxx ä»»åŠ¡"

5. **other_chat** - å…¶ä»–æ™®é€šèŠå¤©
   - ä¸åœ†æ¡Œä¼šè®®æ— å…³çš„é—²èŠã€é—®å€™ã€æ— æ„ä¹‰è¾“å…¥
   - ä¾‹å¦‚ï¼š"ä½ å¥½"ã€"è°¢è°¢"ã€"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"ã€"å˜¿å˜¿"

6. **modify_speech** - ä¿®æ”¹æŒ‡å®šä»»åŠ¡ä¸­æŸæ™ºèƒ½ä½“å‘è¨€
   - ç”¨æˆ·æŒ‡å®šäº†ä»»åŠ¡IDï¼Œå¹¶è¦æ±‚ä¿®æ”¹è¯¥ä»»åŠ¡ä¸­æŸä¸€è§’è‰²/æ™ºèƒ½ä½“çš„å‘è¨€å†…å®¹
   - ä¾‹å¦‚ï¼š"ä¿®æ”¹ discussion_xxx é‡Œ ä¸“å®¶_äººæœºäº¤äº’ çš„å‘è¨€"ã€"æŠŠ discussion_abc ä¸­ äººæœºäº¤äº’ çš„å‘è¨€æ”¹æˆï¼š..."ã€"ä¿®æ”¹ä»»åŠ¡ discussion_xxx ä¸­ ç¬¬äºŒå±‚ å·¥ä¸šè®¾è®¡ çš„å‘è¨€"

**è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š**
{{
    "intent": "start_new|query_latest|query_specific|resume|other_chat|modify_speech",
    "confidence": 0.0-1.0,
    "reasoning": "ç®€è¦åˆ¤æ–­ç†ç”±",
    "speaker": "å½“ intent ä¸º modify_speech æ—¶ï¼Œä»ç”¨æˆ·è¾“å…¥ä¸­è¯†åˆ«çš„æ™ºèƒ½ä½“åç§°ï¼Œå¦‚ ä¸“å®¶_äººæœºäº¤äº’ã€äººæœºäº¤äº’ã€å·¥ä¸šè®¾è®¡ ç­‰ï¼Œå¦åˆ™å¡«ç©ºå­—ç¬¦ä¸²",
    "layer": "å½“ intent ä¸º modify_speech æ—¶å¡« 1 æˆ– 2 è¡¨ç¤ºç¬¬ä¸€å±‚æˆ–ç¬¬äºŒå±‚ï¼Œæ— æ³•åˆ¤æ–­å¡« 0",
    "modification_content": "å½“ intent ä¸º modify_speech ä¸”ç”¨æˆ·ç›´æ¥ç»™å‡ºäº†ä¿®æ”¹åçš„å†…å®¹æ—¶ï¼Œæå–è¯¥å†…å®¹ï¼›å¦åˆ™å¡«ç©ºå­—ç¬¦ä¸²"
}}"""
            
            response = llm.invoke(intent_prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # è§£æJSONå“åº”
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                try:
                    json_text = json_match.group().strip()
                    json_text = re.sub(r'```json\s*', '', json_text)
                    json_text = re.sub(r'```\s*', '', json_text)
                    intent_result = json.loads(json_text)
                    
                    intent = intent_result.get('intent', 'start_new')
                    confidence = intent_result.get('confidence', 0.5)
                    reasoning = intent_result.get('reasoning', '')
                    
                    logger.info(f"ğŸ§  æ„å›¾è¯†åˆ«æ™ºèƒ½ä½“: intent={intent}, confidence={confidence}, reasoning={reasoning[:50]}")
                    
                    # éªŒè¯intentå€¼
                    valid_intents = ['start_new', 'query_latest', 'query_specific', 'resume', 'other_chat', 'modify_speech']
                    if intent not in valid_intents:
                        intent = 'start_new'
                    
                    result = {'intent': intent, 'discussion_id': extracted_id}
                    if intent == 'modify_speech':
                        result['speaker'] = intent_result.get('speaker', '') or ''
                        result['layer'] = int(intent_result.get('layer', 0)) if str(intent_result.get('layer', '')).isdigit() else 0
                        result['modification_content'] = intent_result.get('modification_content', '') or ''
                    return result
                except json.JSONDecodeError as e:
                    logger.warning(f"æ„å›¾è¯†åˆ«JSONè§£æå¤±è´¥: {e}")
            
            # å›é€€ç­–ç•¥ï¼šå…³é”®è¯åŒ¹é…
            return self._fallback_roundtable_intent(query, extracted_id)
            
        except Exception as e:
            logger.warning(f"æ„å›¾è¯†åˆ«å¤±è´¥: {e}ï¼Œä½¿ç”¨å›é€€ç­–ç•¥")
            return self._fallback_roundtable_intent(query, None)
    
    def _fallback_roundtable_intent(self, query: str, extracted_id: str = None) -> dict:
        """
        å›é€€çš„å…³é”®è¯æ„å›¾è¯†åˆ«ï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰
        """
        query_lower = query.lower().strip()
        
        # è‹¥æœ‰ä»»åŠ¡IDä¸”ç”¨æˆ·è¯´ç»§ç»­/é‡å¯ï¼Œåˆ™è§†ä¸ºæ¢å¤æŒ‡å®šä»»åŠ¡ï¼ˆç”¨åŸIDï¼‰
        resume_keywords = ['ç»§ç»­', 'é‡å¯', 'æ¢å¤', 'æ¥ç€', 'resume', 'restart', 'ç»§ç»­è¿è¡Œ']
        if extracted_id and any(kw in query_lower for kw in resume_keywords):
            return {'intent': 'resume', 'discussion_id': extracted_id}
        # è‹¥æœ‰ä»»åŠ¡IDä½†ä»…æŸ¥è¯¢ç±»æªè¾ï¼Œåˆ™ä»…æŸ¥è¯¢
        if extracted_id:
            return {'intent': 'query_specific', 'discussion_id': extracted_id}
        
        # æŸ¥è¯¢ä»»åŠ¡çš„å…³é”®è¯
        query_keywords = ['æŸ¥è¯¢', 'æŸ¥çœ‹', 'ä»»åŠ¡æƒ…å†µ', 'ä»»åŠ¡çŠ¶æ€', 'è¿›åº¦', 'æ€ä¹ˆæ ·äº†', 
                         'å®Œæˆäº†å—', 'ç»“æœ', 'è¿›å±•', 'status', 'progress', 'result']
        if any(kw in query_lower for kw in query_keywords):
            return {'intent': 'query_latest', 'discussion_id': None}
        
        # å…¶ä»–èŠå¤©çš„å…³é”®è¯ï¼ˆçŸ­æ— æ„ä¹‰è¾“å…¥ï¼‰
        other_keywords = ['ä½ å¥½', 'å˜¿', 'å“ˆ', 'å˜¿å˜¿', 'å“ˆå“ˆ', 'è°¢è°¢', 'å†è§', 'hi', 'hello', 'thanks']
        if query_lower in other_keywords or len(query_lower) <= 2:
            return {'intent': 'other_chat', 'discussion_id': None}
        
        # é»˜è®¤ï¼šåˆ›å»ºæ–°ä»»åŠ¡
        return {'intent': 'start_new', 'discussion_id': None}
    
    def _read_task_topic_from_file(self, discussion_id: str) -> str:
        """
        ä»æ–‡ä»¶ä¸­è¯»å–ä»»åŠ¡ä¸»é¢˜
        
        Args:
            discussion_id: ä»»åŠ¡ID
            
        Returns:
            str: ä»»åŠ¡ä¸»é¢˜ï¼Œå¦‚æœè¯»å–å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            state_file = os.path.join("discussion", discussion_id, "discussion_state.json")
            if not os.path.exists(state_file):
                return ""
            
            with open(state_file, 'r', encoding='utf-8') as f:
                state_data = json.load(f)
            
            return state_data.get('topic', '')
        except Exception as e:
            logger.warning(f"è¯»å–ä»»åŠ¡ä¸»é¢˜å¤±è´¥: {e}")
            return ""
    
    def _get_discussion_task_info(self, session_id: str, discussion_id: str = None):
        """
        æµå¼è·å–ä»»åŠ¡æƒ…å†µä¿¡æ¯ï¼ˆç”Ÿæˆå™¨æ–¹æ³•ï¼‰
        
        Args:
            session_id: ä¼šè¯ID
            discussion_id: æŒ‡å®šçš„ä»»åŠ¡IDï¼Œå¦‚æœä¸ºNoneåˆ™æŸ¥è¯¢æœ€æ–°ä»»åŠ¡
            
        Yields:
            dict: æµå¼chunkï¼ŒåŒ…å«æ–‡æœ¬ä¿¡æ¯å’Œæ–‡ä»¶è·¯å¾„
        """
        _id = f"task-info-{int(time.time())}"
        
        try:
            discussion_base_dir = "discussion"
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šIDï¼ŒæŸ¥æ‰¾æœ€æ–°çš„ä»»åŠ¡
            if not discussion_id:
                task_summary = cSingleSqlite.count_discussion_tasks_by_session_id(session_id)
                tasks_list = task_summary.get('tasks', [])
                if tasks_list:
                    discussion_id = tasks_list[0].get('discussion_id')  # æœ€æ–°çš„ä»»åŠ¡
            
            if not discussion_id:
                yield self._create_chunk(_id,
                    content="""â„¹ï¸ **æš‚æ— ä»»åŠ¡è®°å½•**

å½“å‰ä¼šè¯ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åœ†æ¡Œä¼šè®®ä»»åŠ¡ã€‚

ğŸ’¡ **æç¤º**: æ‚¨å¯ä»¥é€šè¿‡è¯´"å¼€å§‹è®¨è®ºXXé—®é¢˜"æ¥å¯åŠ¨æ–°çš„åœ†æ¡Œä¼šè®®ã€‚
""",
                    chunk_type="text", finish_reason=""
                )

                time.sleep(1)

                yield self._create_chunk(_id,
                    content="",
                    chunk_type="text", finish_reason="stop"
                ) 

                return
            
            # è¯»å–ä»»åŠ¡çŠ¶æ€æ–‡ä»¶
            state_file = os.path.join(discussion_base_dir, discussion_id, "discussion_state.json")
            
            if not os.path.exists(state_file):
                yield self._create_chunk(_id,
                    content=f"""âš ï¸ **ä»»åŠ¡ä¸å­˜åœ¨**

æ‰¾ä¸åˆ°ä»»åŠ¡ID: `{discussion_id}` çš„è®°å½•æ–‡ä»¶ã€‚

ğŸ’¡ **æç¤º**: è¯·ç¡®è®¤ä»»åŠ¡IDæ˜¯å¦æ­£ç¡®ï¼Œæˆ–å¯åŠ¨æ–°çš„åœ†æ¡Œä¼šè®®ã€‚
""",
                    chunk_type="text", finish_reason=""
                )

                time.sleep(1)

                yield self._create_chunk(_id,
                    content="",
                    chunk_type="text", finish_reason="stop"
                ) 

                return
            
            # è¯»å–çŠ¶æ€æ–‡ä»¶
            with open(state_file, 'r', encoding='utf-8') as f:
                state_data = json.load(f)
            
            # æ„å»ºåŸºæœ¬ä¿¡æ¯
            status = state_data.get('status', 'æœªçŸ¥')
            status_emoji = {
                'active': 'ğŸŸ¢ è¿›è¡Œä¸­',
                'paused': 'â¸ï¸ å·²æš‚åœ',
                'completed': 'âœ… å·²å®Œæˆ',
                'error': 'âŒ å‡ºé”™'
            }.get(status, f'â“ {status}')
            
            topic = state_data.get('topic', 'æœªæŒ‡å®š')
            current_round = state_data.get('current_round', 0)
            max_rounds = state_data.get('max_rounds', 5)
            participants = state_data.get('participants', [])
            created_at = state_data.get('created_at', 'æœªçŸ¥')
            updated_at = state_data.get('updated_at', 'æœªçŸ¥')
            rounds_info = state_data.get('rounds', [])
            
            # è¿”å›åŸºæœ¬ä¿¡æ¯ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
            basic_info = f"""## ğŸ“Š ä»»åŠ¡æƒ…å†µæŠ¥å‘Š

### ğŸ¯ åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|------|------|
| ä»»åŠ¡ID | `{discussion_id}` |
| çŠ¶æ€ | {status_emoji} |
| è®¨è®ºä¸»é¢˜ | {topic[:80]}{'...' if len(topic) > 80 else ''} |
| å½“å‰è½®æ¬¡ | ç¬¬ {current_round} / {max_rounds} è½® |
| å‚ä¸äººæ•° | {len(participants)} ä½ä¸“å®¶ |
| åˆ›å»ºæ—¶é—´ | {created_at[:19] if created_at else 'æœªçŸ¥'} |
| æ›´æ–°æ—¶é—´ | {updated_at[:19] if updated_at else 'æœªçŸ¥'} |

"""
            yield self._create_chunk(_id,
                content=basic_info,
                chunk_type="text", finish_reason=""
            )
            
            # è¿”å›å‚ä¸è€…åˆ—è¡¨
            if participants:
                participants_text = "### ğŸ‘¥ å‚ä¸ä¸“å®¶\n\n"
                for p in participants:
                    participants_text += f"- {p}\n"
                participants_text += "\n"
                yield self._create_chunk(_id,
                    content=participants_text,
                    chunk_type="text", finish_reason=""
                )
            
            # è¿”å›è½®æ¬¡åŠå‘è¨€æ–‡ä»¶ä¿¡æ¯
            if rounds_info:
                for round_data in rounds_info:
                    r_num = round_data.get('round_number', 0)
                    r_status = round_data.get('status', 'æœªçŸ¥')
                    speeches = round_data.get('speeches', [])
                    
                    # è½®æ¬¡æ ‡é¢˜
                    round_header = f"### ğŸ—£ï¸ ç¬¬{r_num}è½®è®¨è®º ({r_status}) - å…±{len(speeches)}æ¡å‘è¨€\n\n"
                    yield self._create_chunk(_id,
                        content=round_header,
                        chunk_type="text", finish_reason=""
                    )
                    
                    # æ¯ä¸ªå‘è¨€çš„æ–‡ä»¶ä¿¡æ¯
                    for speech in speeches:
                        speaker = speech.get('speaker', 'æœªçŸ¥')
                        file_path = speech.get('file_path', '')
                        timestamp = speech.get('timestamp', '')
                        is_skeptic = speech.get('is_skeptic', False)
                        
                        # è§’è‰²ç±»å‹æ ‡è®°
                        role_tag = "ğŸ¤” è´¨ç–‘è€…" if is_skeptic else "ğŸ’¬ ä¸“å®¶"
                        
                        # è¿”å›å‘è¨€ä¿¡æ¯æ–‡æœ¬
                        speech_info = f"**{role_tag}**: {speaker}\n"
                        yield self._create_chunk(_id,
                            content=speech_info,
                            chunk_type="text", finish_reason=""
                        )
                        
                        # è¿”å›æ–‡ä»¶è·¯å¾„ï¼ˆchunk_type="file"ï¼‰
                        if file_path:
                            yield self._create_chunk(_id,
                                content=file_path,
                                chunk_type="file", finish_reason=""
                            )
                        
                        # æ·»åŠ æ¢è¡Œ
                        yield self._create_chunk(_id,
                            content="\n",
                            chunk_type="text", finish_reason=""
                        )
            else:
                yield self._create_chunk(_id,
                    content="### ğŸ“¨ æš‚æ— è½®æ¬¡è®°å½•\n\nè®¨è®ºå°šæœªå¼€å§‹æˆ–æš‚æ— å‘è¨€è®°å½•ã€‚\n",
                    chunk_type="text", finish_reason=""
                )
            
            # ç¬¬äºŒå±‚ï¼šå®æ–½è®¨è®ºç»„ï¼ˆæ™ºèƒ½ä½“ä¸å‘è¨€ï¼‰
            layer2 = state_data.get('layer2', {})
            if layer2:
                layer2_header = "\n\n### ğŸ› ï¸ ç¬¬äºŒå±‚ å®æ–½è®¨è®ºç»„\n\n"
                yield self._create_chunk(_id, content=layer2_header, chunk_type="text", finish_reason="")
                if layer2.get('participants'):
                    parts_text = "#### ğŸ‘¥ å‚ä¸ä¸“å®¶\n\n" + "\n".join(f"- {p}" for p in layer2['participants']) + "\n\n"
                    yield self._create_chunk(_id, content=parts_text, chunk_type="text", finish_reason="")
                for sp in layer2.get('speeches', []):
                    speaker = sp.get('speaker', 'ä¸“å®¶')
                    yield self._create_chunk(_id, content=f"**å®æ–½ä¸“å®¶**: {speaker}\n", chunk_type="text", finish_reason="")
                    rel_path = sp.get('relative_file_path', '')
                    if rel_path:
                        full_path = os.path.join(discussion_base_dir, discussion_id, rel_path)
                        yield self._create_chunk(_id, content=full_path, chunk_type="file", finish_reason="")
                    yield self._create_chunk(_id, content="\n", chunk_type="text", finish_reason="")

            # ç¬¬ä¸‰å±‚ï¼šå…·åƒåŒ–å±‚ï¼ˆæ™ºèƒ½ä½“å‘è¨€/ç»“æœæ–‡ä»¶ï¼Œå¯ç‚¹å‡»æŸ¥çœ‹ï¼‰
            conc_layer = state_data.get('concretization_layer', {})
            conc_dir = os.path.join(discussion_base_dir, discussion_id, "concretization")
            if conc_layer.get('status') == 'completed' or (os.path.isdir(conc_dir) and os.listdir(conc_dir)):
                layer3_header = "\n\n### ğŸ“ ç¬¬ä¸‰å±‚ å…·åƒåŒ–ç»“æœ\n\n"
                yield self._create_chunk(_id, content=layer3_header, chunk_type="text", finish_reason="")
                # å±•ç¤º concretization/ ä¸‹æ‰€æœ‰ç»“æœæ–‡ä»¶ï¼ˆ.md ä¸ºä¸»ã€.json å¯é€‰ï¼‰
                try:
                    for fname in sorted(os.listdir(conc_dir)):
                        if not fname.endswith('.md') and not fname.endswith('.json'):
                            continue
                        full_path = os.path.join(conc_dir, fname)
                        if not os.path.isfile(full_path):
                            continue
                        label = "å…·åƒåŒ–æŠ¥å‘Š (Markdown)" if fname.endswith('.md') else "å…·åƒåŒ–ç»“æœ (JSON)"
                        yield self._create_chunk(_id, content=f"**{label}**: `{fname}`\n", chunk_type="text", finish_reason="")
                        yield self._create_chunk(_id, content=full_path, chunk_type="file", finish_reason="")
                        yield self._create_chunk(_id, content="\n", chunk_type="text", finish_reason="")
                except OSError:
                    pass

            # ç»“æŸæ ‡è®°
            yield self._create_chunk(_id,
                content="",
                chunk_type="text", finish_reason="stop"
            )
            
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡ä¿¡æ¯å¤±è´¥: {e}")
            yield self._create_chunk(_id,
                content=f"âŒ è·å–ä»»åŠ¡ä¿¡æ¯å¤±è´¥: {str(e)}",
                chunk_type="text", finish_reason="stop"
            )

    # ä¿®æ”¹ chat å‡½æ•°ä»¥æ”¯æŒåŒæ­¥è°ƒç”¨
    def chat(self, user_id, session_id, query):
        """
        æ™ºèƒ½èŠå¤©å‡½æ•°ï¼Œæ”¯æŒæ–‡ä»¶æ„å›¾è¯†åˆ«
        
        æµç¨‹ï¼š
        1. é€šè¿‡ session_id è·å–è¯¥ä¼šè¯ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        2. ä½¿ç”¨LLMè¯†åˆ«ç”¨æˆ·æ„å›¾ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨å’¨è¯¢ä¸Šä¼ æ–‡ä»¶å†…å®¹
        3. å¦‚æœæ˜¯å’¨è¯¢æ–‡ä»¶ï¼Œè°ƒç”¨ chat_with_file è¿”å›æ–‡ä»¶å†…å®¹å’¨è¯¢
        4. å¦åˆ™ï¼Œä½¿ç”¨å¸¸è§„çš„æ„å›¾è¯†åˆ«æ™ºèƒ½ä½“è¿›è¡Œå¯¹è¯
        """
        try:
            # æ­¥éª¤1: è·å–è¯¥ä¼šè¯ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            file_info_list = cSingleSqlite.search_file_basic_info_by_session_id(session_id)
            
            # å¦‚æœæœ‰ä¸Šä¼ çš„æ–‡ä»¶ï¼Œè¿›è¡Œæ„å›¾è¯†åˆ«
            if file_info_list and len(file_info_list) > 0:
                # æå–æ–‡ä»¶ååˆ—è¡¨
                file_names = [file_info.get("file_name", "") for file_info in file_info_list if file_info.get("file_name")]
                
                if file_names:
                    logger.info(f"ğŸ“ ä¼šè¯ {session_id} æœ‰ {len(file_names)} ä¸ªä¸Šä¼ æ–‡ä»¶: {file_names}")
                    
                    # æ­¥éª¤2: ä½¿ç”¨LLMè¯†åˆ«ç”¨æˆ·æ„å›¾
                    llm = get_chat_tongyi(temperature=0.3, enable_thinking=False)
                    
                    # æ„å»ºæ„å›¾è¯†åˆ«æç¤º
                    file_list_text = "\n".join([f"- {name}" for name in file_names])
                    intent_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ„å›¾è¯†åˆ«åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦åœ¨å’¨è¯¢ä¼šè¯ä¸­ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹ã€‚

**ä¼šè¯ä¸­ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨ï¼š**
{file_list_text}

**ç”¨æˆ·é—®é¢˜ï¼š**
{query}

**åˆ¤æ–­æ ‡å‡†ï¼š**
1. ç”¨æˆ·æ˜¯å¦æ˜ç¡®æåˆ°äº†æ–‡ä»¶åæˆ–æ–‡ä»¶ç›¸å…³å†…å®¹ï¼Ÿ
2. ç”¨æˆ·æ˜¯å¦åœ¨è¯¢é—®æ–‡ä»¶çš„å†…å®¹ã€ä¿¡æ¯ã€æ•°æ®ç­‰ï¼Ÿ
3. ç”¨æˆ·æ˜¯å¦åœ¨å’¨è¯¢æ–‡ä»¶ä¸­çš„å…·ä½“ä¿¡æ¯ï¼Ÿ

**è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š**
{{
    "is_file_consultation": true/false,
    "target_file_name": "å¦‚æœis_file_consultationä¸ºtrueï¼Œè¿”å›åŒ¹é…çš„æ–‡ä»¶åï¼Œå¦åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²",
    "confidence": 0.0-1.0ä¹‹é—´çš„ç½®ä¿¡åº¦,
    "reasoning": "åˆ¤æ–­ç†ç”±"
}}"""
                    
                    try:
                        # è°ƒç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«
                        response = llm.invoke(intent_prompt)
                        response_text = response.content if hasattr(response, 'content') else str(response)
                        
                        # è§£æJSONå“åº”
                        # import re
                        # å°è¯•æå–JSONå¯¹è±¡ï¼ˆæ”¯æŒåµŒå¥—ï¼‰
                        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                        if json_match:
                            try:
                                intent_result = json.loads(json_match.group())
                            except json.JSONDecodeError:
                                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æ¸…ç†æ–‡æœ¬åå†è§£æ
                                json_text = json_match.group().strip()
                                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                                json_text = re.sub(r'```json\s*', '', json_text)
                                json_text = re.sub(r'```\s*', '', json_text)
                                try:
                                    intent_result = json.loads(json_text)
                                except json.JSONDecodeError:
                                    logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œå“åº”æ–‡æœ¬: {response_text[:200]}")
                                    intent_result = None
                        else:
                            intent_result = None
                        
                        if intent_result:
                            
                            is_file_consultation = intent_result.get("is_file_consultation", False)
                            target_file_name = intent_result.get("target_file_name", "")
                            confidence = intent_result.get("confidence", 0.0)
                            
                            logger.info(f"ğŸ” æ„å›¾è¯†åˆ«ç»“æœ: is_file_consultation={is_file_consultation}, target_file={target_file_name}, confidence={confidence}")
                            
                            # æ­¥éª¤3: å¦‚æœè¯†åˆ«ä¸ºæ–‡ä»¶å’¨è¯¢ï¼Œè°ƒç”¨ chat_with_file
                            if is_file_consultation and confidence > 0.5:
                                # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶è·¯å¾„
                                target_file_path = None
                                
                                if target_file_name:
                                    # ç²¾ç¡®åŒ¹é…æ–‡ä»¶å
                                    for file_info in file_info_list:
                                        if file_info.get("file_name") == target_file_name:
                                            target_file_path = file_info.get("file_path")
                                            break
                                
                                # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆæ–‡ä»¶ååŒ…å«åœ¨æŸ¥è¯¢ä¸­ï¼‰
                                if not target_file_path:
                                    query_lower = query.lower()
                                    for file_info in file_info_list:
                                        file_name = file_info.get("file_name", "")
                                        file_name_lower = file_name.lower()
                                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åœ¨æŸ¥è¯¢ä¸­è¢«æåŠ
                                        if file_name_lower in query_lower or any(word in query_lower for word in file_name_lower.split('.')[0].split('_')):
                                            target_file_path = file_info.get("file_path")
                                            logger.info(f"ğŸ” é€šè¿‡æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°æ–‡ä»¶: {file_name}")
                                            break
                                
                                # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å®ƒ
                                if not target_file_path and len(file_info_list) == 1:
                                    target_file_path = file_info_list[0].get("file_path")
                                    logger.info(f"ğŸ” åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨: {file_info_list[0].get('file_name')}")
                                
                                if target_file_path and os.path.exists(target_file_path):
                                    logger.info(f"âœ… è¯†åˆ«ä¸ºæ–‡ä»¶å’¨è¯¢ï¼Œè°ƒç”¨ chat_with_file: {target_file_path}")
                                    # è°ƒç”¨æ–‡ä»¶åˆ†æåŠŸèƒ½
                                    for chunk in self.chat_with_file(user_id, session_id, query, target_file_path):
                                        yield chunk
                                    return
                                else:
                                    logger.warning(f"âš ï¸ æ— æ³•æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ï¼Œç»§ç»­ä½¿ç”¨å¸¸è§„å¯¹è¯")
                        else:
                            logger.warning("âš ï¸ æ— æ³•è§£æLLMè¿”å›çš„JSONæ ¼å¼")
                    except Exception as e:
                        logger.error(f"âŒ æ„å›¾è¯†åˆ«å¤±è´¥: {e}", exc_info=True)
                        # æ„å›¾è¯†åˆ«å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å¸¸è§„å¯¹è¯
            else:
                logger.info(f"ğŸ“ ä¼šè¯ {session_id} æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œä½¿ç”¨å¸¸è§„å¯¹è¯")
            
            # æ­¥éª¤4: å¸¸è§„å¯¹è¯æµç¨‹
            messages = self.sess_obj.get_session_messages_by_id(session_id)
            # è°ƒç”¨ä¿®æ”¹åçš„ run_sync å‡½æ•°ï¼Œç°åœ¨å®ƒè¿”å›å®Œæ•´çš„å“åº”
            result = intent_recog_run.run_sync_stream(query, messages)
            for chunk in result:
                yield chunk
                
        except Exception as e:
            logger.error(f"âŒ chat å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œå›é€€åˆ°å¸¸è§„å¯¹è¯
            try:
                messages = self.sess_obj.get_session_messages_by_id(session_id)
                result = intent_recog_run.run_sync_stream(query, messages)
                for chunk in result:
                    yield chunk
            except Exception as e2:
                logger.error(f"âŒ å¸¸è§„å¯¹è¯ä¹Ÿå¤±è´¥: {e2}", exc_info=True)
                yield self._create_chunk("chatcmpl-error", f"å¯¹è¯å¤„ç†å¤±è´¥: {str(e2)}", chunk_type="text", finish_reason="stop", model="chat-model")
        
    def chat_with_file(self, user_id, session_id, query, file_path):
        if(True):
        # try:
            print("chat_with_file: å¼€å§‹è¯»å–æ–‡ä»¶")
            print(f"file_path: {file_path}")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£ææ–¹å¼ï¼ˆå‚è€ƒ control.py çš„é€»è¾‘ï¼‰
            file_extension = os.path.splitext(file_path)[1].lower()
            is_pdf = file_extension == ".pdf"
            is_text_file = file_extension in [".md", ".txt"]
            is_table_file = file_extension in [".csv", ".xlsx", ".xls"]
            
            if is_table_file:
                # CSVã€XLSXã€XLS æ–‡ä»¶: ä½¿ç”¨è¡¨æ ¼æ–‡ä»¶åˆ†ææ™ºèƒ½ä½“
                logger.info(f"ä½¿ç”¨è¡¨æ ¼æ–‡ä»¶åˆ†ææ™ºèƒ½ä½“: {file_path}")
                
                # å®šä¹‰æ­¥éª¤å›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼Œç”¨äºæ­¥éª¤é€šçŸ¥ï¼‰
                def step_callback(step_name: str, step_data: Dict[str, Any]):
                    """æ­¥éª¤å›è°ƒå‡½æ•°"""
                    logger.info(f"ğŸ“Š è¡¨æ ¼åˆ†ææ­¥éª¤: {step_name} - {step_data.get('message', '')}")
                
                # ç›´æ¥æµå¼è¿”å›åˆ†æç»“æœ
                try:
                    for chunk in table_file_run.run_table_analysis_stream(file_path, query=query, step_callback=step_callback):
                        yield chunk
                    # for chunk in run_table_analysis_stream(file_path, query=query, step_callback=step_callback):
                    #     yield chunk
                except Exception as e:
                    logger.error(f"âŒ è¡¨æ ¼åˆ†ææµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
                    traceback.print_exc()
                    yield self._create_chunk("chatcmpl-error", f"è¡¨æ ¼åˆ†æå¤±è´¥: {str(e)}", chunk_type="text", finish_reason="stop", model="file-analysis-model")
                return
            elif is_text_file:
                # md æˆ– txt æ–‡ä»¶: ç›´æ¥è¯»å–æ–‡æœ¬å†…å®¹
                logger.info(f"ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶è¯»å–: {file_path}")
                _txt = self.file_obj.read_txt(file_path)
                if _txt is None:
                    logger.error(f"æ–‡æœ¬æ–‡ä»¶è¯»å–å¤±è´¥: {file_path}")
                    # è¿”å›é”™è¯¯ä¿¡æ¯çš„ç”Ÿæˆå™¨
                    def error_generator():
                        yield self._create_chunk("chatcmpl-error", "æ–‡æœ¬æ–‡ä»¶è¯»å–å¤±è´¥", chunk_type="text", finish_reason="", model="file-analysis-model")
                        yield self._create_chunk("chatcmpl-error", "", chunk_type="text", finish_reason="stop", model="file-analysis-model")
                    return error_generator()
            elif is_pdf and not is_pdf_advanced_enabled():
                # PDF_FLAG=False: ä½¿ç”¨åˆçº§PDFè§£æ
                logger.info(f"ä½¿ç”¨åˆçº§PDFè§£æ: {file_path}")
                _, content_list = self.file_obj.read_pdf_basic(file_path)
                if content_list is None:
                    logger.error(f"åˆçº§PDFè§£æå¤±è´¥: {file_path}")
                    # è¿”å›é”™è¯¯ä¿¡æ¯çš„ç”Ÿæˆå™¨
                    def error_generator():
                        yield self._create_chunk("chatcmpl-error", "PDFè§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ˜¯å¦å®‰è£…äº†PyMuPDFæˆ–PyPDF2", chunk_type="text", finish_reason="", model="file-analysis-model")
                        yield self._create_chunk("chatcmpl-error", "", chunk_type="text", finish_reason="stop", model="file-analysis-model")
                    return error_generator()
                _txt = content_list  # åˆçº§è§£æç›´æ¥è¿”å›æ–‡æœ¬å†…å®¹
            else:
                # PDF_FLAG=True æˆ–å…¶ä»–æ–‡ä»¶ç±»å‹: ä½¿ç”¨é«˜çº§è§£æï¼ˆè°ƒç”¨APIï¼‰
                logger.info(f"ä½¿ç”¨é«˜çº§æ–‡ä»¶è§£æ: {file_path}")
                _, content_list = self.file_obj.read_stream_file(file_path)
                if content_list is None:
                    logger.error(f"é«˜çº§æ–‡ä»¶è§£æå¤±è´¥: {file_path}")
                    # è¿”å›é”™è¯¯ä¿¡æ¯çš„ç”Ÿæˆå™¨
                    def error_generator():
                        yield self._create_chunk("chatcmpl-error", "æ–‡ä»¶è§£æå¤±è´¥", chunk_type="text", finish_reason="", model="file-analysis-model")
                        yield self._create_chunk("chatcmpl-error", "", chunk_type="text", finish_reason="stop", model="file-analysis-model")
                    return error_generator()
                _txt = self.content_list_to_json(content_list, file_id="")
            
            # ç”Ÿæˆ .md æ–‡ä»¶è·¯å¾„å¹¶ä¿å­˜å†…å®¹
            file_name = os.path.basename(file_path)
            txt, _ = os.path.splitext(file_name)
            file_md_path = file_path.replace(file_name, txt + ".md")
            with open(file_md_path, "w", encoding="utf-8") as f:
                f.write(_txt)

            # ä½¿ç”¨æ–‡ä»¶åˆ†ææ™ºèƒ½ä½“çš„æµå¼ç»“æœ
            # ä¼ å…¥æ–‡æœ¬è¯»å–çš„å†…å®¹å’Œç”¨æˆ·çš„é—®é¢˜
            # æ³¨æ„ï¼šç›´æ¥ä¼ å…¥ content è€Œä¸æ˜¯ file_pathï¼Œå› ä¸ºå†…å®¹å·²ç»è¯»å–ï¼Œé¿å…å†æ¬¡è¯»å–æ–‡ä»¶
            input_data = {
                "file_path": file_path,  # ä¿ç•™åŸå§‹æ–‡ä»¶è·¯å¾„ä½œä¸ºå…ƒæ•°æ®
                "content": _txt,  # æ–‡æœ¬è¯»å–çš„å†…å®¹ï¼ˆMDæ ¼å¼ï¼‰
                "query": query,   # ç”¨æˆ·çš„é—®é¢˜
            }
            # éå†ç”Ÿæˆå™¨å¹¶ yield æ¯ä¸ª chunkï¼Œç¡®ä¿æµå¼è¿”å›
            try:
                for chunk in file_analysis_run.run_file_analysis_sync_stream(input_data):
                    yield chunk
            except Exception as e:
                logger.error(f"âŒ æ–‡ä»¶åˆ†ææµå¼å¤„ç†å¤±è´¥: {e}", exc_info=True)
                yield self._create_chunk("chatcmpl-error", f"æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}", chunk_type="text", finish_reason="stop", model="file-analysis-model")
            return
        else:
            e = ""
        # except Exception as e:
            # è¿”å›é”™è¯¯ä¿¡æ¯çš„ç”Ÿæˆå™¨ï¼Œä¸æµå¼APIå…¼å®¹
            def error_generator():
                yield self._create_chunk("chatcmpl-error", f"Error during file upload: {str(e)}", chunk_type="text", finish_reason="", model="file-analysis-model")
                yield self._create_chunk("chatcmpl-error", "", chunk_type="text", finish_reason="stop", model="file-analysis-model")

            return error_generator()

    def chat_with_knowledge_and_sql(self, user_id, session_id, query, knowledge_id, sql_id):
        """
        ç»“åˆçŸ¥è¯†åº“å’ŒSQLçš„æ™ºèƒ½é—®ç­”åŠŸèƒ½ï¼ˆæµå¼è¿”å›ï¼‰
    
        åŠŸèƒ½ï¼š
        1. é€šè¿‡knowledge_idè¿›è¡Œmilvusã€elasticsearchã€graphä¸‰ä¸ªæœç´¢å¼•æ“æœç´¢ä¸šåŠ¡æ•°æ®
        2. ç»“åˆç”¨æˆ·çš„é—®é¢˜ï¼Œåœ¨æœç´¢åˆ°çš„çŸ¥è¯†ä¸­ï¼Œæœç´¢ä¸é—®é¢˜ç›¸å…³çš„æŒ‡æ ‡å®ä½“
        3. å½¢æˆä¸‰å±‚å…³è”å…³ç³»ï¼š
           - ç¬¬ä¸€å±‚ï¼šç”¨æˆ·å’¨è¯¢çš„ä¸šåŠ¡é—®é¢˜ä¸­çš„å®ä½“
           - ç¬¬äºŒå±‚ï¼šä¸šåŠ¡é—®é¢˜ä¸­å…³è”çš„æŒ‡æ ‡å®ä½“
           - ç¬¬ä¸‰å±‚ï¼šæŒ‡æ ‡å®ä½“éœ€è¦çš„è®¡ç®—é€»è¾‘ï¼Œè¯¥é€»è¾‘è¦æ ‡å‡†æ˜ç¡®çš„é€»è¾‘è§„èŒƒ
    
        Args:
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID
            query: ç”¨æˆ·é—®é¢˜
            knowledge_id: çŸ¥è¯†åº“ID
            sql_id: SQLæ•°æ®åº“ID
        """
        # knowledge_id çŸ¥è¯†åº“æœç´¢ ä¸šåŠ¡æ•°æ®
        # sql_id SQLæ•°æ®åº“æœç´¢ ä¸šåŠ¡æ•°æ®
        
        _id = f"chatcmpl-{int(time.time())}"
        
        try:
            logger.info("ğŸ” æ‰§è¡ŒAgentic Queryæ™ºèƒ½ä½“æµç¨‹...")
            
            # ä½¿ç”¨é˜Ÿåˆ—æ”¶é›†æ­¥éª¤ç»“æœ
            step_queue = queue.Queue()
            final_query_result = None
            
            # å®šä¹‰æ­¥éª¤å›è°ƒå‡½æ•°
            def step_callback(step_name: str, step_data: Dict[str, Any]):
                """æ­¥éª¤å›è°ƒå‡½æ•°ï¼Œå°†æ¯ä¸ªæ­¥éª¤çš„ç»“æœæ”¾å…¥é˜Ÿåˆ—"""
                try:
                    step_queue.put({
                        "step_name": step_name,
                        "step_data": step_data
                    })
                except Exception as e:
                    logger.error(f"âš ï¸ æ­¥éª¤å›è°ƒå¤„ç†å¤±è´¥ ({step_name}): {e}")
                    traceback.print_exc()
            
            # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œæ™ºèƒ½ä½“æµç¨‹
            def run_agentic_query_flow():
                """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œæ™ºèƒ½ä½“æµç¨‹"""
                nonlocal final_query_result
                try:
                    final_query_result = agentic_query_run.run_agentic_query(
                        knowledge_id=knowledge_id,
                        query=query,
                        sql_id=sql_id,
                        user_id=user_id,
                        step_callback=step_callback
                    )
                    # å‘é€å®Œæˆä¿¡å·
                    step_queue.put({"step_name": "query_completed", "step_data": {"success": True}})
                except Exception as e:
                    logger.error(f"âŒ Agentic Queryæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
                    traceback.print_exc()
                    step_queue.put({"step_name": "query_completed", "step_data": {"success": False, "error": str(e)}})
            
            # å¯åŠ¨åå°çº¿ç¨‹
            query_thread = threading.Thread(target=run_agentic_query_flow)
            query_thread.daemon = True
            query_thread.start()
            
            # æ ¼å¼åŒ–æ­¥éª¤ä¿¡æ¯çš„æ˜ å°„
            step_messages = {
                "step_1_decision": "ğŸ” æ­¥éª¤1: å†³ç­–æ™ºèƒ½ä½“ - åˆ†æå®ä½“æœ¬æºã€æŒ‡æ ‡ã€å±æ€§ç­‰",
                "step_2_search": "ğŸ” æ­¥éª¤2: åŒå¼•æ“æœç´¢ - åˆæ­¥æŸ¥è¯¢çŸ¥è¯†åº“",
                "step_3_evaluation": "ğŸ“Š æ­¥éª¤3: ç»“æœè¯„ä¼° - åˆ†ææœç´¢ç»“æœè´¨é‡",
                "step_4_expanded_search": "ğŸ” æ­¥éª¤4: æ‰©å±•æœç´¢",
                "step_5_artifact": "ğŸ“‹ æ­¥éª¤5: Artifactå¤„ç† - åˆ†ç¦»æ¸…æ´—å†…å®¹å’ŒåŸå§‹å†…å®¹",
                "step_6_dynamic_prompt": "ğŸ“ æ­¥éª¤6: åŠ¨æ€ç”ŸæˆSystem Prompt",
                "step_7_query_enhancement": "âœ¨ æ­¥éª¤7: æŸ¥è¯¢å¢å¼º - å®Œå–„ç”¨æˆ·é—®é¢˜"
            }
            
            # æµå¼è¿”å›æ­¥éª¤ä¿¡æ¯
            while True:
                try:
                    item = step_queue.get(timeout=1)
                    step_name = item.get("step_name")
                    step_data = item.get("step_data", {})
                    
                    if step_name == "completed":
                        break
                    
                    # æ˜¾ç¤ºæ­¥éª¤ä¿¡æ¯
                    if step_name in step_messages:
                        step_msg = step_messages[step_name]
                        if step_data.get("success"):
                            step_content = f"{step_msg}\nâœ… å®Œæˆ"
                            if step_name == "step_1_decision":
                                entity_analysis = step_data.get("entity_analysis", {})
                                entities_count = len(entity_analysis.get("entities", []))
                                metrics_count = len(entity_analysis.get("metrics", []))
                                attributes_count = len(entity_analysis.get("attributes", []))
                                step_content += f"\n- è¯†åˆ«åˆ° {entities_count} ä¸ªå®ä½“ã€{metrics_count} ä¸ªæŒ‡æ ‡ã€{attributes_count} ä¸ªå±æ€§"
                            elif step_name == "step_2_search":
                                step_content += f"\n- å…±æ‰¾åˆ° {step_data.get('total_count', 0)} ä¸ªæœç´¢ç»“æœ"
                            elif step_name == "step_3_evaluation":
                                quality_score = step_data.get("quality_score", 0.0)
                                step_content += f"\n- è´¨é‡è¯„åˆ†: {quality_score:.3f}"
                            elif step_name == "step_7_query_enhancement":
                                enhanced_query = step_data.get("enhanced_query", "")
                                if enhanced_query:
                                    step_content += f"\n- å¢å¼ºåçš„æŸ¥è¯¢: {enhanced_query}"
                            
                            chunk = self._create_chunk(_id, content=step_content, chunk_type="text", finish_reason="")
                            yield chunk
                        else:
                            error_msg = step_data.get("error", "æœªçŸ¥é”™è¯¯")
                            step_content = f"{step_msg}\nâŒ å¤±è´¥: {error_msg}"
                            chunk = self._create_chunk(_id, content=step_content, chunk_type="text", finish_reason="")
                            yield chunk
                    
                except queue.Empty:
                    continue
            
            # ç­‰å¾…çº¿ç¨‹å®Œæˆ
            query_thread.join(timeout=30)
            
            # æ£€æŸ¥Agentic Queryç»“æœ
            if not final_query_result or not final_query_result.get("success"):
                error_msg = final_query_result.get("error", "æœªçŸ¥é”™è¯¯") if final_query_result else "æµç¨‹æ‰§è¡Œå¤±è´¥"
                chunk = self._create_chunk(_id, content=f"âŒ Agentic Queryæµç¨‹å¤±è´¥: {error_msg}", chunk_type="text", finish_reason="stop")
                yield chunk
                return
            
            # è·å–å¢å¼ºåçš„æŸ¥è¯¢
            enhanced_query = final_query_result.get("enhanced_query", query)
            calculation_descriptions = final_query_result.get("calculation_descriptions", [])
            system_prompt = final_query_result.get("system_prompt", "")
            
            # æ˜¾ç¤ºAgentic Queryç»“æœæ‘˜è¦
            result_content = "## ğŸ“Š Agentic Queryç»“æœ\n\n"
            result_content += f"**åŸå§‹æŸ¥è¯¢ï¼š**\n{query}\n\n"
            result_content += f"**å¢å¼ºåçš„æŸ¥è¯¢ï¼š**\n{enhanced_query}\n\n"
            
            if calculation_descriptions:
                result_content += "**è®¡ç®—æè¿°ï¼š**\n"
                for i, calc_desc in enumerate(calculation_descriptions, 1):
                    if isinstance(calc_desc, dict):
                        attr = calc_desc.get("attribute", "")
                        calc = calc_desc.get("calculation", "")
                        desc = calc_desc.get("description", "")
                        result_content += f"{i}. **{attr}**: {calc}\n   {desc}\n\n"
                    else:
                        result_content += f"{i}. {calc_desc}\n\n"
            
            if system_prompt:
                result_content += f"**åŠ¨æ€ç”Ÿæˆçš„System Promptï¼š**\n```\n{system_prompt}\n```\n\n"
            
            chunk = self._create_chunk(_id, content=result_content, chunk_type="text", finish_reason="")
            yield chunk
            logger.info(f"âœ… Agentic Queryæµç¨‹å®Œæˆï¼Œå¼€å§‹æ‰§è¡ŒSQLæŸ¥è¯¢...")
            
            # å¦‚æœæ²¡æœ‰sql_idï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢
            if not sql_id:
                chunk = self._create_chunk(_id, content="\nâš ï¸ æœªæä¾›SQLæ•°æ®åº“IDï¼Œæ— æ³•æ‰§è¡ŒSQLæŸ¥è¯¢", chunk_type="text", finish_reason="stop")
                yield chunk
                return
            
            # æ­¥éª¤2: ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢æ‰§è¡ŒSQLæŸ¥è¯¢
            logger.info(f"ğŸ” æ‰§è¡ŒSQLæŸ¥è¯¢ï¼Œä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢: {enhanced_query}")
            
            # è·å–æ•°æ®åº“ä¿¡æ¯
            db_info = self._get_database_info(sql_id)
        
            # ä½¿ç”¨é˜Ÿåˆ—æ”¶é›†SQLæ­¥éª¤ç»“æœ
            sql_step_queue = queue.Queue()
            final_sql_workflow_result = None
            
            # å®šä¹‰SQLæ­¥éª¤å›è°ƒå‡½æ•°
            def sql_step_callback(step_name: str, step_data: Dict[str, Any]):
                """SQLæ­¥éª¤å›è°ƒå‡½æ•°"""
                try:
                    sql_step_queue.put({
                        "step_name": step_name,
                        "step_data": step_data
                    })
                except Exception as e:
                    logger.error(f"âš ï¸ SQLæ­¥éª¤å›è°ƒå¤„ç†å¤±è´¥ ({step_name}): {e}")
                    traceback.print_exc()
            
            # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒSQLæ™ºèƒ½ä½“æµç¨‹
            def run_sql_agentic_search():
                """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒSQLæ™ºèƒ½ä½“æµç¨‹"""
                nonlocal final_sql_workflow_result
                try:
                    final_sql_workflow_result = agentic_sql_run.run_sql_agentic_search(
                        sql_id=sql_id,
                        query=enhanced_query,  # ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
                user_id=user_id,
                        step_callback=sql_step_callback
                    )
                    # å‘é€å®Œæˆä¿¡å·
                    sql_step_queue.put({"step_name": "sql_completed", "step_data": {"success": True}})
                except Exception as e:
                    logger.error(f"âŒ SQLæ™ºèƒ½ä½“æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
                    traceback.print_exc()
                    sql_step_queue.put({"step_name": "sql_completed", "step_data": {"success": False, "error": str(e)}})
            
            # å¯åŠ¨SQLåå°çº¿ç¨‹
            sql_thread = threading.Thread(target=run_sql_agentic_search)
            sql_thread.daemon = True
            sql_thread.start()
            
            # æµå¼å¤„ç†SQLæ­¥éª¤ç»“æœï¼ˆè·³è¿‡SQLç”Ÿæˆæµç¨‹çš„ä¸­é—´æ­¥éª¤ï¼‰
            sql_completed = False
            while not sql_completed:
                try:
                    sql_item = sql_step_queue.get(timeout=1.0)
                    sql_step_name = sql_item.get("step_name")
                    sql_step_data = sql_item.get("step_data", {})
                    
                    if sql_step_name == "sql_completed":
                        sql_completed = True
                        if not sql_step_data.get("success"):
                            error_msg = sql_step_data.get("error", "SQLæŸ¥è¯¢å¤±è´¥")
                            chunk = self._create_chunk(_id, content=f"\n## âŒ SQLæŸ¥è¯¢å¤±è´¥\n\n**é”™è¯¯ä¿¡æ¯:** {error_msg}", chunk_type="text", finish_reason="stop")
                            yield chunk
                            return
                        
                    
                    # è·³è¿‡SQLç”Ÿæˆæµç¨‹çš„ä¸­é—´æ­¥éª¤ï¼Œåªè¾“å‡ºæœ€ç»ˆç»“æœ
                    if sql_step_name in ["sql_flow_step_1_generation", "sql_flow_step_2_check_run", 
                                         "sql_flow_step_3_correction", "sql_flow_step_4_optimization",
                                         "sql_flow_step_5_recheck_run", "sql_flow_step_6_verification"]:
                        continue
                    
                    # æ˜¾ç¤ºSQLæ­¥éª¤ä¿¡æ¯ï¼ˆç®€åŒ–æ˜¾ç¤ºï¼‰
                    if sql_step_name == "step_3_sql_generation":
                        if sql_step_data.get("success") and sql_step_data.get("sql"):
                            sql_content = sql_step_data.get("sql", "")
                            step_content = f"\n## ğŸ’» SQLç”Ÿæˆç»“æœ\n\n**ç”Ÿæˆçš„SQLï¼š**\n```sql\n{sql_content}\n```\n"
                            execution_result = sql_step_data.get("execution_result", {})
                            if execution_result and execution_result.get("executed"):
                                row_count = execution_result.get("row_count", 0)
                                step_content += f"\n**æ‰§è¡Œç»“æœï¼š** âœ… æˆåŠŸæ‰§è¡Œï¼Œè¿”å› {row_count} è¡Œæ•°æ®\n"
                            chunk = self._create_chunk(_id, content=step_content, chunk_type="text", finish_reason="")
                            yield chunk
                    
                except queue.Empty:
                    continue
                except Exception as e:
                    logger.error(f"âš ï¸ å¤„ç†SQLæ­¥éª¤ç»“æœå¤±è´¥: {e}")
                    traceback.print_exc()
                    break
            
            # ç­‰å¾…SQLçº¿ç¨‹å®Œæˆ
            sql_thread.join(timeout=300)
            
            # è·å–SQLæŸ¥è¯¢ç»“æœ
            sql_workflow_result = final_sql_workflow_result
            if not sql_workflow_result or not sql_workflow_result.get("success"):
                error_msg = sql_workflow_result.get("error", "SQLæŸ¥è¯¢å¤±è´¥") if sql_workflow_result else "æœªè·å–åˆ°SQLæŸ¥è¯¢ç»“æœ"
                chunk = self._create_chunk(_id, content=f"\n## âŒ SQLæŸ¥è¯¢å¤±è´¥\n\n**é”™è¯¯ä¿¡æ¯:** {error_msg}", chunk_type="text", finish_reason="stop")
                yield chunk
                return
            
            # ä»sql_workflow_resultä¸­æå–SQLå’Œåˆ—ä¿¡æ¯
            sql = sql_workflow_result.get("sql", "")
            columns_with_description = sql_workflow_result.get("columns_with_description", [])
            columns_with_table_prefix = sql_workflow_result.get("columns_with_table_prefix", [])
            logical_calculations = sql_workflow_result.get("logical_calculations", [])
            execution_result = sql_workflow_result.get("execution_result", {})
            
            if not sql:
                logger.warning("âš ï¸ SQLä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œ")
                chunk = self._create_chunk(_id, content="\n## âš ï¸ æ‰§è¡Œå¤±è´¥\n\n**é”™è¯¯:** SQLä¸ºç©ºï¼Œæ— æ³•æ‰§è¡ŒæŸ¥è¯¢", chunk_type="text", finish_reason="stop")
                yield chunk
                return
            
            logger.info(f"âœ… æ‰§è¡ŒSQL: {sql[:100]}...")
            
            # ä»columns_with_descriptionä¸­æå–åˆ—ä¿¡æ¯ï¼ˆä½¿ç”¨ table.col æ ¼å¼ï¼‰
            columns_desc = []
            columns_types = []
            columns_used = []
            
            # ä¼˜å…ˆä½¿ç”¨ columns_with_table_prefixï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if columns_with_table_prefix:
                # æ„å»ºåˆ—ååˆ°æè¿°çš„æ˜ å°„
                col_desc_map = {}
                col_type_map = {}
                for col in columns_with_description:
                    col_name_with_table = col.get("col_name_with_table", f"{col.get('table_name', '')}.{col.get('col_name', '')}")
                    col_desc_map[col_name_with_table] = col.get("col_description", col.get("col_name", ""))
                    col_type_map[col_name_with_table] = col.get("col_type", "unknown")
                
                # ä½¿ç”¨ columns_with_table_prefix ä¸­çš„åˆ—å
                for col_name_with_table in columns_with_table_prefix:
                    columns_used.append(col_name_with_table)
                    columns_types.append(col_type_map.get(col_name_with_table, "unknown"))
                    columns_desc.append(col_desc_map.get(col_name_with_table, col_name_with_table))
            else:
                # å¦‚æœæ²¡æœ‰ columns_with_table_prefixï¼Œä» columns_with_description æ„å»º
                for col in columns_with_description:
                    col_name = col.get("col_name", "")
                    col_name_with_table = col.get("col_name_with_table", f"{col.get('table_name', '')}.{col_name}")
                    col_type = col.get("col_type", "")
                    col_description = col.get("col_description", col_name)
                    
                    if col_name:
                        columns_used.append(col_name_with_table)
                        columns_types.append(col_type or "unknown")
                        columns_desc.append(col_description)
            
            # å¦‚æœæ— æ³•ä»columns_with_descriptionè·å–åˆ—ä¿¡æ¯ï¼Œå°è¯•ä»SQLæ‰§è¡Œç»“æœè·å–
            if not columns_used:
                logger.warning("âš ï¸ æ— æ³•ä»columns_with_descriptionè·å–åˆ—ä¿¡æ¯ï¼Œå°†ä»SQLæ‰§è¡Œç»“æœä¸­è·å–")
                try:
                    sql_url = db_info.get("db_host", "")
                    sql_port = str(db_info.get("db_port", ""))
                    sql_user = db_info.get("db_user", "")
                    sql_password = db_info.get("db_password", "")
                    sql_database = db_info.get("db_name", "")
                    sql_type = db_info.get("db_type")
                    conn, _ = self.sql_obj.connect_database(sql_url, sql_port, sql_type, sql_database, sql_user, sql_password)
                    cursor = conn.cursor()
                    cursor.execute(sql)
                    
                    if cursor.description:
                        col_name_mapping = {}
                        for col in columns_with_description:
                            orig_col = col.get("col_name", "")
                            col_with_table = col.get("col_name_with_table", f"{col.get('table_name', '')}.{orig_col}")
                            if orig_col:
                                col_name_mapping[orig_col.lower()] = col_with_table
                        
                        for desc in cursor.description:
                            orig_col_name = desc[0]
                            mapped_col_name = col_name_mapping.get(orig_col_name.lower(), orig_col_name)
                            columns_used.append(mapped_col_name)
                            columns_types.append(desc[1] or "unknown")
                            columns_desc.append(mapped_col_name)
                    
                    conn.close()
                except Exception as e:
                    logger.error(f"âš ï¸ è·å–åˆ—ä¿¡æ¯å¤±è´¥: {e}")
                    columns_desc = ["åˆ—1", "åˆ—2"]
                    columns_types = ["unknown", "unknown"]
            
            logger.info(f"ğŸ“Š æå–åˆ° {len(columns_used)} ä¸ªåˆ—")
            
            # æ‰§è¡ŒSQLå¹¶ä¿å­˜åˆ°CSV
            file_name = "conf/tmp/sandbox_files/" + uuid.uuid4().hex[:16]
            read_flag, max_num = self._read_data(sql, db_info, file_name, columns_desc, columns_types)
            
            if not read_flag or max_num == 0:
                logger.warning(f"âš ï¸ SQLæ‰§è¡Œå¤±è´¥æˆ–æ²¡æœ‰æ•°æ®")
                error_msg = execution_result.get('error', 'æœªçŸ¥é”™è¯¯') if execution_result else 'æœªçŸ¥é”™è¯¯'
                chunk = self._create_chunk(_id, content=f"\n## âš ï¸ SQLæ‰§è¡Œå¤±è´¥æˆ–æ²¡æœ‰æ•°æ®\n\n**é”™è¯¯:** {error_msg}", chunk_type="text", finish_reason="stop")
                yield chunk
                return
            
            logger.info(f"âœ… SQLæ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {max_num} è¡Œæ•°æ®")
            
                    # è¯»å–CSVæ–‡ä»¶å¹¶ç”ŸæˆHTMLè¡¨æ ¼
            csv_file_path = file_name + ".csv"
            html_table = self._csv_to_html_table(csv_file_path, max_num, max_rows=10)
            logger.info(f"ğŸ“Š ç”ŸæˆHTMLè¡¨æ ¼: {'æˆåŠŸ' if html_table else 'å¤±è´¥'}")
            
            # å¦‚æœç”Ÿæˆäº†HTMLè¡¨æ ¼ï¼Œæµå¼è¿”å›
            if html_table: 
                chunk = self._create_chunk(_id, content=html_table, chunk_type="html_table", finish_reason="")
                yield chunk
            
            # æ­¥éª¤3: é€»è¾‘è®¡ç®—å’Œè§£è¯»
            if logical_calculations:
                logger.info(f"ğŸ”¢ å¼€å§‹æ‰§è¡Œé€»è¾‘è®¡ç®—ï¼Œå…± {len(logical_calculations)} ä¸ªè®¡ç®—è§„åˆ™")
                try:
                    logic_result = agentic_sql_run.run_logic_calculation(
                        csv_file_path=csv_file_path,
                        query=enhanced_query,  # ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
                        logical_calculations=logical_calculations,
                        columns_desc=columns_desc,
                        columns_types=columns_types,
                        sql=sql
                    )
                    print("logic_result:", logic_result)
                    if logic_result.get("success"):
                        calculation_summary = logic_result.get("calculation_summary", "")
                        calculation_result = logic_result.get("calculation_result", {})
                        tools_used = logic_result.get("tools_used", [])
                        interpretation = logic_result.get("interpretation", {})
                        final_interpretation = logic_result.get("final_interpretation", {})
                        
                        if calculation_result or calculation_summary or interpretation or final_interpretation:
                            # æ„å»ºé€»è¾‘è®¡ç®—ç»“æœçš„æ˜¾ç¤ºå†…å®¹
                            logic_content = "\n\n## ğŸ”¢ é€»è¾‘è®¡ç®—ç»“æœ\n\n"
                            
                            # ä¼˜å…ˆæ˜¾ç¤ºæœ€ç»ˆç»¼åˆè§£è¯»
                            if final_interpretation:
                                overall_summary = final_interpretation.get("overall_summary", "")
                                question_answer = final_interpretation.get("question_answer", "")
                                key_findings = final_interpretation.get("key_findings", [])
                                business_insights = final_interpretation.get("business_insights", [])
                                limitations = final_interpretation.get("limitations", "")
                                next_steps = final_interpretation.get("next_steps", "")
                                
                                if overall_summary:
                                    logic_content += f"**ğŸ“Š æ•´ä½“æ€»ç»“ï¼š**\n{overall_summary}\n\n"
                                
                                if question_answer:
                                    logic_content += f"**â“ é—®é¢˜å›ç­”ï¼š**\n{question_answer}\n\n"
                                
                                if key_findings:
                                    logic_content += "**ğŸ” å…³é”®å‘ç°ï¼š**\n"
                                    for i, finding in enumerate(key_findings, 1):
                                        logic_content += f"{i}. {finding}\n"
                                    logic_content += "\n"
                                
                                if business_insights:
                                    logic_content += "**ğŸ’¡ ä¸šåŠ¡æ´å¯Ÿï¼š**\n"
                                    for i, insight in enumerate(business_insights, 1):
                                        logic_content += f"{i}. {insight}\n"
                                    logic_content += "\n"
                                
                                if limitations:
                                    logic_content += f"**âš ï¸ å±€é™æ€§è¯´æ˜ï¼š**\n{limitations}\n\n"
                                
                                if next_steps:
                                    logic_content += f"**ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®ï¼š**\n{next_steps}\n\n"
                            
                            if calculation_summary:
                                logic_content += f"**è®¡ç®—æ‘˜è¦ï¼š** {calculation_summary}\n\n"
                            
                            if tools_used:
                                logic_content += f"**ä½¿ç”¨çš„ç»Ÿè®¡å·¥å…·ï¼š** {', '.join(tools_used)}\n\n"
                            
                            # æ˜¾ç¤ºç»Ÿè®¡ç»“æœè§£è¯»
                            if interpretation:
                                interpretation_summary = interpretation.get("interpretation_summary", "")
                                key_insights = interpretation.get("key_insights", [])
                                detailed_interpretation = interpretation.get("detailed_interpretation", "")
                                
                                if interpretation_summary:
                                    logic_content += f"**ğŸ“ˆ ç»Ÿè®¡ç»“æœè§£è¯»æ‘˜è¦ï¼š**\n{interpretation_summary}\n\n"
                                
                                if key_insights:
                                    logic_content += "**å…³é”®æ´å¯Ÿï¼š**\n"
                                    for i, insight in enumerate(key_insights, 1):
                                        logic_content += f"{i}. {insight}\n"
                                    logic_content += "\n"
                                
                                if detailed_interpretation:
                                    logic_content += f"**è¯¦ç»†è§£è¯»ï¼š**\n{detailed_interpretation}\n\n"
                            
                            # æ˜¾ç¤ºåŸå§‹è®¡ç®—ç»“æœï¼ˆä½œä¸ºè¡¥å……ä¿¡æ¯ï¼‰
                            if calculation_result:
                                logic_content += "**åŸå§‹è®¡ç®—ç»“æœï¼š**\n```json\n"
                                logic_content += json.dumps(calculation_result, ensure_ascii=False, indent=2)
                                logic_content += "\n```\n"
                            
                            chunk = self._create_chunk(_id, content=logic_content, chunk_type="text", finish_reason="")
                            yield chunk
                            logger.info(f"âœ… é€»è¾‘è®¡ç®—å®Œæˆå¹¶è¿”å›ç»“æœï¼ˆåŒ…å«æœ€ç»ˆè§£è¯»ï¼‰")
                        else:
                            logger.warning("âš ï¸ é€»è¾‘è®¡ç®—ç»“æœä¸ºç©º")
                    else:
                        error_msg = logic_result.get("error", "é€»è¾‘è®¡ç®—å¤±è´¥")
                        chunk = self._create_chunk(_id, content=f"\n## âš ï¸ é€»è¾‘è®¡ç®—å¤±è´¥\n\n**é”™è¯¯:** {error_msg}", chunk_type="text", finish_reason="")
                        yield chunk
                except Exception as e:
                    logger.error(f"âŒ é€»è¾‘è®¡ç®—æ‰§è¡Œå¤±è´¥: {e}")
                    traceback.print_exc()
                    chunk = self._create_chunk(_id, content=f"\n## âš ï¸ é€»è¾‘è®¡ç®—æ‰§è¡Œå¤±è´¥\n\n**é”™è¯¯:** {str(e)}", chunk_type="text", finish_reason="")
                    yield chunk
            
            # æ¸…ç†CSVæ–‡ä»¶
            try:
                if os.path.exists(csv_file_path):
                    os.remove(csv_file_path)
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç†CSVæ–‡ä»¶å¤±è´¥: {e}")
            
            # è¿”å›å®Œæˆä¿¡å·
            chunk = self._create_chunk(_id, content="", chunk_type="text", finish_reason="stop")
            yield chunk
            logger.info(f"âœ… å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆ")
                
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(f"âŒ chat_with_knowledge_and_sql æ‰§è¡Œå¤±è´¥: {e}")
            logger.error(f"Traceback: {error_traceback}")
            chunk = self._create_chunk(_id, content=f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}", chunk_type="text", finish_reason="stop")
            yield chunk
                
    def chat_with_sql(self, user_id, session_id, query, sql_id):
        """æ™ºèƒ½SQLé—®æ•°ï¼šæ ¹æ®è‡ªç„¶è¯­è¨€ç”Ÿæˆå¹¶æ‰§è¡Œ SQLï¼Œè¿”å›è¡¨æ ¼ä¸é€»è¾‘è®¡ç®—è§£è¯»ã€‚"""
        _id = f"chatcmpl-{int(time.time())}"
        db_info = self._get_database_info(sql_id)
        if not db_info or not db_info.get("db_host"):
            chunk = self._create_chunk(
                _id,
                content="## âŒ æ™ºèƒ½é—®æ•°å¤±è´¥\n\n**é”™è¯¯:** æœªæ‰¾åˆ°è¯¥æ•°æ®æºæˆ–æ•°æ®åº“é…ç½®ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥ sql_id æˆ–æ•°æ®æºè¿æ¥ã€‚",
                chunk_type="text",
                finish_reason="stop",
            )
            yield chunk
            return

        try:
            # è°ƒç”¨å…¬å…±çš„SQLç”Ÿæˆå·¥ä½œæµå‡½æ•°ï¼ˆæ”¯æŒå¼‚æ­¥æµå¼è¿”å›ï¼‰
            logger.info("ğŸ” æ‰§è¡ŒSQLç”Ÿæˆå·¥ä½œæµ...")
            
            # ä½¿ç”¨é˜Ÿåˆ—æ”¶é›†æ­¥éª¤ç»“æœ
            step_queue = queue.Queue()
            final_sql_workflow_result = None
            
            # å®šä¹‰æ­¥éª¤å›è°ƒå‡½æ•°ï¼Œç”¨äºæ”¶é›†æ¯ä¸ªæ­¥éª¤çš„ç»“æœ
            def step_callback(step_name: str, step_data: Dict[str, Any]):
                """æ­¥éª¤å›è°ƒå‡½æ•°ï¼Œå°†æ¯ä¸ªæ­¥éª¤çš„ç»“æœæ”¾å…¥é˜Ÿåˆ—"""
                try:
                    step_queue.put({
                        "step_name": step_name,
                        "step_data": step_data
                    })
                except Exception as e:
                    logger.error(f"âš ï¸ æ­¥éª¤å›è°ƒå¤„ç†å¤±è´¥ ({step_name}): {e}")
                    traceback.print_exc()
            
            # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œæ™ºèƒ½ä½“æµç¨‹
            def run_agentic_search():
                """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œæ™ºèƒ½ä½“æµç¨‹"""
                nonlocal final_sql_workflow_result
                try:
                    final_sql_workflow_result = agentic_sql_run.run_sql_agentic_search(
                        sql_id=sql_id,
                        query=query,
                user_id=user_id,
                        step_callback=step_callback
                    )
                # å‘é€å®Œæˆä¿¡å·
                    step_queue.put({"step_name": "completed", "step_data": {"success": True}})
                except Exception as e:
                    logger.error(f"âŒ æ™ºèƒ½ä½“æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
                    traceback.print_exc()
                    step_queue.put({"step_name": "completed", "step_data": {"success": False, "error": str(e)}})
            
            agent_thread = threading.Thread(target=run_agentic_search)
            agent_thread.daemon = True
            agent_thread.start()

            step_messages = self._get_sql_step_display_titles()

            # æµå¼æ¶ˆè´¹é˜Ÿåˆ—ä¸­çš„æ­¥éª¤ç»“æœå¹¶æ¨é€ç»™å‰ç«¯
            completed = False
            while not completed:
                try:
                    # ä»é˜Ÿåˆ—ä¸­è·å–æ­¥éª¤ç»“æœï¼ˆè®¾ç½®è¶…æ—¶é¿å…æ— é™ç­‰å¾…ï¼‰
                    step_result = step_queue.get(timeout=1.0)
                    step_name = step_result.get("step_name")
                    step_data = step_result.get("step_data", {})
                    
                    if step_name == "completed":
                        completed = True
                        if not step_data.get("success"):
                            error_msg = step_data.get("error", "æ™ºèƒ½é—®æ•°å¤±è´¥")
                            chunk = self._create_chunk(_id, content=f"## âŒ æ™ºèƒ½é—®æ•°å¤±è´¥\n\n**é”™è¯¯ä¿¡æ¯:** {error_msg}", chunk_type="text", finish_reason="stop")
                            yield chunk
                            return
                        
                    
                    # è·³è¿‡SQLç”Ÿæˆæµç¨‹çš„ä¸­é—´æ­¥éª¤ï¼Œåªè¾“å‡ºæœ€ç»ˆç»“æœ
                    if step_name in ["sql_flow_step_1_generation", "sql_flow_step_2_check_run", 
                                     "sql_flow_step_3_correction", "sql_flow_step_4_optimization",
                                     "sql_flow_step_5_recheck_run", "sql_flow_step_6_verification"]:
                        continue  # è·³è¿‡è¿™äº›ä¸­é—´æ­¥éª¤çš„è¾“å‡º
                    
                    # æ ¼å¼åŒ–æ­¥éª¤ä¿¡æ¯ï¼ˆMarkdownæ ¼å¼ï¼‰
                    step_title = step_messages.get(step_name, f"## æ­¥éª¤: {step_name}")
                    content_parts = [f"{step_title}\n"]
                    
                    if step_data.get("success"):
                        # æ ¹æ®æ­¥éª¤ç±»å‹æ·»åŠ è¯¦ç»†ä¿¡æ¯
                        if step_name == "step_1_database_info":
                            content_parts.append("**æ‰§è¡Œç»“æœ:**\n")
                            content_parts.append(f"- **æ•°æ®åº“åç§°:** {step_data.get('database_name', '')}\n")
                            content_parts.append(f"- **æ•°æ®åº“ç±»å‹:** {step_data.get('database_type', '')}\n")
                        elif step_name == "step_1_2_metadata_query":
                            # æ•°æ®åº“å…ƒæ•°æ®æŸ¥è¯¢ç»“æœ
                            if step_data.get("is_metadata_query"):
                                query_type = step_data.get("query_type", "")
                                result_data = step_data.get("result", {})
                                message = step_data.get("message", "")
                                
                                content_parts.append("**å…ƒæ•°æ®æŸ¥è¯¢ç»“æœ:**\n")
                                content_parts.append(f"- **æŸ¥è¯¢ç±»å‹:** {query_type}\n")
                                
                                if message:
                                    content_parts.append(f"\n**æŸ¥è¯¢ç»“æœ:**\n{message}\n")
                                
                                # æ ¹æ®æŸ¥è¯¢ç±»å‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                                if query_type == "table_count":
                                    count = result_data.get("count", 0)
                                    content_parts.append(f"\næ•°æ®åº“ä¸­å…±æœ‰ **{count}** ä¸ªè¡¨\n")
                                elif query_type == "table_list":
                                    tables = result_data.get("tables", [])
                                    count = result_data.get("count", 0)
                                    content_parts.append(f"\næ•°æ®åº“ä¸­å…±æœ‰ **{count}** ä¸ªè¡¨ï¼š\n\n")
                                    for table in tables[:20]:  # æœ€å¤šæ˜¾ç¤º20ä¸ªè¡¨
                                        table_name = table.get("table_name", "")
                                        table_desc = table.get("table_description", "")
                                        if table_desc:
                                            content_parts.append(f"- **{table_name}**: {table_desc}\n")
                                        else:
                                            content_parts.append(f"- **{table_name}**\n")
                                    if count > 20:
                                        content_parts.append(f"\n... è¿˜æœ‰ {count - 20} ä¸ªè¡¨æœªæ˜¾ç¤º\n")
                                elif query_type == "column_count":
                                    table_name = result_data.get("table_name", "")
                                    count = result_data.get("count", 0)
                                    content_parts.append(f"\nè¡¨ **{table_name}** ä¸­å…±æœ‰ **{count}** ä¸ªåˆ—\n")
                                elif query_type == "column_list":
                                    table_name = result_data.get("table_name", "")
                                    columns = result_data.get("columns", [])
                                    count = result_data.get("count", 0)
                                    content_parts.append(f"\nè¡¨ **{table_name}** ä¸­å…±æœ‰ **{count}** ä¸ªåˆ—ï¼š\n\n")
                                    for col in columns[:30]:  # æœ€å¤šæ˜¾ç¤º30ä¸ªåˆ—
                                        col_name = col.get("column_name", "")
                                        col_type = col.get("column_type", "")
                                        col_comment = col.get("column_comment", "")
                                        if col_comment:
                                            content_parts.append(f"- **{col_name}** ({col_type}): {col_comment}\n")
                                        else:
                                            content_parts.append(f"- **{col_name}** ({col_type})\n")
                                    if count > 30:
                                        content_parts.append(f"\n... è¿˜æœ‰ {count - 30} ä¸ªåˆ—æœªæ˜¾ç¤º\n")
                                elif query_type == "table_description":
                                    table_name = result_data.get("table_name", "")
                                    table_desc = result_data.get("table_description", "")
                                    content_parts.append(f"\nè¡¨ **{table_name}** çš„æè¿°ï¼š\n{table_desc}\n")
                                elif query_type == "column_comment":
                                    table_name = result_data.get("table_name", "")
                                    column_name = result_data.get("column_name", "")
                                    column_type = result_data.get("column_type", "")
                                    column_comment = result_data.get("column_comment", "")
                                    content_parts.append(f"\nåˆ— **{table_name}.{column_name}** ({column_type}) çš„æ³¨é‡Šï¼š\n{column_comment}\n")
                                else:
                                    # æŸ¥è¯¢å¤±è´¥ï¼ˆè¡¨åæˆ–åˆ—åé”™è¯¯ï¼‰
                                    error_msg = step_data.get("error", "å…ƒæ•°æ®æŸ¥è¯¢å¤±è´¥")
                                    error_message = step_data.get("error_message", error_msg)
                                    available_tables = step_data.get("available_tables", [])
                                    available_columns = step_data.get("available_columns", [])
                                    
                                    content_parts.append("**å…ƒæ•°æ®æŸ¥è¯¢é”™è¯¯:**\n")
                                    content_parts.append(f"- **æŸ¥è¯¢ç±»å‹:** {query_type}\n")
                                    content_parts.append(f"- **é”™è¯¯ä¿¡æ¯:** {error_msg}\n\n")
                                    
                                    if error_message:
                                        content_parts.append(f"**è¯¦ç»†é”™è¯¯è¯´æ˜:**\n{error_message}\n\n")
                                    
                                    if available_tables:
                                        content_parts.append(f"**å¯ç”¨çš„è¡¨ååˆ—è¡¨ï¼ˆå…± {len(available_tables)} ä¸ªï¼‰:**\n")
                                        for i, table_name in enumerate(available_tables[:30], 1):
                                            content_parts.append(f"{i}. `{table_name}`\n")
                                        if len(available_tables) > 30:
                                            content_parts.append(f"\n... è¿˜æœ‰ {len(available_tables) - 30} ä¸ªè¡¨æœªæ˜¾ç¤º\n")
                                        content_parts.append("\n")
                                    
                                    if available_columns:
                                        content_parts.append(f"**å¯ç”¨çš„åˆ—ååˆ—è¡¨ï¼ˆå…± {len(available_columns)} ä¸ªï¼‰:**\n")
                                        for i, col_name in enumerate(available_columns[:30], 1):
                                            content_parts.append(f"{i}. `{col_name}`\n")
                                        if len(available_columns) > 30:
                                            content_parts.append(f"\n... è¿˜æœ‰ {len(available_columns) - 30} ä¸ªåˆ—æœªæ˜¾ç¤º\n")
                                        content_parts.append("\n")
                            else:
                                content_parts.append("**æ£€æŸ¥ç»“æœ:** ä¸æ˜¯æ•°æ®åº“å…ƒæ•°æ®æŸ¥è¯¢ï¼Œç»§ç»­åç»­æµç¨‹\n")
                        elif step_name == "step_1_5_query_decomposition":
                            # é—®é¢˜æ‹†è§£ä¸é€»è¾‘åˆ†æç»“æœ
                            content_parts.append("**æ‹†è§£ç»“æœ:**\n")
                            content_parts.append(f"- **å®ä½“:** {step_data.get('entities_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **æŒ‡æ ‡:** {step_data.get('metrics_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **æ—¶é—´ç»´åº¦:** {step_data.get('time_dimensions_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **å…³è”å…³ç³»:** {step_data.get('relationships_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **é€»è¾‘è®¡ç®—:** {step_data.get('logical_calculations_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **ç©ºé—´ç»´åº¦:** {step_data.get('spatial_dimensions_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **é›†åˆè®ºå…³ç³»:** {step_data.get('set_theory_relations_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **å…³ç³»ä»£æ•°:** {step_data.get('relational_algebra_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **å›¾è®ºå…³ç³»:** {step_data.get('graph_theory_relations_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **é€»è¾‘æ¨ç†:** {step_data.get('logical_reasoning_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **è¯­ä¹‰ç½‘ç»œ:** {step_data.get('semantic_network_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **æ•°å­¦å…³ç³»:** {step_data.get('mathematical_relations_count', 0)} ä¸ª\n")
                            
                            if step_data.get('analysis_summary'):
                                content_parts.append(f"\n**åˆ†ææ€»ç»“:**\n{step_data.get('analysis_summary')}\n")
                        elif step_name == "step_1_5_sql_intent_recognition":
                            # SQLæ„å›¾è¯†åˆ«ç»“æœ
                            intent_type_message = step_data.get('intent_type_message', '')
                            if intent_type_message:
                                content_parts.append(f"{intent_type_message}\n\n")
                            content_parts.append("**è¯†åˆ«ç»“æœ:**\n")
                            content_parts.append(f"- **æ˜¯å¦æ˜¯SQLé—®æ•°:** {'âœ… æ˜¯' if step_data.get('is_sql_query', False) else 'âŒ å¦'}\n")
                            content_parts.append(f"- **ç½®ä¿¡åº¦:** `{step_data.get('confidence', 0):.2%}`\n")
                            if step_data.get('search_target'):
                                content_parts.append(f"- **æœç´¢ç›®æ ‡:** {step_data.get('search_target', '')}\n")
                        elif step_name == "step_1_5_sql_intent_recognition_warning":
                            # SQLæ„å›¾è¯†åˆ«è­¦å‘Š
                            warning = step_data.get('warning', '')
                            if warning:
                                content_parts.append(f"> âš ï¸ {warning}\n")
                        elif step_name == "step_1_5_sql_intent_recognition_final":
                            # SQLæ„å›¾è¯†åˆ«æœ€ç»ˆç»“æœï¼ˆä¸æ˜¯SQLé—®æ•°ï¼‰
                            message = step_data.get('message', '')
                            if message:
                                content_parts.append(f"{message}\n")
                        elif step_name == "step_2_intent_recognition":
                            content_parts.append("**è¯†åˆ«ç»“æœ:**\n")
                            content_parts.append(f"- **æŸ¥è¯¢ç±»å‹:** `{step_data.get('query_type', '')}`\n")
                            content_parts.append(f"- **æœ¬æºä¸»ä½“:** {step_data.get('primary_entities_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **ä¸»ä½“å±æ€§:** {step_data.get('entity_attributes_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **ä¸»ä½“æŒ‡æ ‡:** {step_data.get('entity_metrics_count', 0)} ä¸ª\n")
                            content_parts.append(f"- **æ—¶é—´ç»´åº¦:** {step_data.get('time_dimensions_count', 0)} ä¸ª\n")
                            
                            # æ˜¾ç¤ºç›¸å…³è¡¨
                            relevant_tables = step_data.get('relevant_tables', [])
                            if relevant_tables:
                                table_names = [t.get('table_name', '') for t in relevant_tables if t.get('table_name')]
                                if table_names:
                                    content_parts.append(f"\n**ç›¸å…³è¡¨:** {len(relevant_tables)} ä¸ª\n")
                                    tables_str = ', '.join([f"`{name}`" for name in table_names[:10]])
                                    content_parts.append(f"- {tables_str}\n")
                            
                            # æ˜¾ç¤ºç›¸å…³åˆ—ï¼ˆæŒ‰è¡¨åˆ†ç»„ï¼‰
                            relevant_columns = step_data.get('relevant_columns', [])
                            if relevant_columns:
                                columns_by_table = {}
                                for col in relevant_columns:
                                    table_name = col.get('table_name', '')
                                    col_name = col.get('col_name', '')
                                    if table_name and col_name:
                                        if table_name not in columns_by_table:
                                            columns_by_table[table_name] = []
                                        columns_by_table[table_name].append(col_name)
                                
                                if columns_by_table:
                                    content_parts.append(f"\n**ç›¸å…³åˆ—:** {len(relevant_columns)} ä¸ª\n")
                                    for table_name, cols in list(columns_by_table.items())[:5]:
                                        cols_str = ', '.join([f"`{c}`" for c in cols[:10]])
                                        content_parts.append(f"- `{table_name}`: {cols_str}\n")
                        elif step_name == "step_3_schema_search":
                            content_parts.append("**æœç´¢ç»“æœ:**\n")
                            content_parts.append(f"- **æ‰¾åˆ°ç›¸å…³è¡¨:** **{step_data.get('results_count', 0)}** ä¸ª\n")
                            if step_data.get('relevant_tables'):
                                tables = step_data.get('relevant_tables', [])[:5]
                                tables_str = ', '.join([f"`{t.get('table_name', '')}`" for t in tables])
                                content_parts.append(f"- **ç›¸å…³è¡¨:** {tables_str}\n")
                            
                            # æ˜¾ç¤ºéªŒè¯ç»“æœ
                            validation_result = step_data.get('validation_result', {})
                            is_sufficient = step_data.get('is_sufficient', True)
                            
                            if not is_sufficient:
                                content_parts.append("\n**âš ï¸ éªŒè¯ç»“æœ:**\n")
                                content_parts.append("å½“å‰æ‰¾åˆ°çš„è¡¨ä¸è¶³ä»¥å®Œæ•´æè¿°ç”¨æˆ·æŸ¥è¯¢éœ€æ±‚ã€‚\n\n")
                                
                                missing_entities = validation_result.get('missing_entities', [])
                                missing_attributes = validation_result.get('missing_attributes', [])
                                missing_metrics = validation_result.get('missing_metrics', [])
                                missing_time_fields = validation_result.get('missing_time_fields', [])
                                
                                if missing_entities:
                                    entities_str = ', '.join(missing_entities)
                                    content_parts.append(f"- **ç¼ºå¤±çš„å®ä½“:** {entities_str}\n")
                                
                                if missing_attributes:
                                    attrs_str = ', '.join([a.get('name', a.get('col_name', '')) for a in missing_attributes])
                                    content_parts.append(f"- **ç¼ºå¤±çš„å±æ€§:** {attrs_str}\n")
                                
                                if missing_metrics:
                                    metrics_str = ', '.join([m.get('name', m.get('col_name', '')) for m in missing_metrics])
                                    content_parts.append(f"- **ç¼ºå¤±çš„æŒ‡æ ‡:** {metrics_str}\n")
                                
                                if missing_time_fields:
                                    time_str = ', '.join([t.get('name', t.get('col_name', '')) for t in missing_time_fields])
                                    content_parts.append(f"- **ç¼ºå¤±çš„æ—¶é—´å­—æ®µ:** {time_str}\n")
                                
                                suggestion = validation_result.get('suggestion', '')
                                if suggestion:
                                    content_parts.append(f"\n**å»ºè®®:**\n{suggestion}\n")
                            else:
                                content_parts.append("\n**âœ… éªŒè¯ç»“æœ:**\n")
                                content_parts.append("å½“å‰æ‰¾åˆ°çš„è¡¨å·²ç»è¶³å¤Ÿæè¿°ç”¨æˆ·æŸ¥è¯¢éœ€æ±‚ã€‚\n")
                        elif step_name == "step_4_column_search":
                            content_parts.append("**åˆ—æœç´¢ç»“æœ:**\n")
                            content_parts.append(f"- **æ‰¾åˆ°ç›¸å…³åˆ—:** **{step_data.get('total_columns_count', 0)}** ä¸ª\n")
                        elif step_name == "step_3_sql_generation":
                            content_parts.append("**SQLç”Ÿæˆç»“æœ:**\n")
                            if step_data.get('sql'):
                                sql_content = step_data.get('sql', '')
                                content_parts.append("**ç”Ÿæˆçš„SQL:**\n")
                                content_parts.append(f"```sql\n{sql_content}\n```\n")
                            
                            execution_result = step_data.get('execution_result', {})
                            if execution_result:
                                if execution_result.get('executed'):
                                    row_count = execution_result.get('row_count', 0)
                                    content_parts.append(f"\n**æ‰§è¡Œç»“æœ:** âœ… æˆåŠŸæ‰§è¡Œï¼Œè¿”å› {row_count} è¡Œæ•°æ®\n")
                                else:
                                    error = execution_result.get('error', '')
                                    content_parts.append(f"\n**æ‰§è¡Œç»“æœ:** âŒ æ‰§è¡Œå¤±è´¥ - {error}\n")
                            
                            # æ˜¾ç¤ºåˆ—æè¿°ä¿¡æ¯
                            columns_with_description = step_data.get('columns_with_description', [])
                            if columns_with_description:
                                content_parts.append(f"\n**ä½¿ç”¨çš„åˆ—åŠå…¶æè¿°:**\n")
                                # æŒ‰è¡¨åˆ†ç»„æ˜¾ç¤º
                                columns_by_table = {}
                                for col in columns_with_description:
                                    table_name = col.get('table_name', '')
                                    if table_name not in columns_by_table:
                                        columns_by_table[table_name] = []
                                    columns_by_table[table_name].append(col)
                                
                                for table_name, cols in columns_by_table.items():
                                    content_parts.append(f"\n**è¡¨ `{table_name}`:**\n")
                                    for col in cols:
                                        col_name = col.get('col_name', '')
                                        col_type = col.get('col_type', '')
                                        col_desc = col.get('col_description', col_name)
                                        content_parts.append(f"- `{col_name}` ({col_type}): {col_desc}\n")
                            
                            # æ˜¾ç¤ºé€»è¾‘è®¡ç®—ä¿¡æ¯
                            logical_calculations = step_data.get('logical_calculations', [])
                            if logical_calculations:
                                content_parts.append(f"\n**éœ€è¦çš„è®¡ç®—:**\n")
                                for lc in logical_calculations:
                                    operation = lc.get('logical_operation', '')
                                    operands = lc.get('operands', [])
                                    description = lc.get('description', '')
                                    if operation:
                                        operands_str = ', '.join([str(op) for op in operands]) if operands else ''
                                        content_parts.append(f"- **{operation}**")
                                        if operands_str:
                                            content_parts.append(f" ({operands_str})")
                                        if description:
                                            content_parts.append(f": {description}")
                                        content_parts.append("\n")
                            
                            is_satisfied = step_data.get('is_satisfied', True)
                            satisfaction_score = step_data.get('satisfaction_score', 1.0)
                            if satisfaction_score < 1.0 or not is_satisfied:
                                content_parts.append(f"\n**æ»¡è¶³åº¦:** {satisfaction_score:.2f} ({'âœ… æ»¡è¶³' if is_satisfied else 'âŒ ä¸æ»¡è¶³'}ç”¨æˆ·éœ€æ±‚)\n")
                        elif step_name == "step_5_sql_check":
                            content_parts.append("**SQLæ£€æŸ¥ç»“æœ:**\n")
                            is_valid = step_data.get('is_valid', False)
                            is_safe = step_data.get('is_safe', True)
                            content_parts.append(f"- **è¯­æ³•æ­£ç¡®:** {'âœ… æ˜¯' if is_valid else 'âŒ å¦'}\n")
                            content_parts.append(f"- **å®‰å…¨æ€§:** {'âœ… å®‰å…¨' if is_safe else 'âš ï¸ ä¸å®‰å…¨'}\n")
                            if step_data.get('errors'):
                                errors = step_data.get('errors', [])
                                content_parts.append(f"\n**é”™è¯¯:**\n")
                                for err in errors:
                                    content_parts.append(f"- {err}\n")
                            if step_data.get('warnings'):
                                warnings = step_data.get('warnings', [])
                                content_parts.append(f"\n**è­¦å‘Š:**\n")
                                for warn in warnings:
                                    content_parts.append(f"- {warn}\n")
                            if step_data.get('corrected_sql') and step_data.get('corrected_sql') != step_data.get('sql', ''):
                                corrected_sql = step_data.get('corrected_sql', '')
                                content_parts.append(f"\n**ä¿®æ­£åçš„SQL:**\n")
                                content_parts.append(f"```sql\n{corrected_sql}\n```\n")
                        elif step_name == "step_final_result":
                            content_parts.append("**æœ€ç»ˆç»“æœ:**\n")
                            if step_data.get('sql'):
                                sql_content = step_data.get('sql', '')
                                content_parts.append("**æœ€ç»ˆSQL:**\n")
                                content_parts.append(f"```sql\n{sql_content}\n```\n")
                        elif step_name == "step_4_evaluation":
                            content_parts.append("**è¯„ä¼°ç»“æœ:**\n")
                            quality_score = step_data.get('quality_score', 0)
                            content_parts.append(f"- **è´¨é‡è¯„åˆ†:** `{quality_score:.2f}`\n")
                            is_satisfactory = step_data.get('is_satisfactory', False)
                            content_parts.append(f"- **æ˜¯å¦æ»¡æ„:** {'âœ… æ˜¯' if is_satisfactory else 'âŒ å¦'}\n")
                        elif step_name == "step_5_expansion":
                            content_parts.append("**æ‰©å±•ç»“æœ:**\n")
                            content_parts.append(f"- **æ‰©å±•è½®æ•°:** {step_data.get('expansion_rounds', 0)} è½®\n")
                            content_parts.append(f"- **åˆå¹¶ç»“æœ:** **{step_data.get('merged_results_count', 0)}** ä¸ªè¡¨\n")
                        elif step_name == "step_6_rerank":
                            content_parts.append("**é‡æ’åºç»“æœ:**\n")
                            if step_data.get('success'):
                                content_parts.append(f"- **é‡æ’åºæˆåŠŸ:** âœ…\n")
                                if step_data.get('reranked_count'):
                                    content_parts.append(f"- **é‡æ’åºåç»“æœæ•°:** **{step_data.get('reranked_count', 0)}** ä¸ª\n")
                            else:
                                content_parts.append(f"- **é‡æ’åºçŠ¶æ€:** âš ï¸ {step_data.get('message', 'é‡æ’åºå¤±è´¥')}\n")
                        elif step_name == "step_7_artifact":
                            content_parts.append("**Artifactå¤„ç†ç»“æœ:**\n")
                            artifacts_count = step_data.get('artifacts_count', 0)
                            content_parts.append(f"- **Artifactæ•°é‡:** **{artifacts_count}** ä¸ª\n")
                        elif step_name == "step_8_system_prompt":
                            content_parts.append("**System Promptç”Ÿæˆç»“æœ:**\n")
                            prompt_length = step_data.get('prompt_length', 0)
                            content_parts.append(f"- **Prompté•¿åº¦:** **{prompt_length}** å­—ç¬¦\n")
                        elif step_name == "step_9_enhance_results":
                            content_parts.append("**å¢å¼ºæœç´¢ç»“æœ:**\n")
                            enhanced_count = step_data.get('enhanced_results_count', 0)
                            content_parts.append(f"- **å¢å¼ºç»“æœæ•°:** **{enhanced_count}** ä¸ª\n")
                        elif step_name == "step_10_schema_selection":
                            content_parts.append("**é€‰æ‹©ç»“æœ:**\n")
                            content_parts.append(f"- **é€‰ä¸­è¡¨æ•°é‡:** **{step_data.get('total_tables', 0)}** ä¸ª\n")
                            content_parts.append(f"- **é€‰ä¸­åˆ—æ•°é‡:** **{step_data.get('total_columns', 0)}** ä¸ª\n")
                            if step_data.get('selected_tables'):
                                tables_str = ', '.join(step_data.get('selected_tables', [])[:5])
                                content_parts.append(f"- **é€‰ä¸­çš„è¡¨:** `{tables_str}`\n")
                        elif step_name == "step_11_validation":
                            content_parts.append("**æ ¡éªŒç»“æœ:**\n")
                            is_valid = step_data.get('is_valid', False)
                            content_parts.append(f"- **æ ¡éªŒé€šè¿‡:** {'âœ… æ˜¯' if is_valid else 'âŒ å¦'}\n")
                            validation_score = step_data.get('validation_score', 0)
                            content_parts.append(f"- **æ ¡éªŒè¯„åˆ†:** `{validation_score:.2f}`\n")
                            if step_data.get('missing_filter_columns'):
                                missing_filter = step_data.get('missing_filter_columns', {})
                                if missing_filter:
                                    content_parts.append("\n**âš ï¸ ç¼ºå¤±çš„æŸ¥è¯¢æ¡ä»¶åˆ—:**\n")
                                    for table_name, cols in missing_filter.items():
                                        if cols:
                                            content_parts.append(f"- `{table_name}`: {', '.join([f'`{c}`' for c in cols])}\n")
                        elif step_name == "step_12_sql_validation":
                            content_parts.append("**SQLç”Ÿæˆç»“æœ:**\n")
                            if step_data.get('sql'):
                                sql_content = step_data.get('sql', '')
                                content_parts.append("**ç”Ÿæˆçš„SQL:**\n")
                                content_parts.append(f"```sql\n{sql_content}\n```\n")
                            if step_data.get('execution_result', {}).get('row_count', 0) > 0:
                                row_count = step_data.get('execution_result', {}).get('row_count', 0)
                                content_parts.append(f"\n**æ‰§è¡Œç»“æœ:** è¿”å› **{row_count}** è¡Œæ•°æ®\n")
                        elif step_name == "step_13_specified_sql":
                            content_parts.append("**æŒ‡å®šSQLç”Ÿæˆç»“æœ:**\n")
                            if step_data.get('sql'):
                                sql_content = step_data.get('sql', '')
                                content_parts.append("**æŒ‡å®šSQL:**\n")
                                content_parts.append(f"```sql\n{sql_content}\n```\n")
                            if step_data.get('retry_count', 0) > 0:
                                retry_count = step_data.get('retry_count', 0)
                                content_parts.append(f"\n**é‡è¯•æ¬¡æ•°:** {retry_count} æ¬¡\n")
                            if step_data.get('final_execution_result', {}).get('row_count', 0) > 0:
                                row_count = step_data.get('final_execution_result', {}).get('row_count', 0)
                                content_parts.append(f"\n**æ‰§è¡Œç»“æœ:** è¿”å› **{row_count}** è¡Œæ•°æ®\n")
                    else:
                        content_parts.append("**æ‰§è¡ŒçŠ¶æ€:** âŒ æ‰§è¡Œå¤±è´¥\n")
                        if step_data.get("error"):
                            error_msg = str(step_data.get('error', ''))[:200]
                            content_parts.append(f"\n**é”™è¯¯ä¿¡æ¯:**\n```\n{error_msg}\n```\n")
                    
                    # åˆ›å»ºå¹¶yieldæ­¥éª¤ç»“æœchunkï¼ˆMarkdownæ ¼å¼ï¼‰
                    step_content = "".join(content_parts)
                    chunk = self._create_chunk(_id, content=step_content, chunk_type="text", finish_reason="")
                    yield chunk
                    
                except queue.Empty:
                    # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
                    continue
                except Exception as e:
                    logger.error(f"âš ï¸ å¤„ç†æ­¥éª¤ç»“æœå¤±è´¥: {e}")
                    traceback.print_exc()
                    break
            
            # ç­‰å¾…åå°çº¿ç¨‹å®Œæˆ
            agent_thread.join(timeout=300)  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
            
            # è·å–æœ€ç»ˆç»“æœ
            sql_workflow_result = final_sql_workflow_result
            if not sql_workflow_result:
                chunk = self._create_chunk(_id, content="## âŒ æ™ºèƒ½é—®æ•°å¤±è´¥\n\n**é”™è¯¯:** æœªè·å–åˆ°ç»“æœ", chunk_type="text", finish_reason="stop")
                yield chunk
                return
            if not sql_workflow_result.get("success"):
                error_msg = sql_workflow_result.get("error", "æ™ºèƒ½é—®æ•°å¤±è´¥")
                chunk = self._create_chunk(_id, content=f"## âŒ æ™ºèƒ½é—®æ•°å¤±è´¥\n\n**é”™è¯¯ä¿¡æ¯:** {error_msg}", chunk_type="text", finish_reason="stop")
                yield chunk
                return
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å…ƒæ•°æ®æŸ¥è¯¢
            if sql_workflow_result.get("is_metadata_query"):
                # æ˜¯å…ƒæ•°æ®æŸ¥è¯¢ï¼Œç›´æ¥æ˜¾ç¤ºç»“æœå¹¶è¿”å›
                query_type = sql_workflow_result.get("query_type", "")
                
                if sql_workflow_result.get("success"):
                    # æŸ¥è¯¢æˆåŠŸ
                    metadata_result = sql_workflow_result.get("metadata_result", {})
                    message = sql_workflow_result.get("message", "")
                    
                    result_content = f"\n## âœ… æ•°æ®åº“å…ƒæ•°æ®æŸ¥è¯¢ç»“æœ\n\n"
                    result_content += f"**æŸ¥è¯¢ç±»å‹:** {query_type}\n\n"
                    result_content += f"**æŸ¥è¯¢ç»“æœ:**\n{message}\n"
                    
                    chunk = self._create_chunk(_id, content=result_content, chunk_type="text", finish_reason="stop")
                    yield chunk
                    return
                else:
                    # æŸ¥è¯¢å¤±è´¥ï¼ˆè¡¨åæˆ–åˆ—åé”™è¯¯ï¼‰
                    error_msg = sql_workflow_result.get("error", "å…ƒæ•°æ®æŸ¥è¯¢å¤±è´¥")
                    error_message = sql_workflow_result.get("error_message", error_msg)
                    available_tables = sql_workflow_result.get("available_tables", [])
                    available_columns = sql_workflow_result.get("available_columns", [])
                    
                    result_content = f"\n## âš ï¸ æ•°æ®åº“å…ƒæ•°æ®æŸ¥è¯¢é”™è¯¯\n\n"
                    result_content += f"**æŸ¥è¯¢ç±»å‹:** {query_type}\n\n"
                    result_content += f"**é”™è¯¯ä¿¡æ¯:** {error_msg}\n\n"
                    
                    if error_message:
                        result_content += f"**è¯¦ç»†é”™è¯¯è¯´æ˜:**\n{error_message}\n\n"
                    
                    if available_tables:
                        result_content += f"**å¯ç”¨çš„è¡¨ååˆ—è¡¨ï¼ˆå…± {len(available_tables)} ä¸ªï¼‰:**\n"
                        for i, table_name in enumerate(available_tables[:30], 1):
                            result_content += f"{i}. `{table_name}`\n"
                        if len(available_tables) > 30:
                            result_content += f"\n... è¿˜æœ‰ {len(available_tables) - 30} ä¸ªè¡¨æœªæ˜¾ç¤º\n"
                        result_content += "\n"
                    
                    if available_columns:
                        result_content += f"**å¯ç”¨çš„åˆ—ååˆ—è¡¨ï¼ˆå…± {len(available_columns)} ä¸ªï¼‰:**\n"
                        for i, col_name in enumerate(available_columns[:30], 1):
                            result_content += f"{i}. `{col_name}`\n"
                        if len(available_columns) > 30:
                            result_content += f"\n... è¿˜æœ‰ {len(available_columns) - 30} ä¸ªåˆ—æœªæ˜¾ç¤º\n"
                        result_content += "\n"
                    
                    chunk = self._create_chunk(_id, content=result_content, chunk_type="text", finish_reason="stop")
                    yield chunk
                    return
            
            # ä»sql_workflow_resultä¸­æå–SQLå’Œåˆ—ä¿¡æ¯
            sql = sql_workflow_result.get("sql", "")
            columns_with_description = sql_workflow_result.get("columns_with_description", [])
            columns_with_table_prefix = sql_workflow_result.get("columns_with_table_prefix", [])  # table.col æ ¼å¼çš„åˆ—ååˆ—è¡¨
            logical_calculations = sql_workflow_result.get("logical_calculations", [])
            execution_result = sql_workflow_result.get("execution_result", {})
            
            if not sql:
                logger.warning("âš ï¸ SQLä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œ")
                chunk = self._create_chunk(_id, content="## âš ï¸ æ‰§è¡Œå¤±è´¥\n\n**é”™è¯¯:** SQLä¸ºç©ºï¼Œæ— æ³•æ‰§è¡ŒæŸ¥è¯¢", chunk_type="text", finish_reason="stop")
                yield chunk
            else:
                logger.info(f"âœ… æ‰§è¡ŒSQL: {sql[:100]}...")
                
                # ä»columns_with_descriptionä¸­æå–åˆ—ä¿¡æ¯ï¼ˆä½¿ç”¨ table.col æ ¼å¼ï¼‰
                columns_desc = []  # åˆ—æè¿°ï¼ˆç”¨äºCSVè¡¨å¤´ï¼Œä½¿ç”¨ table.col æ ¼å¼ï¼‰
                columns_types = []  # åˆ—ç±»å‹
                columns_used = []   # åˆ—åï¼ˆtable.col æ ¼å¼ï¼‰
                
                # ä¼˜å…ˆä½¿ç”¨ columns_with_table_prefixï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if columns_with_table_prefix:
                    # æ„å»ºåˆ—ååˆ°æè¿°çš„æ˜ å°„
                    col_desc_map = {}
                    col_type_map = {}
                    for col in columns_with_description:
                        col_name_with_table = col.get("col_name_with_table", f"{col.get('table_name', '')}.{col.get('col_name', '')}")
                        col_desc_map[col_name_with_table] = col.get("col_description", col.get("col_name", ""))
                        col_type_map[col_name_with_table] = col.get("col_type", "unknown")
                    
                    # ä½¿ç”¨ columns_with_table_prefix ä¸­çš„åˆ—å
                    for col_name_with_table in columns_with_table_prefix:
                        columns_used.append(col_name_with_table)
                        columns_types.append(col_type_map.get(col_name_with_table, "unknown"))
                        columns_desc.append(col_desc_map.get(col_name_with_table, col_name_with_table))
                else:
                    # å¦‚æœæ²¡æœ‰ columns_with_table_prefixï¼Œä» columns_with_description æ„å»º
                    for col in columns_with_description:
                        col_name = col.get("col_name", "")
                        col_name_with_table = col.get("col_name_with_table", f"{col.get('table_name', '')}.{col_name}")
                        col_type = col.get("col_type", "")
                        col_description = col.get("col_description", col_name)
                        
                        if col_name:
                            # ä½¿ç”¨ table.col æ ¼å¼çš„åˆ—å
                            columns_used.append(col_name_with_table)
                            columns_types.append(col_type or "unknown")
                            columns_desc.append(col_description)
                
                # å¦‚æœæ— æ³•ä»columns_with_descriptionè·å–åˆ—ä¿¡æ¯ï¼Œå°è¯•ä»SQLæ‰§è¡Œç»“æœè·å–
                if not columns_used:
                    logger.warning("âš ï¸ æ— æ³•ä»columns_with_descriptionè·å–åˆ—ä¿¡æ¯ï¼Œå°†ä»SQLæ‰§è¡Œç»“æœä¸­è·å–")
                    # å…ˆæ‰§è¡ŒSQLè·å–åˆ—ä¿¡æ¯
                    try:
                        sql_url = db_info.get("db_host", "")
                        sql_port = str(db_info.get("db_port", ""))
                        sql_user = db_info.get("db_user", "")
                        sql_password = db_info.get("db_password", "")
                        sql_database = db_info.get("db_name", "")
                        sql_type = db_info.get("db_type")
                        conn, _ = self.sql_obj.connect_database(sql_url, sql_port, sql_type, sql_database, sql_user, sql_password)
                        cursor = conn.cursor()
                        cursor.execute(sql)
                        
                        # ä»cursor.descriptionè·å–åˆ—ä¿¡æ¯
                        # å°è¯•ä»columns_with_descriptionä¸­åŒ¹é…åˆ—åï¼Œæ„å»º table.col æ ¼å¼
                        if cursor.description:
                            # æ„å»ºåˆ—åæ˜ å°„ï¼ˆåŸå§‹åˆ—å -> table.colï¼‰
                            col_name_mapping = {}
                            for col in columns_with_description:
                                orig_col = col.get("col_name", "")
                                col_with_table = col.get("col_name_with_table", f"{col.get('table_name', '')}.{orig_col}")
                                if orig_col:
                                    col_name_mapping[orig_col.lower()] = col_with_table
                            
                            for desc in cursor.description:
                                orig_col_name = desc[0]  # åŸå§‹åˆ—å
                                # å°è¯•æ˜ å°„åˆ° table.col æ ¼å¼
                                mapped_col_name = col_name_mapping.get(orig_col_name.lower(), orig_col_name)
                                columns_used.append(mapped_col_name)  # ä½¿ç”¨ table.col æ ¼å¼çš„åˆ—å
                                columns_types.append(desc[1] or "unknown")  # åˆ—ç±»å‹
                                columns_desc.append(mapped_col_name)  # ä½¿ç”¨ table.col æ ¼å¼ä½œä¸ºæè¿°
                        
                        conn.close()
                    except Exception as e:
                        logger.error(f"âš ï¸ è·å–åˆ—ä¿¡æ¯å¤±è´¥: {e}")
                        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        columns_desc = ["åˆ—1", "åˆ—2"]  # é»˜è®¤åˆ—æè¿°
                        columns_types = ["unknown", "unknown"]
                    
                logger.info(f"ğŸ“Š æå–åˆ° {len(columns_used)} ä¸ªåˆ—")
                logger.info(f"   - åˆ—å: {columns_used}")
                logger.info(f"   - åˆ—ç±»å‹: {columns_types}")
                
                # æ‰§è¡ŒSQLå¹¶ä¿å­˜åˆ°CSV
                file_name = "conf/tmp/sandbox_files/" + uuid.uuid4().hex[:16]
                read_flag, max_num = self._read_data(sql, db_info, file_name, columns_desc, columns_types)
                
                if not read_flag or max_num == 0:
                    logger.warning(f"âš ï¸ SQLæ‰§è¡Œå¤±è´¥æˆ–æ²¡æœ‰æ•°æ®")
                    error_msg = execution_result.get('error', 'æœªçŸ¥é”™è¯¯') if execution_result else 'æœªçŸ¥é”™è¯¯'
                    chunk = self._create_chunk(_id, content=f"SQLæ‰§è¡Œå¤±è´¥æˆ–æ²¡æœ‰æ•°æ®: {error_msg}", chunk_type="text", finish_reason="stop")
                    yield chunk
                else:
                    logger.info(f"âœ… SQLæ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {max_num} è¡Œæ•°æ®")
                    
                    # è¯»å–CSVæ–‡ä»¶å¹¶ç”ŸæˆHTMLè¡¨æ ¼
                    csv_file_path = file_name + ".csv"
                    html_table = self._csv_to_html_table(csv_file_path, max_num, max_rows=10)
                    logger.info(f"ğŸ“Š ç”ŸæˆHTMLè¡¨æ ¼: {'æˆåŠŸ' if html_table else 'å¤±è´¥'}")
                        
                    # å¦‚æœç”Ÿæˆäº†HTMLè¡¨æ ¼ï¼Œæµå¼è¿”å›
                    if html_table: 
                        chunk = self._create_chunk(_id, content=html_table, chunk_type="html_table", finish_reason="")
                        yield chunk

                    # é€»è¾‘è®¡ç®—
                    if logical_calculations:
                        logger.info(f"ğŸ”¢ å¼€å§‹æ‰§è¡Œé€»è¾‘è®¡ç®—ï¼Œå…± {len(logical_calculations)} ä¸ªè®¡ç®—è§„åˆ™")
                        try:
                            logic_result = agentic_sql_run.run_logic_calculation(
                                csv_file_path=csv_file_path,
                                query=query,
                                logical_calculations=logical_calculations,
                                columns_desc=columns_desc,
                                columns_types=columns_types,
                                sql=sql
                            )
                            if logic_result.get("success"):
                                calculation_summary = logic_result.get("calculation_summary", "")
                                calculation_result = logic_result.get("calculation_result", {})
                                tools_used = logic_result.get("tools_used", [])
                                interpretation = logic_result.get("interpretation", {})
                                final_interpretation = logic_result.get("final_interpretation", {})
                                
                                if calculation_result or calculation_summary or interpretation or final_interpretation:
                                    # æ„å»ºé€»è¾‘è®¡ç®—ç»“æœçš„æ˜¾ç¤ºå†…å®¹
                                    logic_content = "\n\n## ğŸ”¢ é€»è¾‘è®¡ç®—ç»“æœ\n\n"
                                    
                                    # ä¼˜å…ˆæ˜¾ç¤ºæœ€ç»ˆç»¼åˆè§£è¯»
                                    if final_interpretation:
                                        overall_summary = final_interpretation.get("overall_summary", "")
                                        question_answer = final_interpretation.get("question_answer", "")
                                        key_findings = final_interpretation.get("key_findings", [])
                                        business_insights = final_interpretation.get("business_insights", [])
                                        limitations = final_interpretation.get("limitations", "")
                                        next_steps = final_interpretation.get("next_steps", "")
                                        
                                        if overall_summary:
                                            logic_content += f"**ğŸ“Š æ•´ä½“æ€»ç»“ï¼š**\n{overall_summary}\n\n"
                                        
                                        if question_answer:
                                            logic_content += f"**â“ é—®é¢˜å›ç­”ï¼š**\n{question_answer}\n\n"
                                        
                                        if key_findings:
                                            logic_content += "**ğŸ” å…³é”®å‘ç°ï¼š**\n"
                                            for i, finding in enumerate(key_findings, 1):
                                                logic_content += f"{i}. {finding}\n"
                                            logic_content += "\n"
                                        
                                        if business_insights:
                                            logic_content += "**ğŸ’¡ ä¸šåŠ¡æ´å¯Ÿï¼š**\n"
                                            for i, insight in enumerate(business_insights, 1):
                                                logic_content += f"{i}. {insight}\n"
                                            logic_content += "\n"
                                        
                                        if limitations:
                                            logic_content += f"**âš ï¸ å±€é™æ€§è¯´æ˜ï¼š**\n{limitations}\n\n"
                                        
                                        if next_steps:
                                            logic_content += f"**ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®ï¼š**\n{next_steps}\n\n"
                                    
                                    if calculation_summary:
                                        logic_content += f"**è®¡ç®—æ‘˜è¦ï¼š** {calculation_summary}\n\n"
                                    
                                    if tools_used:
                                        logic_content += f"**ä½¿ç”¨çš„ç»Ÿè®¡å·¥å…·ï¼š** {', '.join(tools_used)}\n\n"
                                    
                                    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœè§£è¯»
                                    if interpretation:
                                        interpretation_summary = interpretation.get("interpretation_summary", "")
                                        key_insights = interpretation.get("key_insights", [])
                                        detailed_interpretation = interpretation.get("detailed_interpretation", "")
                                        
                                        if interpretation_summary:
                                            logic_content += f"**ğŸ“ˆ ç»Ÿè®¡ç»“æœè§£è¯»æ‘˜è¦ï¼š**\n{interpretation_summary}\n\n"
                                        
                                        if key_insights:
                                            logic_content += "**ğŸ” å…³é”®æ´å¯Ÿï¼š**\n"
                                            for i, insight in enumerate(key_insights, 1):
                                                logic_content += f"{i}. {insight}\n"
                                            logic_content += "\n"
                                        
                                        if detailed_interpretation:
                                            logic_content += f"**ğŸ“Š è¯¦ç»†è§£è¯»ï¼š**\n{detailed_interpretation}\n\n"
                                    
                                    # if calculation_result:
                                    #     # æ ¼å¼åŒ–ç»Ÿè®¡ç»“æœï¼ˆä½œä¸ºè¡¥å……ä¿¡æ¯ï¼‰
                                    #     logic_content += "**ğŸ“‹ ç»Ÿè®¡ç»“æœè¯¦æƒ…ï¼š**\n\n"
                                    #     try:
                                    #         # å°†ç»Ÿè®¡ç»“æœè½¬æ¢ä¸ºæ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²
                                    #         stats_json = json.dumps(calculation_result, ensure_ascii=False, indent=2)
                                    #         logic_content += f"```json\n{stats_json}\n```\n\n"
                                    #     except Exception as e:
                                    #         logger.warning(f"âš ï¸ æ ¼å¼åŒ–ç»Ÿè®¡ç»“æœå¤±è´¥: {e}")
                                    #         logic_content += f"{str(calculation_result)}\n\n"
                                    
                                    chunk = self._create_chunk(_id, content=logic_content, chunk_type="text", finish_reason="")
                                    yield chunk
                                    logger.info(f"âœ… é€»è¾‘è®¡ç®—å®Œæˆå¹¶è¿”å›ç»“æœï¼ˆåŒ…å«æœ€ç»ˆè§£è¯»ï¼‰")
                                else:
                                    logger.info(f"â„¹ï¸ é€»è¾‘è®¡ç®—å®Œæˆï¼Œä½†æ— ç»“æœè¿”å›")
                            else:
                                error_msg = logic_result.get("error", "æœªçŸ¥é”™è¯¯")
                                logger.warning(f"âš ï¸ é€»è¾‘è®¡ç®—å¤±è´¥: {error_msg}")
                                # ä¸è¿”å›é”™è¯¯ï¼Œç»§ç»­æ‰§è¡Œåç»­æµç¨‹
                        except Exception as e:
                            logger.error(f"âŒ é€»è¾‘è®¡ç®—æ‰§è¡Œå¼‚å¸¸: {e}")
                            traceback.print_exc()
                            # ä¸è¿”å›é”™è¯¯ï¼Œç»§ç»­æ‰§è¡Œåç»­æµç¨‹
                    else:
                        logger.info(f"â„¹ï¸ æ— éœ€æ‰§è¡Œé€»è¾‘è®¡ç®—ï¼ˆæœªè¯†åˆ«å‡ºé€»è¾‘è®¡ç®—è§„åˆ™ï¼‰")
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        if os.path.exists(csv_file_path):
                            os.remove(csv_file_path)
                        logger.info("âœ… ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(f"âŒ chat_with_sql æ‰§è¡Œå¤±è´¥: {e}")
            logger.error(f"Traceback: {error_traceback}")
            
            # æµå¼è¿”å›é”™è¯¯æ¶ˆæ¯
            content = f"æ™ºèƒ½é—®æ•°æ‰§è¡Œå¤±è´¥: {str(e)}"
            _id = f"chatcmpl-error-{int(time.time())}"
            chunk = self._create_chunk(_id, content, chunk_type="text", finish_reason="stop")
            yield chunk

    def _get_sql_step_display_titles(self) -> Dict[str, str]:
        """æ™ºèƒ½é—®æ•°æµç¨‹ä¸­å„æ­¥éª¤çš„å±•ç¤ºæ ‡é¢˜ï¼ˆæ­¥éª¤å -> Markdown æ ‡é¢˜ï¼‰ã€‚"""
        return {
            "step_1_database_info": "## ğŸ“š æ­¥éª¤1: è·å–æ•°æ®åº“ä¿¡æ¯",
            "step_1_2_metadata_query": "## ğŸ” æ­¥éª¤1.2: æ•°æ®åº“å…ƒæ•°æ®æŸ¥è¯¢",
            "step_1_5_sql_intent_recognition": "## ğŸ” æ­¥éª¤1.5: SQLæ„å›¾è¯†åˆ«",
            "step_1_5_sql_intent_recognition_warning": "### âš ï¸ SQLæ„å›¾è¯†åˆ«è­¦å‘Š",
            "step_1_5_sql_intent_recognition_final": "### ğŸ’¬ SQLæ„å›¾è¯†åˆ«ç»“æœ",
            "step_2_intent_recognition": "## ğŸ§  æ­¥éª¤2: æ„å›¾è¯†åˆ«",
            "step_3_sql_generation": "## ğŸ’» æ­¥éª¤3: SQLç”Ÿæˆç»“æœ",
            "step_final_result": "## âœ… æœ€ç»ˆç»“æœ",
            "step_4_evaluation": "## ğŸ“ˆ æ­¥éª¤4: ç»“æœè¯„ä¼°",
            "step_5_expansion": "## ğŸš€ æ­¥éª¤5: æ‰©å±•æœç´¢",
            "step_6_rerank": "## ğŸ”„ æ­¥éª¤6: ç»“æœé‡æ’åº",
            "step_7_artifact": "## ğŸ¨ æ­¥éª¤7: Artifactå¤„ç†",
            "step_8_system_prompt": "## ğŸ“ æ­¥éª¤8: ç”ŸæˆSystem Prompt",
            "step_9_enhance_results": "## ğŸ“Š æ­¥éª¤9: å¢å¼ºæœç´¢ç»“æœ",
            "step_10_schema_selection": "## ğŸ¯ æ­¥éª¤10: Schemaç²¾ç¡®é€‰æ‹©",
            "step_11_validation": "## âœ… æ­¥éª¤11: Schemaæ ¡éªŒ",
            "step_12_sql_validation": "## ğŸ” æ­¥éª¤12: SQLç”Ÿæˆå’Œæ ¡éªŒ",
            "step_13_specified_sql": "## ğŸ“‹ æ­¥éª¤13: ç”ŸæˆæŒ‡å®šSQL",
        }

    def _read_data(self, sql, db_info, file_name, columns_desc, columns_types=None):
        """
        è¯»å–æ•°æ®åº“æ•°æ®å¹¶è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        
        Args:
            sql: SQLæŸ¥è¯¢è¯­å¥
            db_info: æ•°æ®åº“è¿æ¥ä¿¡æ¯
            file_name: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            columns_desc: åˆ—æè¿°åˆ—è¡¨ï¼ˆç”¨äºCSVè¡¨å¤´ï¼‰
            columns_types: åˆ—ç±»å‹åˆ—è¡¨ï¼ˆç”¨äºç»Ÿè®¡è®¡ç®—ï¼Œå¯é€‰ï¼‰
        
        Returns:
            tuple: (æ˜¯å¦æˆåŠŸ, æ•°æ®è¡Œæ•°)
        """        
        sql_url = db_info.get("db_host", "")
        sql_port = str(db_info.get("db_port", ""))
        sql_user = db_info.get("db_user", "")
        sql_password = db_info.get("db_password", "")
        sql_database = db_info.get("db_name", "")
        sql_type = db_info.get("db_type")
        conn, _ = self.sql_obj.connect_database(sql_url, sql_port, sql_type, sql_database, sql_user, sql_password)      
        cursor = conn.cursor()
        try:
            cursor.execute(sql)
        except Exception as e:
            logger.error(f"æ‰§è¡ŒSQLå¤±è´¥: {e}")
            if cursor:
                cursor.close()
            return False, 0
        
        csv_f = file_name + ".csv"
        _len = 0
        with open(csv_f, mode='w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            # å†™å…¥è¡¨å¤´
            writer.writerow(columns_desc)
            # å†™å…¥æ•°æ®è¡Œ
            for row in cursor.fetchall():
                tmp_l = list(row)
                writer.writerow(tmp_l)
                _len = _len + 1
        return True, _len
    
    def statictics_data(self, csv_f, columns_desc, columns_types=None):
        """è°ƒç”¨ç»Ÿè®¡å‡½æ•°è¿›è¡Œæ•°æ®åˆ†æ"""
        try:
            # å¦‚æœæ²¡æœ‰æä¾› columns_typesï¼Œå°è¯•ä»æ•°æ®åº“è·å–
            if columns_types is None or len(columns_types) == 0:
                return {}
            # ç¡®ä¿ columns_types é•¿åº¦ä¸ columns_desc ä¸€è‡´
            if len(columns_types) < len(columns_desc):
                columns_types.extend(["unknown"] * (len(columns_desc) - len(columns_types)))
            elif len(columns_types) > len(columns_desc):
                columns_types = columns_types[:len(columns_desc)]
            
            statistics_result = calculate_statistics(csv_f, columns_types)
            if statistics_result:
                logger.info(f"âœ… ç»Ÿè®¡è®¡ç®—å®Œæˆï¼Œç”Ÿæˆäº† {len(statistics_result)} ç±»ç»Ÿè®¡æŒ‡æ ‡")
                # å¯ä»¥å°†ç»Ÿè®¡ç»“æœä¿å­˜åˆ°æ–‡ä»¶æˆ–è¿”å›
                # statistics_file = file_name + "_statistics.json"
                # with open(statistics_file, 'w', encoding='utf-8') as f:
                #     json.dump(statistics_result, f, ensure_ascii=False, indent=2)
                return statistics_result
            else:
                return {}
        except Exception as e:
            logger.warning(f"âš ï¸ ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return {}
    
    def _csv_to_html_table(self, csv_file_path: str, max_num, max_rows: int = 50) -> str:
        """
        è¯»å–CSVæ–‡ä»¶å¹¶ç”ŸæˆHTMLè¡¨æ ¼æ ¼å¼
        
        Args:
            csv_file_path: CSVæ–‡ä»¶è·¯å¾„
            max_rows: æœ€å¤§æ˜¾ç¤ºè¡Œæ•°ï¼Œé»˜è®¤50è¡Œ
            max_num: CSVæ–‡ä»¶æ€»è¡Œæ•°
        Returns:
            HTMLè¡¨æ ¼å­—ç¬¦ä¸²ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            if not os.path.exists(csv_file_path):
                logger.warning(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
                return ""
            
            total_rows = max_num
            
            with open(csv_file_path, 'r', encoding='utf-8') as f:
                csv_reader = csv.reader(f)
                
                # è¯»å–è¡¨å¤´
                headers = next(csv_reader, None)
                if not headers:
                    logger.warning(f"CSVæ–‡ä»¶ä¸ºç©º: {csv_file_path}")
                    return ""
                
                # ç”Ÿæˆè¡¨å¤´HTML
                header_html = "<thead><tr>"
                for header in headers:
                    header_html += f'<th style="border: 1px solid #ddd; padding: 8px; background-color: #f2f2f2; text-align: left;">{header}</th>'
                header_html += "</tr></thead>"
                # è¯»å–æ•°æ®è¡Œ
                body_html = "<tbody>"
                for idx, row in enumerate(csv_reader):
                    if idx < max_rows:
                        body_html += "<tr>"
                        for cell in row:
                            # è½¬ä¹‰HTMLç‰¹æ®Šå­—ç¬¦
                            cell_str = str(cell) if cell is not None else ""
                            cell_str = cell_str.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace('"', "&quot;")
                            body_html += f'<td style="border: 1px solid #ddd; padding: 8px;">{cell_str}</td>'
                        body_html += "</tr>"
                    else:
                        break
                
                body_html += "</tbody>"
            
            # æ„å»ºå®Œæ•´çš„HTMLè¡¨æ ¼
            html_table = f"""
<table style="border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; font-size: 14px;">
{header_html}
{body_html}
</table>
"""
            
            # å¦‚æœæ•°æ®è¶…è¿‡æœ€å¤§è¡Œæ•°ï¼Œæ·»åŠ æç¤ºä¿¡æ¯
            if total_rows > max_rows:
                html_table += f'<p style="color: #666; font-size: 12px; margin-top: 10px;">æ³¨ï¼šæ•°æ®å…± {total_rows} è¡Œï¼Œæ­¤å¤„ä»…æ˜¾ç¤ºå‰ {max_rows} è¡Œ</p>'
            
            logger.info(f"âœ… HTMLè¡¨æ ¼ç”ŸæˆæˆåŠŸ: {csv_file_path}ï¼Œå…± {total_rows} è¡Œï¼Œæ˜¾ç¤º {min(max_rows, total_rows)} è¡Œ")
            return html_table
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆHTMLè¡¨æ ¼å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return ""
        
    def _get_database_info(self, sql_id: str) -> dict:
        """è·å–æ•°æ®åº“åŸºç¡€ä¿¡æ¯"""
        try:
            # ä»SQLiteè·å–æ•°æ®åº“è¿æ¥ä¿¡æ¯
            base_sql_info = cSingleSqlite.query_base_sql_by_sql_id(sql_id)
            if not base_sql_info:
                logger.warning(f"æœªæ‰¾åˆ°sql_id {sql_id} çš„æ•°æ®åº“ä¿¡æ¯")
                return {}
            # print(base_sql_info)
            # è·å–è¡¨ä¿¡æ¯
            tables_info = cSingleSqlite.query_table_sql_by_sql_id(sql_id)
            tables = []
            for table in tables_info:
                table_id = table.get("table_id")
                # è·å–åˆ—ä¿¡æ¯
                columns = cSingleSqlite.query_col_sql_by_table_id(table_id)
                tables.append({
                    "table_name": table.get("table_name"),
                    "table_description": table.get("table_description", ""),
                    "columns": [{
                        "col_name": col.get("col_name"),
                        "col_type": col.get("col_type"),
                        "is_nullable": col.get("col_info").get("is_nullable", True),
                        "column_default":col.get("col_info").get("column_default", None),
                        "comment":col.get("col_info").get("comment", None)
                    } for col in columns]
                })

            # è·å–å…³ç³»ä¿¡æ¯
            relations = cSingleSqlite.query_rel_sql_by_sql_id(sql_id)

            db_info = {
                "db_type": base_sql_info.get("sql_type", "mysql"),
                "db_name": base_sql_info.get("sql_name", ""),
                "db_host": base_sql_info.get("ip", ""),
                "db_port": base_sql_info.get("port", ""),
                "db_user": base_sql_info.get("sql_user_name", ""),
                "db_password": base_sql_info.get("sql_user_password", ""),
                "db_description": base_sql_info.get("sql_description", ""),
                "tables": tables,
                "relations": relations
            }

            return db_info

        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥ (sql_id: {sql_id}): {e}")
            return {}

    def _create_chunk(self, _id, content, chunk_type="text", finish_reason="", model="emb-graph-chat-model"):
        """åˆ›å»ºæµå¼å“åº”å—çš„è¾…åŠ©å‡½æ•°"""
        # å¯¹äºfileç±»å‹çš„chunkï¼Œç¡®ä¿contentåªåŒ…å«æ–‡ä»¶è·¯å¾„ï¼Œä¸åŒ…å«å…¶ä»–æ–‡æœ¬
        if chunk_type == "file":
            # æ¸…ç†contentï¼Œç¡®ä¿åªåŒ…å«æ–‡ä»¶è·¯å¾„
            cleaned_content = str(content).strip()
            # ç§»é™¤å¯èƒ½çš„æ¢è¡Œç¬¦å’Œå…¶ä»–ç©ºç™½å­—ç¬¦
            cleaned_content = cleaned_content.replace('\n', '').replace('\r', '').strip()
            # å¦‚æœcontentåŒ…å«å¤šä¸ªè·¯å¾„ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œåªå–ç¬¬ä¸€ä¸ª
            if '\n' in cleaned_content or '\r' in cleaned_content:
                logger.warning(f"æ–‡ä»¶chunkçš„contentåŒ…å«æ¢è¡Œç¬¦ï¼Œå·²æ¸…ç†: {content}")
                cleaned_content = cleaned_content.split('\n')[0].split('\r')[0].strip()
            content = cleaned_content
            logger.debug(f"åˆ›å»ºæ–‡ä»¶chunkï¼Œæ–‡ä»¶è·¯å¾„: {content}")
        
        # å¦‚æœæä¾›äº†å†…å®¹ï¼Œå³ä½¿finish_reason="stop"ï¼Œä¹Ÿè¦åŒ…å«åœ¨deltaä¸­
        # å¦‚æœcontentä¸ºç©ºä¸”finish_reason="stop"ï¼Œåˆ™deltaä¸ºç©ºï¼ˆçº¯finish chunkï¼‰
        if finish_reason == "stop" and not content:
            delta = {}
        else:
            delta = {
                "content": content,
                "type": chunk_type
            }
        
        return {
            "id": _id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [
                {
                    "index": 0,
                    "delta": delta,
                    "finish_reason": finish_reason
                }
            ],
            "finish_reason": finish_reason
        }



